{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uPfYWLl5tEs"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lSmG-XFX5th0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510e5d9f-cb27-48b4-b251-2d851577355d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, bitsandbytes\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "Successfully installed bitsandbytes-0.48.2 transformers-4.57.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U peft bitsandbytes transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o4pCCIvL6mAF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d101ad1a-98cc-45bb-c807-be20c3be1c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting trl\n",
            "  Downloading trl-0.25.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from trl) (1.12.0)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from trl) (4.0.0)\n",
            "Requirement already satisfied: transformers>=4.56.1 in /usr/local/lib/python3.12/dist-packages (from trl) (4.57.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
            "Downloading trl-0.25.1-py3-none-any.whl (465 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U1wXymmf6xvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b5cd97-d78f-43d8-c567-27fd78c4358c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.6\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaDaOyySERMq"
      },
      "source": [
        "## Our own custom data (non instrcution data) for domain specific finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jsj-dR1tEUGx"
      },
      "outputs": [],
      "source": [
        "import fitz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, load_dataset\n"
      ],
      "metadata": {
        "id": "DaCyiG4wlnuO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1kMylwolEWJY"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    text_blocks = []\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for page in doc:\n",
        "            text = page.get_text(\"text\").strip()\n",
        "            if text:\n",
        "                text_blocks.append(text)\n",
        "    return text_blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Iy53cBJ-EWMZ"
      },
      "outputs": [],
      "source": [
        "pdf_texts = extract_text_from_pdf(\"/content/Prompt Engineering_v4.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sbR0pq1dEWPJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "707139d7-6189-45c7-9272-79f3d2a5fab6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Prompt  \\nEngineering\\nAuthor: Lee Boonstra'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "pdf_texts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPbViIqzE_yh"
      },
      "source": [
        "| Step                                     | What Happens                                                                             | Example                                     |\n",
        "| ---------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------- |\n",
        "| **1 Data Collection**                  | Crawl text from web (Wikipedia, CommonCrawl, Books, PubMed, Code, etc.)                  | Billions of documents                       |\n",
        "| **2 Cleaning / Filtering**             | Remove HTML, junk, duplicates, low-quality content                                       | Similar to your `strip()` & len filter      |\n",
        "| **3 Splitting / Chunking**             | Split into paragraphs / segments using regex rules, sentence boundaries, or token length | Like your `re.split(r'\\n\\s*\\n')`            |\n",
        "| **4 Tokenization**                     | Convert text chunks  token IDs using BPE                                                | The cat  `[1012, 52, 199]`               |\n",
        "| **5 Training (Next-Token Prediction)** | Model learns to predict the next token in each chunk                                     | Input: The cat sat on the  Target: mat |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "up7ge9zQEWSH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def split_paragraphs(pages):\n",
        "    paragraphs = []\n",
        "    for page_text in pages:\n",
        "        # Split on double line breaks or long newlines\n",
        "        chunks = re.split(r'\\n\\s*\\n', page_text)\n",
        "        for chunk in chunks:\n",
        "            clean = chunk.strip()\n",
        "            if len(clean) > 30:  # ignore too short lines\n",
        "                paragraphs.append(clean)\n",
        "    return paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hy_CMrFuFhwY"
      },
      "outputs": [],
      "source": [
        "paragraphs = split_paragraphs(pdf_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mSEs3pV0FimN"
      },
      "outputs": [],
      "source": [
        "data = [{\"text\": p} for p in paragraphs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xca8d4o8FkTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa1f2d6b-464d-41ab-c30b-b63575529efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'text': 'Prompt  \\nEngineering\\nAuthor: Lee Boonstra'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n2\\nAcknowledgements\\nReviewers and Contributors\\nMichael Sherman\\nYuan Cao\\nErick Armbrust\\nAnant Nawalgaria\\nAntonio Gulli\\nSimone Cammel\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning'}, {'text': 'Introduction\\x08\\n6\\nPrompt engineering\\x08\\n7\\nLLM output configuration\\x08\\n8'}, {'text': 'Putting it all together\\t\\n11\\nPrompting techniques\\x08\\n12'}, {'text': 'General prompting / zero shot\\x08\\n13'}, {'text': 'System, contextual and role prompting\\x08\\n17'}, {'text': 'Contextual prompting\\t\\n23\\nTable of contents'}, {'text': 'Automatic Prompt Engineering\\x08\\n40'}, {'text': 'Prompts for explaining code\\t\\n44'}, {'text': 'Prompts for translating code\\t\\n46'}, {'text': 'Prompts for debugging and reviewing code\\t\\n48'}, {'text': 'What about multimodal prompting?\\t\\n54\\nBest Practices\\x08\\n54'}, {'text': 'Be specific about the output\\x08\\n56'}, {'text': 'Use Instructions over Constraints\\x08\\n56'}, {'text': 'Control the max token length\\x08\\n58'}, {'text': 'Experiment with input formats and writing styles\\x08\\n59'}, {'text': 'For few-shot prompting with classification tasks, mix up the classes\\x08\\n59'}, {'text': 'Experiment with output formats\\x08\\n60'}, {'text': 'Experiment together with other prompt engineers\\x08\\n61'}, {'text': 'Document the various prompt attempts\\x08\\n62\\nSummary\\x08\\n63\\nEndnotes\\x08\\n65'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n6\\nIntroduction\\nWhen thinking about a large language model input and output, a text prompt (sometimes \\naccompanied by other modalities such as image prompts) is the input the model uses \\nto predict a specific output. You dont need to be a data scientist or a machine learning \\nengineer  everyone can write a prompt. However, crafting the most effective prompt can be \\ncomplicated. Many aspects of your prompt affect its efficacy: the model you use, the models \\ntraining data, the model configurations, your word-choice, style and tone, structure, and \\ncontext all matter. Therefore, prompt engineering is an iterative process. Inadequate prompts \\ncan lead to ambiguous, inaccurate responses, and can hinder the models ability to provide \\nmeaningful output.\\nYou dont need to be a data \\nscientist or a machine learning \\nengineer  everyone can write \\na prompt.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n7\\nWhen you chat with the Gemini chatbot,1 you basically write prompts, however this \\nwhitepaper focuses on writing prompts for the Gemini model within Vertex AI or by using  \\nthe API, because by prompting the model directly you will have access to the configuration \\nsuch as temperature etc.\\nThis whitepaper discusses prompt engineering in detail. We will look into the various \\nprompting techniques to help you getting started and share tips and best practices to \\nbecome a prompting expert. We will also discuss some of the challenges you can face  \\nwhile crafting prompts.\\nPrompt engineering\\nRemember how an LLM works; its a prediction engine. The model takes sequential text as \\nan input and then predicts what the following token should be, based on the data it was \\ntrained on. The LLM is operationalized to do this over and over again, adding the previously \\npredicted token to the end of the sequential text for predicting the following token. The next \\ntoken prediction is based on the relationship between whats in the previous tokens and what \\nthe LLM has seen during its training. \\nWhen you write a prompt, you are attempting to set up the LLM to predict the right sequence \\nof tokens. Prompt engineering is the process of designing high-quality prompts that guide \\nLLMs to produce accurate outputs. This process involves tinkering to find the best prompt, \\noptimizing prompt length, and evaluating a prompts writing style and structure in relation \\nto the task. In the context of natural language processing and LLMs, a prompt is an input \\nprovided to the model to generate a response or prediction.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n8\\nThese prompts can be used to achieve various kinds of understanding and generation \\ntasks such as text summarization, information extraction, question and answering, text \\nclassification, language or code translation, code generation, and code documentation \\nor reasoning.\\nPlease feel free to refer to Googles prompting guides2,3 with simple and effective \\nprompting examples.\\nWhen prompt engineering, you will start by choosing a model. Prompts might need to be \\noptimized for your specific model, regardless of whether you use Gemini language models in \\nVertex AI, GPT, Claude, or an open source model like Gemma or LLaMA.\\nBesides the prompt, you will also need to tinker with the various configurations of a LLM.\\nLLM output configuration\\nOnce you choose your model you will need to figure out the model configuration. Most LLMs \\ncome with various configuration options that control the LLMs output. Effective prompt \\nengineering requires setting these configurations optimally for your task.\\nOutput length\\nAn important configuration setting is the number of tokens to generate in a response. \\nGenerating more tokens requires more computation from the LLM, leading to higher energy \\nconsumption, potentially slower response times, and higher costs.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n9\\nReducing the output length of the LLM doesnt cause the LLM to become more stylistically \\nor textually succinct in the output it creates, it just causes the LLM to stop predicting more \\ntokens once the limit is reached. If your needs require a short output length, youll also \\npossibly need to engineer your prompt to accommodate.\\nOutput length restriction is especially important for some LLM prompting techniques, like \\nReAct, where the LLM will keep emitting useless tokens after the response you want.\\nSampling controls\\nLLMs do not formally predict a single token. Rather, LLMs predict probabilities for what the \\nnext token could be, with each token in the LLMs vocabulary getting a probability. Those \\ntoken probabilities are then sampled to determine what the next produced token will be. \\nTemperature, top-K, and top-P are the most common configuration settings that determine \\nhow predicted token probabilities are processed to choose a single output token.\\nTemperature\\nTemperature controls the degree of randomness in token selection. Lower temperatures \\nare good for prompts that expect a more deterministic response, while higher temperatures \\ncan lead to more diverse or unexpected results. A temperature of 0 (greedy decoding) is \\ndeterministic: the highest probability token is always selected (though note that if two tokens \\nhave the same highest predicted probability, depending on how tiebreaking is implemented \\nyou may not always get the same output with temperature 0).\\nTemperatures close to the max tend to create more random output. And as temperature gets \\nhigher and higher, all tokens become equally likely to be the next predicted token.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n10\\nThe Gemini temperature control can be understood in a similar way to the softmax function \\nused in machine learning. A low temperature setting mirrors a low softmax temperature (T), \\nemphasizing a single, preferred temperature with high certainty. A higher Gemini temperature \\nsetting is like a high softmax temperature, making a wider range of temperatures around \\nthe selected setting more acceptable. This increased uncertainty accommodates scenarios \\nwhere a rigid, precise temperature may not be essential like for example when experimenting \\nwith creative outputs.\\nTop-K and top-P\\nTop-K and top-P (also known as nucleus sampling)4 are two sampling settings used in LLMs \\nto restrict the predicted next token to come from tokens with the top predicted probabilities.  \\nLike temperature, these sampling settings control the randomness and diversity of \\ngenerated text.\\n\\t Top-K sampling selects the top K most likely tokens from the models predicted \\ndistribution. The higher top-K, the more creative and varied the models output; the \\nlower top-K, the more restive and factual the models output. A top-K of 1 is equivalent to \\ngreedy decoding.\\n\\t Top-P sampling selects the top tokens whose cumulative probability does not exceed \\na certain value (P). Values for P range from 0 (greedy decoding) to 1 (all tokens in the \\nLLMs vocabulary).\\nThe best way to choose between top-K and top-P is to experiment with both methods (or \\nboth together) and see which one produces the results you are looking for.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n11\\nAnother important configuration setting is the number of tokens to generate in a response. \\nBe aware, generating more tokens requires more computation from the LLM, leading \\nto higher energy consumption and potentially slower response times, which leads to \\nhigher costs.\\nPutting it all together\\nChoosing between top-K, top-P, temperature, and the number of tokens to generate, \\ndepends on the specific application and desired outcome, and the settings all impact one \\nanother. Its also important to make sure you understand how your chosen model combines \\nthe different sampling settings together.\\nIf temperature, top-K, and top-P are all available (as in Vertex Studio), tokens that meet \\nboth the top-K and top-P criteria are candidates for the next predicted token, and then \\ntemperature is applied to sample from the tokens that passed the top-K and top-P criteria. If \\nonly top-K or top-P is available, the behavior is the same but only the one top-K or P setting \\nis used. \\nIf temperature is not available, whatever tokens meet the top-K and/or top-P criteria are then \\nrandomly selected from to produce a single next predicted token.\\nAt extreme settings of one sampling configuration value, that one sampling setting either \\ncancels out other configuration settings or becomes irrelevant.\\n\\t If you set temperature to 0, top-K and top-P become irrelevantthe most probable \\ntoken becomes the next token predicted. If you set temperature extremely high (above \\n1generally into the 10s), temperature becomes irrelevant and whatever tokens make \\nit through the top-K and/or top-P criteria are then randomly sampled to choose a next \\npredicted token.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n12\\n\\t If you set top-K to 1, temperature and top-P become irrelevant. Only one token passes the \\ntop-K criteria, and that token is the next predicted token. If you set top-K extremely high, \\nlike to the size of the LLMs vocabulary, any token with a nonzero probability of being the \\nnext token will meet the top-K criteria and none are selected out.\\n\\t If you set top-P to 0 (or a very small value), most LLM sampling implementations will then \\nonly consider the most probable token to meet the top-P criteria, making temperature and \\ntop-K irrelevant. If you set top-P to 1, any token with a nonzero probability of being the \\nnext token will meet the top-P criteria, and none are selected out.\\nAs a general starting point, a temperature of .2, top-P of .95, and top-K of 30 will give you \\nrelatively coherent results that can be creative but not excessively so. If you want especially \\ncreative results, try starting with a temperature of .9, top-P of .99, and top-K of 40. And if you \\nwant less creative results, try starting with a temperature of .1, top-P of .9, and top-K of 20. \\nFinally, if your task always has a single correct answer (e.g., answering a math problem), start \\nwith a temperature of 0.\\nNOTE: With more freedom (higher temperature, top-K, top-P, and output tokens), the LLM \\nmight generate text that is less relevant.\\nPrompting techniques\\nLLMs are tuned to follow instructions and are trained on large amounts of data so they can \\nunderstand a prompt and generate an answer. But LLMs arent perfect; the clearer your \\nprompt text, the better it is for the LLM to predict the next likely text. Additionally, specific \\ntechniques that take advantage of how LLMs are trained and how LLMs work will help you get \\nthe relevant results from LLMs'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n13\\nNow that we understand what prompt engineering is and what it takes, lets dive into some \\nexamples of the most important prompting techniques.\\nGeneral prompting / zero shot\\nA zero-shot5 prompt is the simplest type of prompt. It only provides a description of a task \\nand some text for the LLM to get started with. This input could be anything: a question, a \\nstart of a story, or instructions. The name zero-shot stands for no examples.\\nLets use Vertex AI Studio (for Language) in Vertex AI,6 which provides a playground to test \\nprompts. In Table 1, you will see an example zero-shot prompt to classify movie reviews. \\nThe table format as used below is a great way of documenting prompts. Your prompts will \\nlikely go through many iterations before they end up in a codebase, so its important to keep \\ntrack of your prompt engineering work in a disciplined, structured way. More on this table \\nformat, the importance of tracking prompt engineering work, and the prompt development \\nprocess is in the Best Practices section later in this chapter (Document the various prompt \\nattempts).  \\nThe model temperature should be set to a low number, since no creativity is needed, and we \\nuse the gemini-pro default top-K and top-P values, which effectively disable both settings \\n(see LLM Output Configuration above). Pay attention to the generated output. The words \\ndisturbing and masterpiece should make the prediction a little more complicated, as both \\nwords are used in the same sentence.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n14\\nName\\n1_1_movie_classification\\nGoal\\nClassify movie reviews as positive, neutral or negative.\\nModel\\ngemini-pro\\nTemperature\\n0.1\\nToken Limit\\n5\\nTop-K\\nN/A\\nTop-P\\n1\\nPrompt\\nClassify movie reviews as POSITIVE, NEUTRAL or NEGATIVE. \\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. I wish there were more movies like this masterpiece. \\nSentiment:\\nOutput\\nPOSITIVE\\nTable 1. An example of zero-shot prompting\\nWhen zero-shot doesnt work, you can provide demonstrations or examples in the prompt, \\nwhich leads to one-shot and few-shot prompting. General prompting / zero shot\\nOne-shot & few-shot\\nWhen creating prompts for AI models, it is helpful to provide examples. These examples can \\nhelp the model understand what you are asking for. Examples are especially useful when you \\nwant to steer the model to a certain output structure or pattern. \\nA one-shot prompt, provides a single example,  hence the name one-shot. The idea is the \\nmodel has an example it can imitate to best complete the task.\\nA few-shot prompt 7 provides multiple examples to the model. This approach shows the  \\nmodel a pattern that it needs to follow. The idea is similar to one-shot, but multiple examples \\nof the desired pattern increases the chance the model follows the pattern.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n15\\nThe number of examples you need for few-shot prompting depends on a few factors, \\nincluding the complexity of the task, the quality of the examples, and the capabilities of the \\ngenerative AI (gen AI) model you are using. As a general rule of thumb, you should use at \\nleast three to five examples for few-shot prompting. However, you may need to use more \\nexamples for more complex tasks, or you may need to use fewer due to the input length \\nlimitation of your model.\\nTable 2 shows a few-shot prompt example, lets use the same gemini-pro model \\nconfiguration settings as before, other than increasing the token limit to accommodate the \\nneed for a longer response.\\nGoal\\nParse pizza orders to JSON\\nModel\\ngemini-pro\\nTemperature\\n0.1\\nToken Limit\\n250\\nTop-K\\nN/A\\nTop-P\\n1\\nPrompt\\nParse a customer\\'s pizza order into valid JSON:\\nEXAMPLE:\\nI want a small pizza with cheese, tomato sauce, and pepperoni.\\nJSON Response:\\n```\\n{\\n\"size\": \"small\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\\n}\\n```\\nContinues next page...'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n16\\nPrompt\\nEXAMPLE:\\nCan I get a large pizza with tomato sauce, basil and mozzarella \\n{\\n\"size\": \"large\",\\n\"type\": \"normal\",\\n\"ingredients\": [[\"tomato sauce\", \"bazel\", \"mozzarella\"]]\\n}\\nNow, I would like a large pizza, with the first half cheese and \\nmozzarella. And the other tomato sauce, ham and pineapple.\\nJSON Response:\\nOutput\\n```\\n{\\n\"size\": \"large\",\\n\"type\": \"half-half\",\\n\"ingredients\": [[\"cheese\", \"mozzarella\"], [\"tomato sauce\", \\n\"ham\", \"pineapple\"]]\\n}\\n```\\nTable 2. An example of few-shot prompting\\nWhen you choose examples for your prompt, use examples that are relevant to the task you \\nwant to perform. The examples should be diverse, of high quality, and well written. One small \\nmistake can confuse the model and will result in undesired output.\\nIf you are trying to generate output that is robust to a variety of inputs, then it is important to \\ninclude edge cases in your examples. Edge cases are inputs that are unusual or unexpected, \\nbut that the model should still be able to handle.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n17\\nSystem, contextual and role prompting\\nSystem, contextual and role prompting are all techniques used to guide how LLMs generate \\ntext, but they focus on different aspects:\\n\\t System prompting sets the overall context and purpose for the language model. It \\ndefines the big picture of what the model should be doing, like translating a language, \\nclassifying a review etc.\\n\\t Contextual prompting provides specific details or background information relevant to \\nthe current conversation or task. It helps the model to understand the nuances of whats \\nbeing asked and tailor the response accordingly.\\n\\t Role prompting assigns a specific character or identity for the language model to adopt. \\nThis helps the model generate responses that are consistent with the assigned role and its \\nassociated knowledge and behavior.\\nThere can be considerable overlap between system, contextual, and role prompting. E.g. a \\nprompt that assigns a role to the system, can also have a context.\\nHowever, each type of prompt serves a slightly different primary purpose:\\n\\t System prompt: Defines the models fundamental capabilities and overarching purpose.\\n\\t Contextual prompt: Provides immediate, task-specific information to guide the response. \\nIts highly specific to the current task or input, which is dynamic.\\n\\t Role prompt: Frames the models output style and voice. It adds a layer of specificity \\nand personality.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n18\\nDistinguishing between system, contextual, and role prompts provides a framework for \\ndesigning prompts with clear intent, allowing for flexible combinations and making it easier to \\nanalyze how each prompt type influences the language models output.\\nLets dive into these three different kinds of prompts.\\nSystem prompting\\nTable 3 contains a system prompt, where I specify additional information on how to return the \\noutput. I increased the temperature to get a higher creativity level, and I specified a higher \\ntoken limit. However, because of my clear instruction on how to return the output the model \\ndidnt return extra text.\\nGoal\\nClassify movie reviews as positive, neutral or negative.\\nModel\\ngemini-pro\\nTemperature\\n1\\nToken Limit\\n5\\nTop-K\\n40\\nTop-P\\n0.8\\nPrompt\\nClassify movie reviews as positive, neutral or negative. Only \\nreturn the label in uppercase.\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSentiment:\\nOutput\\nNEGATIVE\\nTable 3. An example of system prompting'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n19\\nSystem prompts can be useful for generating output that meets specific requirements. The \\nname system prompt actually stands for providing an additional task to the system. For \\nexample, you could use a system prompt to generate a code snippet that is compatible with \\na specific programming language, or you could use a system prompt to return a certain \\nstructure. Have a look into Table 4, where I return the output in JSON format.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n20\\nGoal\\nClassify movie reviews as positive, neutral or negative, return JSON.\\nModel\\ngemini-pro\\nTemperature\\n1\\nToken Limit\\n1024\\nTop-K\\n40\\nTop-P\\n0.8\\nPrompt\\nClassify movie reviews as positive, neutral or negative. Return \\nvalid JSON:\\nReview: \"Her\" is a disturbing study revealing the direction \\nhumanity is headed if AI is allowed to keep evolving, \\nunchecked. It\\'s so disturbing I couldn\\'t watch it.\\nSchema:\\n```\\nMOVIE:\\n{\\n\"sentiment\": String \"POSITIVE\" | \"NEGATIVE\" | \"NEUTRAL\",\\n\"name\": String\\n}\\nMOVIE REVIEWS:\\n{\\n\"movie_reviews\": [MOVIE]\\n}\\n```\\nJSON Response:\\nOutput\\n```\\n{\\n  \"movie_reviews\": [\\n    {\\n      \"sentiment\": \"NEGATIVE\",\\n      \"name\": \"Her\"\\n    }\\n  ]\\n}\\n```\\nTable 4. An example of system prompting with JSON format'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n21\\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I dont need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nSystem prompts can also be really useful for safety and toxicity. To control the output, simply \\nadd an additional line to your prompt like: You should be respectful in your answer..\\nRole prompting\\nRole prompting is a technique in prompt engineering that involves assigning a specific role to \\nthe gen AI model. This can help the model to generate more relevant and informative output, \\nas the model can craft its responses to the specific role that it has been assigned.\\nFor example, you could role prompt a gen AI model to be a book editor, a kindergarten \\nteacher, or a motivational speaker. Once the model has been assigned a role, you can then \\ngive it prompts that are specific to that role. For example, you could prompt a teacher to \\ncreate a lesson plan that they can then review. Have a look into Table 5, where the model \\ntakes the role of a travel guide.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n22\\nGoal\\nAct as travel guide and provide 3 travel suggestions\\nModel\\ngemini-pro\\nTemperature\\n1\\nToken Limit\\n1024\\nTop-K\\n40\\nTop-P\\n0.8\\nPrompt\\nI want you to act as a travel guide. I will write to you \\nabout my location and you will suggest 3 places to visit near \\nme. In some cases, I will also give you the type of places I \\nwill visit.\\nMy suggestion: \"I am in Amsterdam and I want to visit \\nonly museums.\"\\nTravel Suggestions:\\nOutput\\n1. Rijksmuseum: World-renowned museum, known for Dutch masterpieces \\nlike Rembrandts The Night Watch. Dont miss the incredible collection of \\npaintings and artifacts showcasing Dutch art and history.\\n2. Van Gogh Museum: Immerse yourself in the captivating works of Vincent \\nvan Gogh. The museum houses the largest collection of his paintings and \\ndrawings, including The Starry Night and Sunflowers.\\n3. Stedelijk Museum Amsterdam: Discover modern and contemporary art \\nfrom around the world. The museums collection includes works by Picasso, \\nKandinsky, and Rietveld, housed in a striking modern building.\\nTable 5. An example of role prompting\\nDefining a role perspective for an AI model gives it a blueprint of the tone, style, and focused \\nexpertise youre looking for to improve the quality, relevance, and effectiveness of your \\noutput. \\nHere are some styles you can choose from which I find effective:\\nConfrontational, Descriptive, Direct, Formal, Humorous, Influential, Informal, \\nInspirational, Persuasive'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n23\\nLets change our prompt in Table 6 to include a humorous and inspirational style.\\nPrompt\\nI want you to act as a travel guide. I will write to you about \\nmy location and you will suggest 3 places to visit near me in \\na humorous style.\\nMy suggestion: \"I am in Manhattan.\"\\nTravel Suggestions:\\nOutput\\n1. Behold the Empire State of Mind: Ascend to the dizzying heights of the Empire \\nState Building and bask in the glory of Manhattan\\'s skyline. Prepare to feel like \\nKing Kong atop the Big Apple, minus the giant ape-sized banana.\\n2. Get Artsy-Fartsy at MoMA: Unleash your inner art aficionado at the Museum \\nof Modern Art (MoMA). Gaze upon masterpieces that will boggle your \\nmind and make you question whether your stick-figure drawings have any \\nartistic merit.\\n3. Shop Til You Drop on Fifth Avenue: Indulge in some retail therapy on the \\niconic Fifth Avenue. Brace yourself for sticker shock as you window-shop (or \\nactually shop) at designer boutiques that will make your wallet cry. But hey, \\nyoure in Manhattan, so you might as well embrace the fabulousness!\\nTable 6. An example of role prompting with a humorous tone and style\\nContextual prompting\\nBy providing contextual prompts, you can help ensure that your AI interactions are as \\nseamless and efficient as possible. The model will be able to more quickly understand your \\nrequest and be able to generate more accurate and relevant responses, as you can see in the \\nexample of Table 7.'}, {'text': \"Prompt Engineering\\nSeptember 2024\\n24\\nGoal\\nSuggest articles for a blog about retro games\\nModel\\ngemini-pro\\nTemperature\\n1\\nToken Limit\\n1024\\nTop-K\\n40\\nTop-P\\n0.8\\nPrompt\\nContext: You are writing for a blog about retro 80's arcade \\nvideo games.\\nSuggest 3 topics to write an article about with a few lines of \\ndescription of what this article should contain.\\nOutput\\n1. **The Evolution of Arcade Cabinet Design:** \\nThis article would explore the evolution of arcade cabinet designs, from the \\nearly wood and metal cabinets of the 1970s to the sleek, neon-lit designs of \\nthe 1980s and beyond.\\n2. **Blast From The Past: Iconic Arcade Games of The 80s:** \\n- This article would feature a glimpse into the history, impact, and legacy \\nof some of the most iconic and influential arcade games of the 1980s, \\nhighlighting their innovations, popular mechanics, and enduring charm.\\n3. **The Rise and Retro Revival of Pixel Art:** \\n- This article would delve into the evolution of pixel art as a defining visual \\nstyle of early arcade games, tracing its roots, exploring its techniques, and \\ndiscussing the resurgence of pixel art in modern games and digital art.\\nTable 7. An example of contextual prompting\"}, {'text': 'Prompt Engineering\\nSeptember 2024\\n25\\nStep-back prompting\\nStep-back8 prompting is a technique for improving the performance by prompting the LLM \\nto first consider a general question related to the specific task at hand, and then feeding the \\nanswer to that general question into a subsequent prompt for the specific task. This step \\nback allows the LLM to activate relevant background knowledge and reasoning processes \\nbefore attempting to solve the specific problem.\\nBy considering the broader and underlying principles, LLMs can generate more accurate and \\ninsightful responses. Step-back prompting encourages LLMs to think critically and apply their \\nknowledge in new and creative ways. It changes the final prompt doing the task by utilizing \\nmore knowledge in the LLMs parameters than would otherwise come into play when the LLM \\nis prompted directly.\\nIt can help to mitigate biases in LLM responses, by focusing on general principles instead of \\nspecific details, step-back prompting.\\nLets have a look into these examples to understand how step-back prompting can improve \\nthe results. Lets first review a traditional prompt (Table 8) before we compare it to a step \\nback prompt (Table 9):'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n26\\nGoal\\nWrite a storyline for a level of a first-person shooter video game.\\nModel\\ngemini-pro\\nTemperature\\n1\\nToken Limit\\n1024\\nTop-K\\n40\\nTop-P\\n0.8\\nPrompt\\nWrite a one paragraph storyline for a new level of a first-\\nperson shooter video game that is challenging and engaging.\\nOutput\\nThe level begins with the players squad being ambushed in a dense urban area \\nby a relentless enemy faction. Players must fight their way through a maze of \\nback alleys and dilapidated buildings, utilizing stealth and precision to eliminate \\nenemy forces and avoid detection. The player must use their environment to their \\nadvantage, setting traps, utilizing cover, and making calculated moves to survive. \\nAlong the way, they uncover intel suggesting the enemy is planning a devastating \\nattack, and the player must race against time to gather evidence and relay it back \\nto their headquarters before the enemy can carry out their plans.\\nTable 8. A traditional prompt before we compare it with a step back prompt\\nWhen you set the temperature to 1, you might end up with all kinds of creative writings for a \\nstoryline, but its also quite random and generic. So lets take a step back:'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n27\\nPrompt\\nBased on popular first-person shooter action games, what are \\n5 fictional key settings that contribute to a challenging and \\nengaging level storyline in a first-person shooter video game?\\nOutput\\n1. **Abandoned Military Base**: A sprawling, post-apocalyptic military complex \\ncrawling with mutated soldiers and rogue robots, ideal for challenging \\nfirearm combat.\\n2. **Cyberpunk City**: A neon-lit, futuristic urban environment with towering \\nskyscrapers and dense alleyways, featuring cybernetically enhanced enemies \\nand hacking mechanics.\\n3. **Alien Spaceship**: A vast alien vessel stranded on Earth, with eerie \\ncorridors, zero-gravity sections, and extraterrestrial creatures to encounter.\\n4. **Zombie-Infested Town**: A desolate town overrun by hordes of aggressive \\nzombies, featuring intense close-quarters combat and puzzle-solving to find \\nsafe passage.\\n5. **Underwater Research Facility**: A deep-sea laboratory flooded with water, \\nfilled with mutated aquatic creatures, and requiring stealth and underwater \\nexploration skills to survive.\\nTable 9. An example of prompting for self consistency\\nYeah those topics seem like a good fit for a first-person video game. Lets go back to the \\noriginal prompt, but this time we include the answer of the step back as context and see what \\nit will return.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n28\\nPrompt\\nContext: 5 engaging themes for a first person shooter video game:\\n1.\\t**Abandoned Military Base**: A sprawling, post-apocalyptic \\nmilitary complex crawling with mutated soldiers and rogue \\nrobots, ideal for challenging firearm combat.\\n2.\\t**Cyberpunk City**: A neon-lit, futuristic urban environment \\nwith towering skyscrapers and dense alleyways, featuring \\ncybernetically enhanced enemies and hacking mechanics.\\n3.\\t**Alien Spaceship**: A vast alien vessel stranded on \\nEarth, with eerie corridors, zero-gravity sections, and \\nextraterrestrial creatures to encounter.\\n4.\\t**Zombie-Infested Town**: A desolate town overrun by hordes of \\naggressive zombies, featuring intense close-quarters combat and \\npuzzle-solving to find safe passage.\\n5.\\t**Underwater Research Facility**: A deep-sea laboratory flooded \\nwith water, filled with mutated aquatic creatures, and requiring \\nstealth and underwater exploration skills to survive.\\nTake one of the themes and write a one paragraph storyline \\nfor a new level of a first-person shooter video game that is \\nchallenging and engaging.\\nOutput\\nIn the heart of a murky abyss, lies a dilapidated underwater research facility, \\nstanding as a testament to human ambition and its disastrous consequences. \\nShrouded in darkness, pulsating with the hum of malfunctioning machinery, and \\nhaunted by the echoes of drowned souls, the facility is now a breeding ground for \\na nightmare borne from the deep sea. The player, an elite marine equipped with \\nadvanced diving gear and experimental weaponry, is tasked with venturing into \\nthis aquatic realm of terror. Armed with resilience and resourcefulness, they must \\nnavigate treacherous corridors, solve cryptic puzzles, and confront gruesome sea \\nmonstrosities that lurk in the depths. From grappling with the crushing pressure \\nof the deep sea to outsmarting cunning aquatic predators, every moment in this \\nuncharted underworld tests the limits of human endurance and courage.\\nTable 10. An example of prompting for self consistency\\nThat looks like an interesting video game! By using step back prompting techniques you can \\nincrease the accuracy of your prompts.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n29\\nChain of Thought (CoT)\\nChain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities \\nof LLMs by generating intermediate reasoning steps. This helps the LLM generate more \\naccurate answers. You can combine it with few-shot prompting to get better results on more \\ncomplex tasks that require reasoning before responding as its a challenge with a zero-shot \\nchain of thought.\\nCoT has a lot of advantages. First of all, its low-effort while being very effective and works \\nwell with off-the-shelf LLMs (so no need to finetune). You also get interpretability with CoT \\nprompting, as you can learn from the LLMs responses and see the reasoning steps that were \\nfollowed. If theres a malfunction, you will be able to identify it. Chain of thought appears \\nto improve robustness when moving between different LLM versions. Which means the \\nperformance of your prompt should drift less between different LLMs than if your prompt \\ndoes not use reasoning chains. Of course there are also disadvantages, but they are \\nsomewhat intuitive.\\nThe LLM response includes the chain of thought reasoning, which means more output \\ntokens, which means predictions cost more money and take longer.\\nTo explain the following example in Table 11, lets first try to create a prompt that is not using \\nCoT prompting to showcase the flaws of a large language model.\\nPrompt\\nWhen I was 3 years old, my partner was 3 times my age. Now, I \\nam 20 years old. How old is my partner?\\nOutput\\n63 years old\\nTable 11. An example of a prompt which is trying to solve a mathematical problem'}, {'text': \"Prompt Engineering\\nSeptember 2024\\n30\\nYikes. Thats obviously the wrong answer. As a matter of fact, LLMs often struggle with \\nmathematical tasks and can provide incorrect answers  even for a task as simple as \\nmultiplying two numbers. This is because they are trained on large volumes of text and math \\nmay require a different approach. So lets see if intermediate reasoning steps will improve \\nthe output.\\nPrompt\\nWhen I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nOutput\\n1. Lets assume my current age is x years.\\n2. When I was 3 years old, my age was 3 years.\\n3. My partners age at that time was 3 times my age, which means my partners \\nage was 3 * 3 = 9 years.\\n4. Now, I am 20 years old, which means my age has increased by 20 - 3 = 17 \\nyears since I was 3 years old.\\n5. Since my partners age was 9 years when I was 3 years old, and my age has \\nincreased by 17 years, my partners age must have also increased by 17 years.\\n6. Therefore, my partners current age is 9 + 17 = 26 years.\\nSo, my partner is 26 years old.\\nTable 12. An example of Chain of Thought prompting\\nAlright, now the final answer is correct. Thats because we clearly instructed the LLM to \\nexplain each step instead of just returning an answer. Its interesting to see that the model \\nadds up the 17 increased years. In my head I would take the difference of years between my \\npartner and me and add those up. (20+(9-3)). Lets help the model to think a little bit more \\nlike me.\"}, {'text': \"Prompt Engineering\\nSeptember 2024\\n31\\nTable 12 is an example of zero-shot Chain of thought. Chain of thought prompting can be \\nvery powerful when combined with a single-shot or few-shot, as what you can see in Table 13:\\nPrompt\\nQ: When my brother was 2 years old, I was double his age. Now \\nI am 40 years old. How old is my brother? Let's think step \\nby step.\\nA: When my brother was 2 years, I was 2 * 2 = 4 years old. \\nThat's an age difference of 2 years and I am older. Now I am 40 \\nyears old, so my brother is 40 - 2  = 38 years old. The answer \\nis 38.\\nQ: When I was 3 years old, my partner was 3 times my age. Now, \\nI am 20 years old. How old is my partner? Let's think step \\nby step.\\nA:\\nOutput\\nWhen I was 3 years old, my partner was 3 * 3 = 9 years old. Thats an age \\ndifference of 6 years and my partner is older. Now I am 20 years old, so my \\npartner is 20 + 6 = 26 years old. The answer is 26.\\nTable 13. An example of chain of thought prompting with a single-shot\\nChain of thought can be useful for various use-cases. Think of code generation, for breaking \\ndown the request into a few steps, and mapping those to specific lines of code. Or for \\ncreating synthetic data when you have some kind of seed like The product is called XYZ, \\nwrite a description guiding the model through the assumptions you would make based on \\nthe product given title. Generally, any task that can be solved by talking through is a good \\ncandidate for a chain of thought. If you can explain the steps to solve the problem, try chain \\nof thought.\\nPlease refer to the notebook10 hosted in the GoogleCloudPlatform Github repository which \\nwill go into further detail on CoT prompting:\"}, {'text': 'Prompt Engineering\\nSeptember 2024\\n32\\nIn the best practices section of this chapter, we will learn some best practices specific to \\nChain of thought prompting.\\nSelf-consistency\\nWhile large language models have shown impressive success in various NLP tasks, their \\nability to reason is often seen as a limitation that cannot be overcome solely by increasing \\nmodel size. As we learned in the previous Chain of Thought prompting section, the model can \\nbe prompted to generate reasoning steps like a human solving a problem. However CoT uses \\na simple greedy decoding strategy, limiting its effectiveness. Self-consistency11 combines \\nsampling and majority voting to generate diverse reasoning paths and select the most \\nconsistent answer. It improves the accuracy and coherence of responses generated by LLMs.\\nSelf-consistency gives a pseudo-probability likelihood of an answer being correct, but \\nobviously has high costs.\\nIt follows the following steps:\\n1. Generating diverse reasoning paths: The LLM is provided with the same prompt multiple \\ntimes. A high temperature setting encourages the model to generate different reasoning \\npaths and perspectives on the problem.\\n2. Extract the answer from each generated response.\\n3. Choose the most common answer.\\nLets look into an example of an email classification system, which classifies an email as \\nIMPORTANT or NOT IMPORTANT. A zero-shot chain of thought prompt will be sent to the LLM \\nmultiple times, to see if the responses differ after each submit. Notice the friendly tone, the \\nword choice and the sarcasm thats been used in the email. All this could trick the LLM.'}, {'text': \"Prompt Engineering\\nSeptember 2024\\n33\\nPrompt\\nEMAIL:\\n```\\nHi,\\nI have seen you use Wordpress for your website. A great open \\nsource content management system. I have used it in the past \\ntoo. It comes with lots of great user plugins. And it's pretty \\neasy to set up.\\nI did notice a bug in the contact form, which happens when \\nyou select the name field. See the attached screenshot of me \\nentering text in the name field. Notice the JavaScript alert \\nbox that I inv0k3d.\\nBut for the rest it's a great website. I enjoy reading it. Feel \\nfree to leave the bug in the website, because it gives me more \\ninteresting things to read.\\nCheers,\\nHarry the Hacker.\\n```\\nClassify the above email as IMPORTANT or NOT IMPORTANT. Let's \\nthink step by step and explain why.\\nContinues next page...\"}, {'text': 'Prompt Engineering\\nSeptember 2024\\n34\\nOutput \\nAttempt 1\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form \\non their website. The sender also mentions that they enjoy reading the website \\nand that they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the senders identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender \\nis a malicious actor who is trying to trick the recipient into clicking on a malicious \\nlink or downloading a malicious file.\\n**Conclusion: IMPORTANT**\\nBased on the potential impact of the bug and the credibility of the sender, \\nthe email should be classified as **IMPORTANT**. The recipient should take \\nimmediate action to fix the bug and protect their website from attack.\\nOutput \\nAttempt 2\\n**Explanation:**\\n1. **Lack of urgency:** The email does not convey any sense of urgency or \\ntime-sensitivity. It starts with a casual greeting and presents the information \\nin a matter-of-fact tone.\\n2. **Non-critical bug report:** The bug reported in the contact form is \\ndescribed as interesting but not critical. The email sender even suggests that \\nthe website owner can leave the bug in place.\\n3. **Lack of personal impact:** The email does not indicate any direct or \\npersonal impact on the website owner. It simply mentions that the sender \\nenjoys reading the website.\\nContinues next page...'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n35\\nOutput \\nAttempt 2\\n4. **Absence of action request:** The email does not explicitly request any \\naction from the website owner. It does not ask for the bug to be fixed or \\nprovide any specific suggestions for improvement.\\n5. **Senders intent:** The senders intention seems to be more focused on \\nsharing an observation and expressing interest in the website rather than \\nraising a serious concern or requesting assistance.\\n**Conclusion: NOT IMPORTANT**\\nOutput \\nAttempt 3\\n**Step 1: Identify the purpose of the email.**\\nThe purpose of the email is to inform the recipient of a bug in the contact form on \\ntheir website. The sender also mentions that they enjoy reading the website and \\nthat they are leaving the bug in place for the time being.\\n**Step 2: Assess the potential impact of the bug.**\\nThe bug in the contact form could allow an attacker to execute arbitrary \\nJavaScript code on the website. This could lead to the attacker being able to take \\ncontrol of the website, steal sensitive information, or deface the website.\\n**Step 3: Consider the credibility of the sender.**\\nThe sender of the email claims to be a hacker named Harry. However, there is no \\nway to verify the senders identity. It is possible that the sender is a legitimate \\nhacker who is trying to help the recipient, but it is also possible that the sender is \\na malicious actor who is trying to exploit the bug for their own gain.\\n**Conclusion:**\\nBased on the above factors, the email should be classified as **IMPORTANT**. \\nThe bug in the contact form is a serious security risk, and the senders credibility \\nis unknown. The recipient should take immediate action to fix the bug and \\ni\\ni\\nh\\nd\\nl i\\nTable 14. An example of prompting for self consistency\\nYou can use the above prompt and try to see if it returns a consistent classification. \\nDepending on the model that you are using and the temperature configuration, it might \\nreturn IMPORTANT or NOT IMPORTANT.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n36\\nBy generating many Chains of Thoughts, and taking the most commonly occurring answer \\n(IMPORTANT), we can get a more consistently correct answer from the LLM.\\nThis example shows how self-consistency prompting can be used to improve the accuracy \\nof an LLMs response by considering multiple perspectives and selecting the most \\nconsistent answer.\\nTree of Thoughts (ToT)\\nNow that we are familiar with chain of thought and self-consistency prompting, lets review \\nTree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs \\nto explore multiple different reasoning paths simultaneously, rather than just following a \\nsingle linear chain of thought. This is depicted in Figure 1.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n37\\nFigure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on \\nthe right\\nThis approach makes ToT particularly well-suited for complex tasks that require exploration. It \\nworks by maintaining a tree of thoughts, where each thought represents a coherent language \\nsequence that serves as an intermediate step toward solving a problem. The model can then \\nexplore different reasoning paths by branching out from different nodes in the tree. \\nTheres a great notebook, which goes into a bit more detail showing The Tree of Thought \\n(ToT) which is based on the paper Large Language Model Guided Tree-of-Thought.9\\nReAct (reason & act)\\nReason and act (ReAct) [10]13 prompting is a paradigm for enabling LLMs to solve complex \\ntasks using natural language reasoning combined with external tools (search, code \\ninterpreter etc.) allowing the LLM to perform certain actions, such as interacting with external \\nAPIs to retrieve information which is a first step towards agent modeling.\\nReAct mimics how humans operate in the real world, as we reason verbally and can \\ntake actions to gain information. ReAct performs well against other prompt engineering \\napproaches in a variety of domains.\\nReAct prompting works by combining reasoning and acting into a thought-action loop. The \\nLLM first reasons about the problem and generates a plan of action. It then performs the \\nactions in the plan and observes the results. The LLM then uses the observations to update \\nits reasoning and generate a new plan of action. This process continues until the LLM \\nreaches a solution to the problem.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n38\\nTo see this in action, you need to write some code. In code Snippet 1 I am using the langchain \\nframework for Python, together with VertexAI (google-cloud-aiplatform) and the \\ngoogle-search-results pip packages.\\nTo run this sample you must create a (free) SerpAPI key from https://serpapi.com/manage-\\napi-key and set an environment variable SERPAPI_API_KEY.\\nNext lets write some Python code, with the task for the LLM to figure out: How many children \\nhave a famous dad that performs in the band Metallica.\\nPython\\nfrom langchain.agents import load_tools\\nfrom langchain.agents import initialize_agent\\nfrom langchain.agents import AgentType\\nfrom langchain.llms import VertexAI\\nprompt = \"How many kids do the band members of Metallica have?\"\\nllm = VertexAI(temperature=0.1)\\ntools = load_tools([\"serpapi\"], llm=llm)\\nagent = initialize_agent(tools, llm,  \\nagent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\nagent.run(prompt)\\nSnippet 1. Creating a ReAct Agent with LangChain and VertexAI\\nCode Snippet 2 shows the result. Notice that ReAct makes a chain of five searches. In fact, \\nthe LLM is scraping Google search results to figure out the band names. Then, it lists the \\nresults as observations and chains the thought for the next search.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n39\\nUnset\\n> Entering new AgentExecutor chain...\\nMetallica has 4 members. \\nAction: Search\\nAction Input: How many kids does James Hetfield have?\\nObservation: three children\\nThought: 1/4 Metallica band members have 3 children\\nAction: Search\\nAction Input: How many kids does Lars Ulrich have?\\nObservation: 3\\nThought: 2/4 Metallica band members have 6 children\\nAction: Search\\nAction Input: How many kids does Kirk Hammett have?\\nObservation: Hammett has been married to his second wife Lani since 1998. \\nThey have two sons, Angel (b. September 29, 2006) and Vincenzo (b. June \\n28, 2008).\\nThought: 3/4 Metallica band members have 8 children\\nAction: Search\\nAction Input: How many kids does Robert Trujillo have?\\nObservation: 2\\nThought: 4/4 Metallica band members have 10 children\\nFinal Answer: 10\\n Snippet 2. Executing ReAct Agent in the terminal\\nCode Snippet 2 figures out that the band Metallica has four band members. Then it searches \\neach band member to request the total of children and add up the total. Finally, it returns the \\ntotal number of children as the final answer.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n40\\nReAct prompting in practice requires understanding that you continually have to resend the \\nprevious prompts/responses (and do trimming of the extra generated content) as well as set \\nup the model with appropriate examples/instructions. Please refer to the notebook14 hosted \\nin the GoogleCloudPlatform Github repository, which goes into a bit more detail showing the \\nactual LLM inputs and outputs with a more elaborate example\\nAutomatic Prompt Engineering\\nAt this point you might realize that writing a prompt can be complex. Wouldnt it be nice to \\nautomate this (write a prompt to write prompts)? Well, theres a method: Automatic Prompt \\nEngineering (APE). This method15 not only alleviates the need for human input but also \\nenhances the models performance in various tasks.\\nYou will prompt a model to generate more prompts. Evaluate them, possibly alter the good \\nones. And repeat.\\nFor example, you can use automatic prompt engineering that will help with training a chatbot \\nfor a merchandise t-shirt webshop. We want to figure out all the various ways customers \\ncould phrase their order for buying a band merchandise t-shirt.\\n1. Write the prompt which will generate the output variants. In this example, I am using \\ngemini-pro to generate 10 instructions. See Table 15:'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n41\\nPrompt\\nWe have a band merchandise t-shirt webshop, and to train a \\nchatbot we need various ways to order: \"One Metallica t-shirt \\nsize S\". Generate 10 variants, with the same semantics but keep \\nthe same meaning.\\nOutput\\n1. Id like to purchase a Metallica t-shirt in size small.\\n2. Can I order a small-sized Metallica t-shirt?\\n3. Id like to buy one Metallica t-shirt in small.\\n4. One Metallica shirt, size small, please.\\n5. I want one Metallica t-shirt in size small.\\n6. Id like to place an order for one Metallica t-shirt in small.\\n7. Small Metallica t-shirt, one please.\\n8. Im looking for one Metallica t-shirt in small.\\n9. Can you get me one Metallica t-shirt in small?\\n10. One Metallica small t-shirt, please.\\nTable 15. Use an LLM to generate prompts\\n2. Evaluate all of the instruction candidates by scoring the candidates based on a chosen \\nmetric, For example, you can use. BLEU (Bilingual Evaluation Understudy) or ROUGE \\n(Recall-Oriented Understudy for Gisting Evaluation).\\n3. Select the instruction candidate with the highest evaluation score. This candidate will be \\nthe final prompt you can use in your software application or chatbot. You can also tweak \\nthe select prompt and evaluate again.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n42\\nCode prompting\\nGemini focuses primarily on text-based prompts, which also includes writing prompts for \\nreturning code. Lets go to the Vertex AI Studio and test these prompts to look at some \\ncoding examples.\\nPrompts for writing code\\nGemini can also be a developer and help you with writing code in any programming language \\nof choice. As a developer this can help you speed up the process of writing code.\\nImagine a folder on your machine with hundreds of files that needs to be renamed. \\nRenaming each file would take you a lot of time. You know a little Bash, and could write \\na script to automate this, but that may also take awhile. So lets write a prompt. You can \\nwrite the prompt in the public consumer chatbot Gemini, or if you are more concerned \\nabout confidentiality, you can write these prompts within your Google Cloud account and \\nopen Vertex AI Studio. The advantage of Vertex AI Studio is that you can configure the \\ntemperature etc.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n43\\nName\\n1_bash_rename_files_in_folder\\nGoal\\nWrite a prompt to write code in Bash to rename files in a folder.\\nModel\\ngemini-pro\\nTemperature\\n0.1\\nToken Limit\\n1024\\nTop-K\\nN/A\\nTop-P\\n1\\nPrompt\\nWrite a code snippet in Bash, which asks for a folder name. \\nThen it takes the contents of the folder and renames all the \\nfiles inside by prepending the name draft to the file name.\\nOutput\\n```bash\\n#!/bin/bash\\n# Ask for the folder name\\necho \"Enter the folder name: \"\\nread folder_name\\n# Check if the folder exists\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\n# Get the list of files in the folder\\nfiles=( \"$folder_name\"/* )\\n# Rename each file by prepending \"draft\" to the file name\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nTable 16. Using Gemini to program code\\nThat seems like good code to me - its even documented! However, since LLMs cant reason, \\nand repeat training data, its essential to read and test your code first. \\nThe moment we are all waiting for, does it really work?'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n44\\nLets try it first with a test folder with only a few files in it, which will need to be renamed from \\nfilename.txt to draft_filename.txt. \\n1. Copy the output from Table 16 (without the ```bash ``` text wrapper), and paste it in a new \\nfile called: rename_files.sh. \\n2. Open a terminal window and type: . rename_files.sh. It will ask to enter a folder name, e.g. \\ntest. and hit enter. \\n3. The script seems to run fine. You will see the message: Files renamed successfully. \\nWhen you look into the test folder, notice that all the files were perfectly renamed to \\ndraft_filename.txt. \\nIt worked!\\nPrompts for explaining code\\nAs a developer when you work in teams you have to read someone elses code. Gemini can \\nhelp you with this as well. Lets take the code output from Table 16, remove the comments \\nand ask the large language model to explain whats happening, See Table 17:'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n45\\nName\\n1_bash_explain_the_code\\nGoal\\nWrite a prompt to explain Bash code\\nModel\\ngemini-pro\\nTemperature\\n0.1\\nToken Limit\\n1024\\nTop-K\\nN/A\\nTop-P\\n1\\nPrompt\\nExplain to me the below Bash code:\\n```\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput\\nThis Bash script performs the following tasks:\\n1. **User Input**: \\n- It prompts the user to enter a folder name using the `echo` and `read` \\ncommands. The entered folder name is stored in the `folder_name` variable.\\n2. **Folder Existence Check**: \\n- It checks if the folder with the specified name exists using the `[ ! -d \\n$folder_name ]` condition. If the folder does not exist, it displays an error \\nmessage (Folder does not exist.) and exits the script with an exit code of 1.\\nContinues next page...'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n46\\nOutput\\n3. **File Listing**: \\n- It lists all the files inside the specified folder using the `files=( $folder_\\nname/* )` command. This creates an array called `files` containing the paths \\nto all the files in the folder.\\n4. **File Renaming**: \\n- It iterates through each file in the `files` array using a `for` loop. \\n- For each file, it generates a new file name using the `new_file_\\nname=draft_$(basename $file) ` command. This new file name is created \\nby prefixing the original file name with the string draft_. \\n- It then renames the file using the `mv $file $new_file_name` command, \\nwhich moves the file to the new file name.\\n5. **Success Message**: \\n- After renaming all the files, it displays a success message (Files renamed \\nsuccessfully.) to indicate that the renaming operation was successful\\nTable 17. A prompt for explaining code\\nPrompts for translating code\\nThe bash code from Table 16 seems to work fine. However this script could really be reusable \\nwhen it would prompt me for file names, ideally it should work as a separate application with \\nan UI. As a starting point, Python would be a better language for a (web) application than \\nBash. LLMs can help with translating code from one language to another. See the example in \\nTable 18:'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n47\\nName\\n1_python_rename_files\\nGoal\\nWrite a prompt to translate Bash code to Python\\nModel\\ngemini-pro\\nTemperature\\n0.1\\nToken Limit\\n1024\\nTop-K\\nN/A\\nTop-P\\n1\\nPrompt\\nTranslate the below Bash code to a Python snippet.\\n```bash\\n#!/bin/bash\\necho \"Enter the folder name: \"\\nread folder_name\\nif [ ! -d \"$folder_name\" ]; then\\necho \"Folder does not exist.\"\\nexit 1\\nfi\\nfiles=( \"$folder_name\"/* )\\nfor file in \"${files[@]}\"; do\\nnew_file_name=\"draft_$(basename \"$file\")\"\\nmv \"$file\" \"$new_file_name\"\\ndone\\necho \"Files renamed successfully.\"\\n```\\nOutput\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\nprint(\"Folder does not exist.\")\\n    exit(1)\\nContinues next page...'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n48\\nOutput\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_file_name = f\"draft_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nTable 18. A prompt for translating code from Bash to Python\\nRead and review the code. Copy the output from the prompt and paste it into a new file: \\nfile_renamer.py. Test the code by opening a Terminal window, and execute the following \\ncommand python file_renamer.py. \\nNOTE: When prompting for (Python) code in the Language Studio in Vertex AI, you will have \\nto click on the Markdown button. Otherwise you will receive plain text which is missing the \\nproper indenting of lines, which is important for running Python code.\\nPrompts for debugging and reviewing code\\nLets manually write some edits to the code of Table 18. It should prompt the user for the \\nfilename prefix, and write this prefix in upper case characters. See the example code in \\nSnippet *3, but what a bummer. It now returns Python errors!'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n49\\nPython\\nimport os\\nimport shutil\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\nfiles = os.listdir(folder_name)\\nfor file in files:\\n    new_filename = f\"{text}_{file}\"\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\nprint(\"Files renamed successfully.\")\\nSnippet 3. A broken Python script\\nDoh! That looks like a bug:\\nUnset\\nThe below Python code gives an error:\\nTraceback (most recent call last):\\n  File /Users/leeboonstra/Documents/test_folder/rename_files.py, line 7, \\nin <module>\\n    text = toUpperCase(prefix)\\nNameError: name toUpperCase is not defined\\nSnippet 4. I broke the Python code'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n50\\nLets see if we can ask the large language model to debug and review code. Have a look into \\nTable 19:\\nName\\n1_python_debug_code\\nGoal\\nWrite a prompt to debug and review Python code.\\nModel\\ngemini-pro\\nTemperature\\n0.1\\nToken Limit\\n1024\\nTop-K\\nN/A\\nTop-P\\n1\\nPrompt\\nThe below Python code gives an error:\\nTraceback (most recent call last):\\n                                                   File \"/\\nUsers/leeboonstra/Documents/test_folder/rename_files.py\", line \\n7, in <module>\\n    text = toUpperCase(prefix)\\nNameError: name \\'toUpperCase\\' is not defined\\nDebug what\\'s wrong and explain how I can improve the code.\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = toUpperCase(prefix)\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n51\\nPrompt\\n# Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file), \\nos.path.join(folder_name, new_file_name))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nOutput\\nThe code calls the `toUpperCase` function to convert `prefix` to uppercase, but \\nthat function is not defined. To fix this issue, you can use the `upper()` method of \\nthe string class which converts a given string into uppercase. The modified code \\nis shown below:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\ntext = prefix.upper()\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\n    # Create a new file name with the prefix \"draft_\"\\n    new_filename = f\"{text}_{file}\"\\n# Move the file to the new name\\nshutil.move(os.path.join(folder_name, file),\\nContinues next page...'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n52\\nOutput\\nos.path.join(folder_name, new_filename))\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nAdditionally, there are a few other improvements that can be made to the code:\\n1. The file extension of the new filenames is not kept. Its better to extract the \\nextension from the original filename and append it to the new filename.\\n2. The code assumes that there are no spaces in the folder name. However, \\nit is a good practice to handle spaces gracefully by replacing them with \\nunderscores or hyphens.\\n3. Using the `f` string syntax for string interpolation is more readable and \\nconcise than the traditional `+` operator.\\n4. The code doesnt handle errors that might occur during the renaming process. \\nIt would be better to wrap the `shutil.move` call in a `try...except` block to \\ncatch any potential errors.\\nHere is the improved code with these suggestions:\\n```python\\nimport os\\nimport shutil\\n# Get the folder name from the user\\nfolder_name = input(\"Enter the folder name: \")\\nprefix = input(\"Enter the string to prepend to the filename: \")\\n# Check if the folder exists\\nif not os.path.isdir(folder_name):\\n    print(\"Folder does not exist.\")\\n    exit(1)\\n# Get a list of all files in the folder\\nfiles = os.listdir(folder_name)\\n# Iterate over the files\\nfor file in files:\\nContinues next page...'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n53\\nOutput\\n    # Get the file extension\\n    ext = os.path.splitext(file)[1]\\n# Create a new file name with the prefix and the file extension\\n    new_filename = f\"{prefix}_{os.path.splitext(file)[0]}{ext}\"\\n# Build the full paths for the original and new files\\n    old_path = os.path.join(folder_name, file)\\n    new_path = os.path.join(folder_name, new_filename)\\n# Try to move the file\\n    try:\\n        shutil.move(old_path, new_path)\\nexcept Exception as e:\\n        print(f\"An error occurred while moving the file \\n{file}: {e}\")\\n# Print a success message\\nprint(\"Files renamed successfully.\")\\n```\\nThese improvements make the code more robust and flexible while also making \\nit easier to read and understand... (The response was truncated because \\nit has reached the token limit. Try to increase the token limit if you need a \\nlonger response.)\\nTable 19. A prompt for debugging and reviewing Python code\\nThats great. Not only did it tell me how to solve the problem, it also figured out that my code \\nhad more bugs and how to solve them, too. The last part of the prompt gave suggestions to \\nimprove the code in general.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n54\\nWhat about multimodal prompting?\\nPrompting for code still uses the same regular large language model. Multimodal prompting \\nis a separate concern, it refers to a technique where you use multiple input formats to guide \\na large language model, instead of just relying on text. This can include combinations of text, \\nimages, audio, code, or even other formats, depending on the models capabilities and the \\ntask at hand. \\nBest Practices\\nFinding the right prompt requires tinkering. Language Studio in Vertex AI is a perfect place to \\nplay around with your prompts, with the ability to test against the various models.\\nUse the following best practices to become a pro in prompt engineering.\\nProvide examples\\nThe most important best practice is to provide (one shot / few shot) examples within a \\nprompt. This is highly effective because it acts as a powerful teaching tool. These examples \\nshowcase desired outputs or similar responses, allowing the model to learn from them \\nand tailor its own generation accordingly. Its like giving the model a reference point or \\ntarget to aim for, improving the accuracy, style, and tone of its response to better match \\nyour expectations.'}, {'text': \"Prompt Engineering\\nSeptember 2024\\n55\\nDesign with simplicity\\nPrompts should be concise, clear, and easy to understand for both you and the model. As a \\nrule of thumb, if its already confusing for you it will likely be also confusing for the model. Try \\nnot to use complex language and dont provide unnecessary information. \\nExamples:\\nBEFORE:\\nI am visiting New York right now, and I'd like to hear more about great \\nlocations. I am with two 3 year old kids. Where should we go during \\nour vacation?\\nAFTER REWRITE:\\nAct as a travel guide for tourists. Describe great places to visit in \\nNew York Manhattan with a 3 year old.\\nTry using verbs that describe the action. Heres a set of examples:\\nAct, Analyze, Categorize, Classify, Contrast, Compare, Create, Describe, Define, \\nEvaluate, Extract, Find, Generate, Identify, List, Measure, Organize, Parse, Pick, \\nPredict, Provide, Rank, Recommend, Return, Retrieve, Rewrite, Select, Show, Sort, \\nSummarize, Translate, Write.\"}, {'text': 'Prompt Engineering\\nSeptember 2024\\n56\\nBe specific about the output\\nBe specific about the desired output. A concise instruction might not guide the LLM enough \\nor could be too generic. Providing specific details in the prompt (through system or context \\nprompting) can help the model to focus on whats relevant, improving the overall accuracy.\\nExamples:\\nDO: \\nGenerate a 3 paragraph blog post about the top 5 video game consoles. \\nThe blog post should be informative and engaging, and it should be \\nwritten in a conversational style.\\nDO NOT: \\nGenerate a blog post about video game consoles.\\nUse Instructions over Constraints\\nInstructions and constraints are used in prompting to guide the output of a LLM. \\n\\t An instruction provides explicit instructions on the desired format, style, or content of the \\nresponse. It guides the model on what the model should do or produce.\\n\\t A constraint is a set of limitations or boundaries on the response. It limits what the model \\nshould not do or avoid.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n57\\nGrowing research suggests that focusing on positive instructions in prompting can be more \\neffective than relying heavily on constraints. This approach aligns with how humans prefer \\npositive instructions over lists of what not to do. \\nInstructions directly communicate the desired outcome, whereas constraints might leave the \\nmodel guessing about what is allowed. It gives flexibility and encourages creativity within the \\ndefined boundaries, while constraints can limit the models potential. Also a list of constraints \\ncan clash with each other.\\nConstraints are still valuable but in certain situations. To prevent the model from generating \\nharmful or biased content or when a strict output format or style is needed.\\nIf possible, use positive instructions: instead of telling the model what not to do, tell it what to \\ndo instead. This can avoid confusion and improve the accuracy of the output. \\nDO: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nOnly discuss the console, the company who made it, the year, and total \\nsales. \\nDO NOT: \\nGenerate a 1 paragraph blog post about the top 5 video game consoles. \\nDo not list video game names.\\nAs a best practice, start by prioritizing instructions, clearly stating what you want the model \\nto do and only use constraints when necessary for safety, clarity or specific requirements. \\nExperiment and iterate to test different combinations of instructions and constraints to find \\nwhat works best for your specific tasks, and document these.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n58\\nControl the max token length\\nTo control the length of a generated LLM response, you can either set a max token limit in the \\nconfiguration or explicitly request a specific length in your prompt. For example:\\n\"Explain quantum physics in a tweet length message.\"\\nUse variables in prompts\\nTo reuse prompts and make it more dynamic use variables in the prompt, which can be \\nchanged for different inputs. E.g. as shown in Table 20, a prompt which gives facts about a \\ncity. Instead of hardcoding the city name in the prompt, use a variable. Variables can save \\nyou time and effort by allowing you to avoid repeating yourself. If you need to use the same \\npiece of information in multiple prompts, you can store it in a variable and then reference \\nthat variable in each prompt. This makes a lot of sense when integrating prompts into your \\nown applications.\\nPrompt\\nVARIABLES\\n{city} = \"Amsterdam\"\\nPROMPT\\nYou are a travel guide. Tell me a fact about the city: {city}\\nOutput\\nAmsterdam is a beautiful city full of canals, bridges, and narrow streets. Its a \\ngreat place to visit for its rich history, culture, and nightlife.\\nTable 20. Using variables in prompts'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n59\\nExperiment with input formats and writing styles\\nDifferent models, model configurations, prompt formats, word choices, and submits can yield \\ndifferent results. Therefore, its important to experiment with prompt attributes like the style, \\nthe word choice, and the type prompt (zero shot, few shot, system prompt).\\nFor example a prompt with the goal to generate text about the revolutionary video game \\nconsole Sega Dreamcast, can be formulated as a question, a statement or an instruction, \\nresulting in different outputs:\\n\\t Question: What was the Sega Dreamcast and why was it such a revolutionary console?\\n\\t Statement: The Sega Dreamcast was a sixth-generation video game console released by \\nSega in 1999. It...\\n\\t Instruction: Write a single paragraph that describes the Sega Dreamcast console and \\nexplains why it was so revolutionary.\\nFor few-shot prompting with classification tasks, mix up \\nthe classes\\nGenerally speaking, the order of your few-shots examples should not matter much. However, \\nwhen doing classification tasks, make sure you mix up the possible response classes in the \\nfew shot examples. This is because you might otherwise be overfitting to the specific order of \\nthe examples. By mixing up the possible response classes, you can ensure that the model is \\nlearning to identify the key features of each class, rather than simply memorizing the order of \\nthe examples. This will lead to more robust and generalizable performance on unseen data.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n60\\nA good rule of thumb is to start with 6 few shot examples and start testing the accuracy \\nfrom there.\\nAdapt to model updates\\nIts important for you to stay on top of model architecture changes, added data, and \\ncapabilities. Try out newer model versions and adjust your prompts to better leverage new \\nmodel features. Tools like Vertex AI Studio are great to store, test, and document the various \\nversions of your prompt.\\nExperiment with output formats\\nBesides the prompt input format, consider experimenting with the output format. For non-\\ncreative tasks like extracting, selecting, parsing, ordering, ranking, or categorizing data try \\nhaving your output returned in a structured format like JSON or XML. \\nThere are some benefits in returning JSON objects from a prompt that extracts data. In \\na real-world application I dont need to manually create this JSON format, I can already \\nreturn the data in a sorted order (very handy when working with datetime objects), but most \\nimportantly, by prompting for a JSON format it forces the model to create a structure and \\nlimit hallucinations.\\nTable 4 in the few-shot prompting section shows an example on how to return \\nstructured output.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n61\\nExperiment together with other prompt engineers\\nIf you are in a situation where you have to try to come up with a good prompt, you might \\nwant to find multiple people to make an attempt. When everyone follows the best practices \\n(as listed in this chapter) you are going to see a variance in performance between all the \\ndifferent prompt attempts.\\nCoT Best practices\\nFor CoT prompting, putting the answer after the reasoning is required because the \\ngeneration of the reasoning changes the tokens that the model gets when it predicts the \\nfinal answer.\\nWith CoT and self-consistency you need to be able to extract the final answer from your \\nprompt, separated from the reasoning. \\nFor CoT prompting, set the temperature to 0.\\nChain of thought prompting is based on greedy decoding, predicting the next word in a \\nsequence based on the highest probability assigned by the language model. Generally \\nspeaking, when using reasoning, to come up with the final answer, theres likely one single \\ncorrect answer. Therefore the temperature should always set to 0.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n62\\nDocument the various prompt attempts\\nThe last tip was mentioned before in this chapter, but we cant stress enough how important \\nit is: document your prompt attempts in full detail so you can learn over time what went well \\nand what did not. \\nPrompt outputs can differ across models, across sampling settings, and even across different \\nversions of the same model. Moreover, even across identical prompts to the same model, \\nsmall differences in output sentence formatting and word choice can occur. (For example, as \\nmentioned previously, if two tokens have the same predicted probability, ties may be broken \\nrandomly. This can then impact subsequent predicted tokens.).\\nWe recommend creating a Google Sheet with Table 21 as a template. The advantages of \\nthis approach are that you have a complete record when you inevitably have to revisit your \\nprompting workeither to pick it up in the future (youd be surprised how much you can \\nforget after just a short break), to test prompt performance on different versions of a model, \\nand to help debug future errors.\\nBeyond the fields in this table, its also helpful to track the version of the prompt (iteration), \\na field to capture if the result was OK/NOT OK/SOMETIMES OK, and a field to capture \\nfeedback. If youre lucky enough to be using Vertex AI Studio, save your prompts (using the \\nsame name and version as listed in your documentation) and track the hyperlink to the saved \\nprompt in the table. This way, youre always one click away from re-running your prompts.\\nWhen working on a retrieval augmented generation system, you should also capture the \\nspecific aspects of the RAG system that impact what content was inserted into the prompt, \\nincluding the query, chunk settings, chunk output, and other information.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n63\\nOnce you feel the prompt is close to perfect, take it to your project codebase. And in the \\ncodebase, save prompts in a separate file from code, so its easier to maintain. Finally, ideally \\nyour prompts are part of an operationalized system, and as a prompt engineer you should \\nrely on automated tests and evaluation procedures to understand how well your prompt \\ngeneralizes to a task.\\nPrompt engineering is an iterative process. Craft and test different prompts, analyze, \\nand document the results. Refine your prompt based on the models performance. Keep \\nexperimenting until you achieve the desired output. When you change a model or model \\nconfiguration, go back and keep experimenting with the previously used prompts.\\nName\\n[name and version of your prompt]\\nGoal\\n[One sentence explanation of the goal of this attempt]\\nModel\\n[name and version of the used model]\\nTemperature\\n[value between 0 - 1]\\nToken Limit\\n[number]\\nTop-K\\n[number]\\nTop-P\\n[number]\\nPrompt\\n[Write all the full prompt]\\nOutput\\n[Write out the output or multiple outputs]\\nTable 21. A template for documenting prompts\\nSummary\\nThis whitepaper discusses prompt engineering. We learned various prompting techniques, \\nsuch as:\\n\\t Zero prompting\\n\\t Few shot prompting'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n64\\n\\t System prompting\\n\\t Role prompting\\n\\t Contextual prompting\\n\\t Step-back prompting\\n\\t Chain of thought\\n\\t Self consistency\\n\\t Tree of thoughts\\n\\t ReAct\\nWe even looked into ways how you can automate your prompts. \\nThe whitepaper then discusses the challenges of gen AI like the problems that can happen \\nwhen your prompts are insufficient. We closed with best practices on how to become a better \\nprompt engineer.'}, {'text': 'Prompt Engineering\\nSeptember 2024\\n65\\nEndnotes\\n1. Google, 2023, Gemini by Google. Available at: https://gemini.google.com.\\n2. Google, 2024, Gemini for Google Workspace Prompt Guide. Available at:  \\nhttps://inthecloud.withgoogle.com/gemini-for-google-workspace-prompt-guide/dl-cd.html.\\n3. Google Cloud, 2023, Introduction to Prompting. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design.\\n4. Google Cloud, 2023, Text Model Request Body: Top-P & top-K sampling methods. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text#request_body.\\n5. Wei, J., et al., 2023, Zero Shot - Fine Tuned language models are zero shot learners. Available at:  \\nhttps://arxiv.org/pdf/2109.01652.pdf.\\n6. Google Cloud, 2023, Google Cloud Model Garden. Available at: https://cloud.google.com/model-garden.\\n7. Brown, T., et al., 2023, Few Shot - Language Models are Few Shot learners. Available at:  \\nhttps://arxiv.org/pdf/2005.14165.pdf.\\n8. Zheng, L., et al., 2023, Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. \\nAvailable at: https://openreview.net/pdf?id=3bq3jsvcQ1\\n9. Wei, J., et al., 2023, Chain of Thought Prompting. Available at: https://arxiv.org/pdf/2201.11903.pdf.\\n10. Google Cloud Platform, 2023, Chain of Thought and React. Available at: https://github.com/ \\nGoogleCloudPlatform/generative-ai/blob/main/language/prompts/examples/chain_of_thought_react.ipynb. \\n11. Wang, X., et al., 2023, Self Consistency Improves Chain of Thought reasoning in language models.  \\nAvailable at: https://arxiv.org/pdf/2203.11171.pdf.\\n12. Yao, S., et al., 2023, Tree of Thoughts: Deliberate Problem Solving with Large Language Models.  \\nAvailable at: https://arxiv.org/pdf/2305.10601.pdf.\\n13. Yao, S., et al., 2023, ReAct: Synergizing Reasoning and Acting in Language Models. Available at:  \\nhttps://arxiv.org/pdf/2210.03629.pdf.\\n14. Google Cloud Platform, 2023, Advance Prompting: Chain of Thought and React. Available at: \\nhttps://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-  \\non-vertex-ai/advanced_prompting_training/cot_react.ipynb.\\n15. Zhou, C., et al., 2023, Automatic Prompt Engineering - Large Language Models are Human-Level Prompt \\nEngineers. Available at: https://arxiv.org/pdf/2211.01910.pdf.'}]\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ERx8UMjaFwr0"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_list(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S0LY3ZEbFzRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554eae28-82a3-42af-a84f-85801f46f9ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 80\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "putj8l4BGk5h"
      },
      "source": [
        "## Lets Select the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VLCJqrMHE1D"
      },
      "source": [
        "The checkpoint is from a TinyLLaMA model trained for ~1.4M steps over 3 trillion tokens,\n",
        "captured midway before final convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_1GvuhsaF7uX"
      },
      "outputs": [],
      "source": [
        "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
        "# model_name = \"meta-llama/Llama-3.1-8B\"\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EJHok8TtHcGv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_ExND06CF7xY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288,
          "referenced_widgets": [
            "ca709c4b01da4f78bbb225630d940531",
            "f012b02d9e7c4ac1897decab613c584d",
            "052ced0cceac4314bd34c3c83676b7b2",
            "29b2a2e089d94ac683fbeec33b3f75a7",
            "b1bc7adb51ba4b469c89cfb8c62f788a",
            "2c2602c5587e464d99c1c512ea4539cb",
            "de419d2cf000408aa22fb4ee8da20550",
            "ee295af00978446b9e004c93fdd86dc3",
            "7455a32fbdfb4efe91afd123fd977e56",
            "102f7c76756d4a9d8c9246cdcf0c9f9c",
            "e490120f4a684fc0b2be33aa9c271d33",
            "01e37ac189b0477e9313b1c93d7ae8fa",
            "1713c81bc7804ebbba2d3bf388d82b7f",
            "3aa64bbe67ff41528329268269316b06",
            "b8a6660406674ffb9d6fb33a1228dc88",
            "7b17c3e5f0e64cdabb2ef4ae7f134f18",
            "0294495c8734466cb0fc0db414ec5140",
            "0c81437b3341438f932e90ba20db1323",
            "145cedc69b5c43f984ec9d4ac8e156e9",
            "4e10a4eedbeb4c68bc643341a78d41d1",
            "c8269ae409604947ae548b330251880c",
            "23a1e9a8bed141dbbaa98996f2204a6e",
            "8fa34720fe6847e38a61ab91c29fc92a",
            "c50b70b060a14a67a351c625ab46ab25",
            "24318c5de1ff43b78b83244e244f29a7",
            "de210994dbb2498db3cd30f5a505205d",
            "0ee51ab2809449d6953db7f06902963e",
            "90c253a82f7b4254bf2f64f5411e3747",
            "84b8a84596074848a0f718e831930817",
            "3ef6c7c1a46346fe849164872ca2a148",
            "04f12d3445a743b7a894e8add0c78ee0",
            "2da6d5c2930d4df88ca1105c80ea46ce",
            "421a20f3ba4248ca896e22451dec3cdd",
            "78b2d57190cf47c1bcbc249bd14536d4",
            "54e2517d071044c08a6e6527aa276a8b",
            "234c6faa4e4c4ca88d3b24dd997b215d",
            "cae25f7a21c44fe997b38f2a569d2be4",
            "34c251a2e4f345e09d9e837dfa8359d4",
            "41fee3ce63b54c2c99b426abc2918be5",
            "1088ffa1ae3047599e95545a8c33a7be",
            "bd1c266ae8c645be9999fbbf380da595",
            "5072dcdf7305489f83f513def3c4185d",
            "b48bc19bad4a4a96ad66f67faaaa8b09",
            "fc8d7d455e7943b5bf20b7d6a098ae0c"
          ]
        },
        "outputId": "ffcda3b2-f07f-45f2-c8a6-424ad0d76929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca709c4b01da4f78bbb225630d940531"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "01e37ac189b0477e9313b1c93d7ae8fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fa34720fe6847e38a61ab91c29fc92a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78b2d57190cf47c1bcbc249bd14536d4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6jLsr6TSHsk-"
      },
      "outputs": [],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qCOp5vqHxW9"
      },
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoMAAAGiCAYAAACcdsyAAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAJikSURBVHhe7N15eBRVvvj/dy/pTifp7EBWEghLIDFhE1FZwuKCbI4igow4eqMoMCijM+Mdrz++XGcUZlQUwQFEx4FRFhFEL4qICIKCiOyBoCYkhJBAFpJ00p1Ob78/SMp0J0ASEhb783qeep6kzqnqU1Wnqj99zqkq1YABA1zU8/L8V7h//IT6s4QQQgghxK+U2nOGEEIIIYTwHhIMCiGEEEJ4MVVj3cTPzHq6/iwhhBBCCPErJS2DQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwotJMCiEEEII4cVUAwYMcNWf8fL8V3hm1tP1ZzXqb//1veesC3ru7Rs9ZwkhhBBCiGvAZQWD89fe4Dm7gVnjD0swKIQQQghxjWrVYFCTOAmNMQJUKmVeaucSdh9r75avARXoDAYcptMUf/Mfz1QhhBBCCNFGWi0YdN28BENsNwztAtzyNZXBoMLXF0qP7uPnf830TBZCCCGEEG2g1W4gsXGJ1r9LcDhc2Bygj0r2TBIeUlJSmD9/PosWLWLo0KGeyUIIIYQQTdZqwaDLBQ6nC7u9ZZPF4sJS5cRidnquus0kJSWxYMEC5s2bR3R0tGfyNSsxMRGj0YherycpKckzWVzEhAkTWLZsGRMmTPBMumpGjBjB4sWLmTFjhmfSFTVjxgyWLl3K/PnzmT59OrGxsZ5ZhBBC/Aq1WjAIgBNwui44jeqo5p5OavSqhmk4XdhrXNisbr3WzTZ79mwWLFhwwSCp7ov3SgQD06ZN4x//+McFy9JSmZmZmEwmzGYzhw8f9kwWbaxv377MnTuXJ554wjPpuqdWqzEajfTu3Zs//vGPDBs2zDOLEEKIX5lWDQZdtS2EjU2/SdAwsZuG3yRo6B6sbpBeN7U1vV6Pqt4NLm2pXbt26PV6z9mX7dChQ8yaNYuZM2eyfft2z2TRxkJCQggODkaj0XgmXdcWLlzI9OnTWbp0KdnZ2fj6+jJmzBgSExM9swohhPgVab1g0HXhVsEx8WruSfjli1NDwzytFRGeO3cOnU5HZGSkZxIA/v7+uFwuysrKPJOE8HpWq5U9e/Ywd+5cMjMzCQgI4MYb5dFQQgjxa9ZqdxNbb1qHf2Qg+hB/t3yxRhVzB7m3jr28t4b9Z93HBqrUoFZDTWUNpxbf4ZbWHDNmzCA5OZm1a9eyZcsWz2QmTJjAsGHDWLt2LQUFBUydOhWTycSnn37KyJEjad/+/I0wWVlZLF++nIKCAmXZ2NhYxo0bR/fu3fH19cXlclFVVcX333/PqlWrcDqdTJgwgdtvv73eJzZ04MABFi5c6Dn7kmbMmEGvXr2U/+12+wW3U61WM2rUKAYPHkxQUBAqlYrKykq2bdvGJ598gtPZsrGZt9xyC8OHDycqKgofHx8cDgdFRUVs3LiRXbt2eWa/pKSkJKZOnUpJSQmbN2/mN7/5DcHBwXAZx6BOeHg448ePJykpCV9fXwAcDgdarZbNmzezZs0aJe+lREdHM3PmTMLCwjyTFGazmSVLlpCRkQG1rdDjx4/nxhtvVH6ENFbWESNGMH78eI4cOaLUC6PRyJNPPkl8fDw5OTksXryY4uJiqN2u+++/n+TkZHx8fLDZbGRlZbF69Wry8vKU8syePZuwsDBWrlzJ4MGD6dy5M2q1mrKyMtatW3fJ4zVp0iSGDx/e4voqhBDi+tDKLYMNWwfLLE6qasBU45m3kdbBuukyWK1WqA2GAB566CEWLlxIv379ADAYDDidTiwWi7KM0WjkgQceICIiArVajVqtpmvXrkycOFHJAzBu3Dh69eqFwWBApVIp46vS0tIYN26cW96r7b777mPMmDGEhISgVqtRqVQYjUaGDx9Ojx49PLM3SXR0NOPGjSMuLg4fHx8ANBoNERERTJgw4bK6E8PDw5kyZQqhoaFux+Dhhx8mIOCXxxU19RiEh4czffp0+vXrp+RVqVRotVolT1tSq9U8+uijpKWlERAQ4FbWoUOHMm3aNKWOejIajaSnpxMXF0dOTg6vv/66EggajUYef/xxevfurRwDHx8fEhMTmT59OnFxcW7r0ul0TJ48ma5du6LRaFCpVISEhHDvvfc2yFtfYmIiqampUO+cEkII8evU+LdRC7lcLlxO96mi2sWjm8w8u+2X4EvVSL7z0/l1XI7y8nK0Wi3BwcGo1Wo6duyIr6+vchOH0WjEbre7dRP7+fkB8Nlnn/H73/+eDRs2YLfbiYyMJCYmRslXXV3NgQMHeOWVV0hPT2fWrFlK60rd+tesWUN6ejrp6enk5eVhNpuZP3++Mi89Pb3FrSwLFy5U1nHgwAHPZDeJiYnYbDbef/99HnvsMaZPn86KFSsoLCxscasgtd3wW7Zs4c9//jPp6em89NJLnDp1ioCAAJKTW/5YIIPBgM1mY+PGjUpZzWYzUVFRbkFmU44BwMiRI4mJiaG4uJilS5fy2GOPMWvWLHbu3KnkaY78/Hxlm1etWoXdbufAgQNux3XmzJlKq+Dtt99OcnIyFotF2aZZs2axY8cOHA4H3bt3p0+fPp4fg1qt5qGHHqJHjx6cOHGC119/HZPJpKSPHj2ajh07curUKWUfvPTSS2RlZREaGsrgwYPd1qfVatHpdBw8eJBnn32Wl19+mdLSUgICAujevbtbXmpbE5ctW8YzzzxDeHg41dXVHDx40DObEEKIX5FWDQYb3A3iOTUpX/0VXp6EhAQCAwOxWCx07Njxgi0xNpuNjz/+mA8//BCLxcK+ffsoLy/H19eXoKAgJd+yZctYuHAhx44dA8BkMrF7926qq6uvuZsJampq8PHxISkpidDQUKxWK9u3b+fFF19Uyt9c+fn5zJ07l1WrVlFSUgK1XblHjx5VWr5aymw28+9//5v169crZf3xxx/R6/XEx8cr+ZpyDMLCwkhMTKS6upq1a9eyZ88enE6ncgf2lZCUlIRarWbbtm3KNplMJv79739z/PhxDAYDnTt3dltGpVLxxBNPkJKSQk5ODm+88YZbIKhWq+nSpQtVVVWsXr1a2QdZWVls3LgRq9XaoLXP5XJx6NAhFi1aRHFxMZmZmZw8eVL5weSpsrISAKfTyZkzZ3jvvffYs2ePZzYhhBC/Ii3/9vbgcrnOx3INWvt+mX7J2zBfa3UTl5WVYbfb0Wg0xMXF4XK5OHbsGKGhoSQkJKDX66murqa8vFxZpqKigiNHjritpzGxsbE888wzLFq0iGXLlrFs2TJmzZqltCxeSzZu3EhFRQWpqam8+OKLzJkzh7vvvvuy724eOHAgf/3rX1myZImyDy41RrIpLBYLhYWFbvPOnj3bIMhsyjEIDAxEr9djNpvdxhteSQEBAVRXV3P8+HHPJE6cOIHT6VTGp9aJi4sjNTWV/Pz8BoEgQGRkJAEBARiNRp5++mll+5ctW8bMmTMxGAwYDAa3cY0Oh4PMzMwmtwZbrVbMZjOvv/46zz333CXHFQohhLj+tVowCJdq8Wtqy+DlBYN1X3phYWF06tRJeRafRqOhW7duBAQE1AauzfucuLg4pk+fTmJi4mUHVFfCoUOHeO6551i3bh0FBQVEREQwevRo5s6d22j3ZFPccccdTJ48mYiIiKvSEtrUY+Dr64tOp/Ocfc3xDNAKCwspLS0lKiqK0aNHu6UJIYQQbaX1gsHahwx6tvg11jJ4wXwtCNI8WSwWnE4narWa2NhY8vPzyc7OxmKxKGPP7HY71dXVnoteVO/evQkNDaW0tJTFixcr48Tmz59/0a7HC3XHXQlWq5VPP/2U2bNn88wzz7B//378/f1b/Aq7Pn36oNVqOXr0KM8//7yyDzZv3uyZ9bLVdYk6HA6qqqqgGcegoqICi8WCTqdzazE0Go1069ZN+f9yGY1Gz1mKmpoafH19G4zLU6vVSvfw6dOn3dKqq6tZs2YNZrOZwYMHc8cd7nfVl5eXY7VaKSsr429/+5vbeMW66bnnnlO68Fti4cKFbmMfhRBC/Pq1XjBI/TuKG04Oxy/ZbI6G6W7TZajrJo6LiyMoKIgTJ06Qn5/P2bNniYmJwWAwUFNT0+wvTJ1Oh1qtpqqqivz8fPR6PUOGDGH8+PHKY0s8VVZWotPpGDBgwAWfe9gWoqOjefbZZ5k4caLymr2amhry8vJwOp0YDAbPRZpEp9PhcrkoKSmhtLSUsLAwJk6cyE033eSZtdnUarXSvRkZGcmMGTOIj4/HZDIpXfhNPQb5+fmUlZUREBDAyJEjMRqN9OvXj2effdZt/GFL1dWxyMhIhg8f3uhYybru4bS0NEaMGIFer8doNPLwww/TvXt3qqqqyMzM9FyMffv2sWLFCqxWK2PHjmXEiBFKWmVlJfn5+QQFBTF58mT69u3b6Ge3lNFo5C9/+Yu881oIIbxM632TKL2/v7Tw1Z/Kq50s3Wth/VErhwpsDdLrT60hKChIef4atYPs/f39CQ8P98zaJKdOncJqtRIbG8sLL7zAokWLePDBB93uNvaUnZ2N0+mkR48evPDCC8r4rpa8gzY6Opp58+Yp6+jVqxdarZaJEyeybNmyBq/gCwkJYcSIEcyZM4dly5axaNEixo4di1qtbnGrz6lTp1CpVAwaNIhFixYxb948RowYgdFovOzjFhISwsyZM1m2bBkvvPACKSkpOBwOvvjiC3Jzc6GZx2DPnj3Y7XZSUlKYP38+jz/+OO3atSMvLw+73e6ZvVlycnIoKyvDz8+PSZMmsXTp0gbH4IsvvuDkyZP4+fkxceJEFi1axPz587n55puhtnyNBYPUBoSfffYZarWa0aNHu3Xrb9q0iZKSEuLj43niiSeUz66bLuc1ix07diQiIkLeeS2EEF6m1YJBl8OJy25v+MzAetOXP1lZddCC09EwTZkuL6agrKxMeYZgSUkJJ06cACAjI0PpbmxuqyDAt99+y6ZNmzCZTLhcLpxOJyUlJWzcuPGCbzPZsGED27Zto7Ky8rKDpebIz8/nk08+obCwEEdtk6zD4aCwsJDly5ezfv16z0WaZM2aNRw4cACbzQa1d2FnZ2fzxRdfKJ/TGurK+t577/H5558r85tzDL788ku++OILpfvYbDazbds2vvnmG7d8LVFcXMz7779PTk7OBbfbZDKxePFi9u7dq9THujt0V6xYwcqVKz0XcfP555+zY8cO/Pz8ePDBB5WAMDc3l0WLFpGZmdnqz/87efIkhYWFWK3WFv9gEEIIcf1ptTeQmFNXowvQow/0Re3T/Af7qlWg1qiwWWwUrrrbM1n8StW9gcRisbBgwQLy8/M9swghhBCiDbVay6DTYcdusWEtr8ZaZqGmvHmTtcJCdZkFy9kcz1ULIYQQQog20motg66Q/lijp4JGjer8HAD89Haqqs+/NuuiVKBymrHsm4fdfNYzVfxKXc2Wwbp3AjflFXUlJSVXvHxCCCHEldBqLYOqc3vwPfJf+B58GP3Bh9EffAT9wUeY2fFmKnc8cenp6ycw7XxaAkEhhBBCiCuo1VoGL2TW+MM89/aNnrOFEEIIIcQ1oNVaBoUQQgghxPVHgkEhhBBCCC92Wd3ETSXdxEIIIYQQ16YWB4NCCCGEEOL6J93EQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwotJMCiEEEII4cUkGBRCCCGE8GISDAohhBBCeDEJBoUQQgghvJgEg0IIIYQQXkyCQSGEEEIILybBoBBCCCGEF5NgUAghhBDCi0kwKIQQQgjhxSQYFEIIIYTwYhIMCiGEEEJ4MQkGhRBCCCG8mASDQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQvyqPPHEEyxdupS//OUvBAQEeCYLIYTwoBowYICr/oyX57/CM7Oerj/rupOUlMTUqVMpKSlhzpw5nslea8KECdx+++1s3ryZNWvWeCZfU0aMGMH48ePRarXKvAMHDrBw4UK3fDQzr/h1i46OZubMmYSFhWE2m1myZAkZGRme2a4bs2fPJjIykrVr17JlyxbP5GvKjBkz6NWr11W7vtRdBwoKCuS638rqjm0du91+XdTJi5H64q7FLYNJSUksWLCAefPmER0d7ZYWHR3NvHnzGk27nkybNo1//OMfJCUleSYJIa5B+fn55OTk4HQ6OXnyJLm5uZ5ZRK2+ffsyd+5cnnjiCc+ka871VFZx9Ul9ab4WB4PeoF27duj1es/Z4grYsmULjz/+OOnp6axatQq73e6ZRdGcvOLX75///CePPfYYL7/8MpWVlZ7JolZISAjBwcFoNBrPpGvO9VTWX6OFCxeSnp5Oeno6Bw4c8Ey+5kh9aT4JBoUQQgghvFiLxwzWjcuzWCwsWLCA/Px8Ja1u3A7glhYbG8v9999PQkICPj4+OBwOsrOzef/998nLy1OWBxg4cCBjx44lODgYgPLycr7++ms2btyI0+lU8qnVakaNGsWgQYMIDg5GrVZjt9tRq9Xk5+c3eyxA3bi6i/Ecj5aamsqYMWOIjY1Fo9Fgs9nIz89n3bp1HD161G3ZS6nbd2azmePHj5OWlobNZuOjjz4iISGBvn37YjabWbFiBfv27QPglltuYfjw4URFRSn7taioiI0bN7Jr1y5l3Y2NGYyLi+OJJ54gLCyMw4cPs2TJEqxWKzTjGDRHU8taX93YjiNHjlxyHGBz8l5KXd0aPHgwQUFBqFQqKisr2bZtG5988onbPggPD+f+++8nOTkZHx8fbDYbWVlZrF69ukHd9jwPnE4nFosFp9PJ//3f//Hll19e8Pyqqx8Gg8FtPJxarWbMmDGkpaUREBCAy+WiqKiIzz77jJ07dyqfPWPGDJKSktiwYQMJCQlKeU0mE1u3buWTTz5R8tYZOHAgI0eOpF27dqjVaiwWC9999x3vv/++2z6IjY3lgQceoHPnzmg0GiwWCxkZGaxdu5bi4uJLrnPPnj2sWbNGqX/NUbe//Pz8lHl5eXmNnv+zZ88mLCyMlStXMnjwYDp37oxaraasrIx169ZdsB5eSmxsLOPGjaN79+74+vricrmoqqri+++/Z9WqVS06Z2bPnk14eDgrV67klltuISEhAa1W22hZw8PDGT16NKmpqfj7+6NSqaiurubIkSO8//77mEwmtzGVF9LYWMvw8HDGjx9PUlISBoMBp9NJQUEBq1evVq5xM2bMICUlhU2bNhESEkKfPn3Q6/UXrVsX09yy1p37ubm57Nmzh5EjRxIUFITD4eDw4cMsX74ck8mkLNucc7Y5mlq3m3O8mvN90FbbVWfGjBkkJydfcMxgW12LUlNTueeee4iMjEStdm/HMpvNrF27llGjRrVZffm1u2Itg9HR0UybNo3ExER8fHwA0Gg0dO3alXHjxrnlveOOO5g8eTKhoaGo1WrUajUhISGMGTOGiRMnuuWdMmUKY8aMUfICaLXaBpWlrfTp04eHH36Y+Ph4pUnax8eH+Ph4HnvsMfr06eO5SJMEBwczaNAgtFotBoOB2267jT59+qDRaDAajdxyyy1Qu1/HjRtHXFyc236NiIhgwoQJJCYmeqz5F5GRkfzud78jLCyMgwcPsnDhQuVi1Zxj0FSXU9ar4b777mPMmDGEhISgVqtRqVQYjUaGDx9Ojx49lHxGo5HHH3+c3r17K9vl4+NDYmIi06dPJy4uTsmbkpLCH/7wB7fzQK1W4+/vj8FgQKVSKXmbY+LEiYwaNQqj0YhKpUKtVtOhQwcmT57MHXfc4ZZXo9EwatQot/IajUZuv/12+vXr55Z30qRJPPjgg3To0EE5pwwGA/3793fbB3FxcUyfPp2uXbsq54HBYKBfv348/vjjGI1GJe/w4cOZPHlyg3XeeuutDBo0SMnXlnQ6HZMnT1bKq1KpCAkJ4d5773U7Xs0xbtw4evXqpRxHtVqN0WgkLS2twTWuOXx8fLjnnnuUOlNX1gkTJpCSkqLkGzZsGAMHDsRoNCr1te4YTJo0yW2dzREXF8cf/vAH+vXrh8FggNo6Gx0dzbBhw9zyulwuBg0axIABA5QhNheqW22lffv2TJgwgeDgYFQqFVqtll69enH33XcreZpzzjZHc+p2c45XU78P2mq7mqMtrkUpKSn87ne/Izo6utW/25tSX7yBJiYm5v/Vn3H7nXew+fPN9Wc1qn379vTr1w+73c53333nFkEHBgZy0003AShpN910E3379iUrK4u///3vrF69moMHD6LT6XA4HMqvmujoaCZOnIher2fLli0sWrRI+YUQGxtL+/btOX78OOXl5fTr14+RI0ficrmUvOvWraOkpITu3btjNpvZvn27Uq6myMjI4OOPP+bjjz9WKujChQt5++23lfl79uwBICAggEceeYSwsDCys7NZsmQJ//73vzl9+jSxsbGEhobi5+fH7t27PT/mgur2XXBwMGVlZXzwwQd0796doKAg8vLy+Oyzz0hMTMTpdLJ9+3YCAwPp1q0b+/btY8mSJaxevZqMjAw6depEeHg4JpNJ+eWelJREQkICWVlZnDx5kieeeILY2FgOHjzIP//5T6XlojnHoDmaU9b6OnfuTM+ePTl79qyy7y+kOXkvZfz48RgMBtasWcMbb7zBpk2bKC0tJTQ0lGPHjimtXffeey+9evUiPz+ft956i3feeYeMjAyioqKIiopCpVJx6NAhAB544AGio6M5deoUy5Yt45133uGbb74hPj6ekJAQjh49SnZ29gXPr7r64ePjww8//EBRURG9e/dm9OjROBwOPvzwQ/75z3+yZcsW/P39lXq4d+9eampq6N+/P5GRkWi1WrKzs3njjTf48ssv6dq1K2FhYZSXl3PkyBEAZb0+Pj4cPnyYpUuX8t5773Hq1CkCAwP5+eeflX3w4IMP0qlTJ44dO8bixYtZsWKFch5ERERgNpv5+eefARg1ahQRERF88cUXzJ8/nw0bNnDq1CmCgoKUG0Caq67l4eOPPyYrK4vU1FQsFkuj539aWpoS4B86dIjXXnuNAwcOkJiYiNFopKioiKysLM/FLiklJYXi4mLee+893nnnHb766iuCgoKIiYlBr9fz9ddfey5ySXVl1el0HD58mIULF7Jjxw66dOlCu3btqKmp4eDBgwB07doVgDVr1vDWW2+xadMmNBoN8fHx+Pn5cfjwYQoKCtiyZQsff/wxZrOZxMREDh06xP/8z/8o17fPPvuMoqIipQwPPvggCQkJlJeXs27dOhYvXszXX3+NWq3G4XAon9+/f3+ioqLQ6XScOnWKN998k82bNzdat5rCZDI1q6x1577BYKCqqooPP/yQpUuXYjQaiY2NxcfHh6+//hqXy9Wsc7Y5mlO3m3K81Gp1s74P2mq76uvfvz/t27dXrlX1tdW1aOzYsXTu3JmsrCzmz5/Phx9+iEajIS4ujqKiIubNm8exY8farL54g9YNsS+irhusffv2SotCbm4ub7/9NsuWLVPy9ejRg6CgIA4cOMAHH3yAyWTCarWyfv16fv75Z/z9/encuTPUVjxfX192797N2rVrMZlMOJ1Ozp07d0UOYGJiImFhYRQXF/PWW28pXyB79+5l/fr1WCwWIiIiWnRHtd1uZ+fOnZSWluJyubBarWzbto3CwkJsNpuSLz8/n7lz57Jq1SpKSkoAyMrK4ujRo8qvMk9arZbf//73dOzYsUEgSDOPQXO0pKxXU01NDT4+PiQlJREaGorVamX79u28+OKLHDt2DGpbSLp06UJVVRWrV69W5mdlZbFx40asVqvya7xTp05ER0dTXl7Of/7zHyVvSUkJVVVV9T65ebp164bBYGDbtm1s2bIFq9WKyWRixYoVFBYWEhISQmxsrNsyubm5LFy4kLy8PAoKCpSy1O9iSUlJwd/fX+lyz83Nxel08sMPP/Daa68py0RHRxMbG0txcTHLly9X7uDdu3evEozVry9Wq1XZb+3bt1fW+fLLL/PVV18p+dqSy+Xi0KFDLFq0iOLiYjIzMzl58iRarVYZFtFcy5YtY+HChcp+MZlM7N69m+rq6ssayO50Ovn6669ZsGABBQUF5OXlsXPnTux2u9tx/eijj/j73//ODz/8gNPpxGq1smvXLioqKpQWj+bq1KkT8fHxmM1m3n//fbZu3YrVaqWkpISVK1fy73//23MRcnNzefXVV8nKyrpg3WpLlZWVrFixgq1bt2KxWNi7dy/V1dUYDAali7Gp52xzNaduN+d4NeX7oC23q6na6lrUoUMH7HY733//PQUFBVitVtauXUthYSGBgYF06NCh3hqb51L1xVtcsW/fXbt2cfDgQYxGI7/73e9YsGCBMmagvtDQULRaLTfeeCPLli1zm1JSUtBoNAQFBUFtK4nD4aCgoMBtHVdKcHAwWq2W/Pz8BmOiTpw4QUVFBQaDoUVfLuXl5ezfv1/5v7KyssGvsDoDBw7kr3/9K0uWLFH21cXGPfbu3ZtOnTpx9OjRBoEgzTwGzdXcsl5NGzdupKKigtTUVF588UXmzJnD3Xff7XaHeWRkJAEBARiNRp5++mm3fVU3ts9gMBAWFkZYWBi+vr6YTKYWtTxdSN3YpLvuusvt85cuXUpMTAxarZbQ0FC3ZX788cdLjocJDw/Hbrdz9OjRBnWkvuDgYAwGA+3atWPu3LluZZgwYQJardatvmzevJnTp0+TkJDA888/z0svvcQDDzxAeHi423rbksPhIDMz86Lb1VyxsbE888wzLFq0SNn+WbNmuY1jbAmn00lhYaHbvIKCAmpqatyCzPDwcB5//HHeeOMN3nrrLZYtW8acOXMuKwirq7MlJSX88MMPnsmNakrdakvnzp1Tepoa05xztrmaU7ebc7ya8n3QltvVVG11LSotLUWr1ZKamkpkZCR6vZ7x48cTERFBTU0NZrPZc5Emu1R98RZXLBh0Op0sWbKEefPmsX//fpxOJ6mpqTz55JNMnz69Ra1ChtrxK9e61vzC8VQ3ti8iIqLJrQ9ZWVlUVVXRtWvXBmN+2lJLyno1HTp0iOeee45169ZRUFBAREQEo0ePZu7cuS0aC2o0Gq+L7a6j0+k8Z7WK3Nxc5syZw7vvvktubi7BwcEMGzaMOXPmMGLECM/s14W6MZOJiYlX5XFUdWPF6sb1qVo49tSTXq9v0bXZWzW1brfV8fq12r17NxaLhR49evDCCy+waNEi7rzzTrRaLT/++CMnTpzwXEQ0U4vP8vLycqqrq9HpdAQGBrqlRUZG4u/vj8vlavDMt6ysLBYtWsRTTz3F22+/TWVlJT169FC+XKuqqnA4HOzYsUN5rlH96bHHHuOjjz4CoKysDLVa3eCVU6mpqfj6+rrNa6mLdRvVdX1HR0c3+OWXkJBAUFAQJpOJs2fPuqW1pj59+qDVajl69CjPP/+8sp82b77wuM/S0lL+7//+D4DRo0c3CGyacwyaoyVlvdqsViuffvops2fP5plnnmH//v34+/szdOhQqD0PrFYrZWVl/O1vf2uwr9LT03nuuecoKSnh7Nmz2Gw2fH193epLXFwcMTEx9T71whITE/H393ebZzKZcDgcrF+/vsFnp6enM23aNL755hu3ZZqivLwcjUZzyWEOZrOZmpoaTp8+zVNPPdXg89PT03nppZfclnE6nezcuZO//e1vPPXUU2zduhWNRsPgwYMbnM/Xg969exMaGkppaSmLFy9Wtnv+/PmX1WpxIXV3LNc9RzElJYWoqCilO/exxx4jPT2d2bNnK0MyLsRY7+YeT6WlpdjtdoxGIwkJCZ7JV9zFytpUzTlnW6IpdftyjteFtPV2NUVbXYv69euHTqdThoK5XC4qKyv56quvWFZvmJmn1qgv3qLFweDp06cxmUwEBARw9913KxeK5ORkxo0bh6+vL2fOnOHMmTMA3H333fzpT3+if//+yi/ngoIC5Qun7qBlZ2djNpvp27cvkyZNumiT9qlTpwAYMGAAiYmJREZGMnPmTNLS0lrl12xlZSU6nY4BAwY0Onag7iaK8PBwpkyZoozH6NevH/fccw96vZ7c3Nw2Pfl0Oh0ul4uSkhJKS0sJCwtj4sSJyg08F1I30Fav1/Pggw+6BYTNOQbN0dKyXg3R0dE8++yzTJw4UQmGampqyMvLw+l0Kq3SlZWV5OfnExQUxOTJk+nbt+8F615hYSEmk4mwsDBGjhyJwWBgyJAhzJw5s8GPibKyMiwWC0FBQcqdmePGjVPOrfpOnDiB0+lk2LBh3HXXXa12AczOzsbhcHDzzTcrd51TG/g89dRTbmN/S0tLiYiI4NFHH3W7y7gxf/jDH3j44YeVAfQ2m41Tp05RU1ODTqdrEOxeD3Q6HWq1mqqqKvLz89Hr9QwZMoTx48c3OF4tERAQgF6vR6/XM2LECNLS0nA6ncrjXwwGA2q1murqak6fPg21b2GYPHnyBX/MlpWVYbfbiYyMZPjw4Y3W27y8PM6dO0dwcDCPPPII/fv3R117l/TEiRN56KGHPBdpE00pa1M155xtrqbW7ZYcr0tpy+1qqra4FnXo0IHOnTtTUVHBypUrefHFF3n00Ud56qmnGjzeqk5r1hdPjz76KEuWLOGVV15p0JByPWv+iOJaTqeTPXv2EBUVRUJCAv/93//tlm4ymfjyyy+V/3U6Hd26daNbt25u+aj9kjx8+DAAx44d4/vvvyctLY3hw4czfPhwt7wlJSXKc9e+//57brrpJtq1a8czzzyj5CkqKmqVrprs7Gy6d++uNE3XqXvOYHFxMdu2bePuu++mZ8+e9OzZ0235s2fP8vnnn7vNa22nTp0iJiaGQYMGuT26oO7X08V8/vnnhIWFkZaWxoQJEygpKSE3N7dZx6A5mlNWz3dhAvTq1Uv5FVj/WYnNydscISEhdOnSpUHXZf0vYYBNmzYpA+0be/1R3eeXlJRw5MgRhg4dypAhQxgyZAjUri8/P99tEHR+fj6FhYVK4Dhy5EioHUhu9XgO386dO0lJSVGew3XPPfe4pV/oeXuXsnnzZrp06UJqaioDBw5k4MCBSprZbFYusE6nk02bNjFlyhSSkpIajAO2e7zH1Gg00rNnT2699Va3fC6XiyNHjig/IJujseeD+vn5KXXA89mgre3UqVNYrVZiY2PdrhUul6tB3W4urVbL6NGjGT16tDLP5XJx8OBBpVX9xIkTWCwWQkND3a6FF/v8nJwcysrKiIiIYNKkScrjTOo/i62yspINGzYwZcoUOnTowGOPPcZjjz2mrONKvY2iKWVtjqaes83V1LrdkuPVFG2xXXXPOvRsFJg4cSITJ050OwZtcS2qqqrCarUSHR3tVveoHftbWFjo9rxL2qC+1ImOjqZLly7KuPkuXbr8asYbXla4/Pnnn/Pee+9x5swZJTq32WxkZmby6quvut3CvnXrVvbu3YvFYlEqfN2dO6+99prbDRirVq3ik08+obS0tNGov05+fj4rV66ksLAQp9OJ3W4nMzOTFStWuN1x21IbNmxg27ZtVFZWXvAEbWwf1G3Xq6++2ubvRl2zZg0HDhxQttdms5Gdnc0XX3yBw+HwzN7AqlWrOHToEGFhYUydOlVp3WzqMWiOyy3rlZSfn88nn3xCYWGhUra6C8/y5ctZv369kjc3N5dFixaRmZnZIFDztHr1avbu3YvNZsPlcmEymdi4cWODG5CoPQY//fQTTqcTp9NJaWkp//nPfxoMHHc6nbz11lts27ZN6UZpDY2t11XbPbN7927lrj+Affv28a9//YucnJxLnnsfffSRW766bfv4448v2uVzLfv222/ZtGkTJpMJl8uF0+mkpKSEjRs3UlZW5pm9yQ4fPkxRURE1NTVQu69MJhNfffUVb775pnKss7KyWLNmjXK+Op1OysvL+fLLL5WWJ0/FxcW8//775OTkXPT8a+zYOhwOcnJy2Lp1q2f2NtHUsjZVc87Z5mhq3W7J8WqKttqupmrsmnG5Kisr+fbbb7HZbDgcDqxWq/J9XDeM5aGHHnIbztLa9aVOfn4+P//8Mw6Hg/LycuVxWb8GLX4DiRCi9Vzqqf5CCOGNEhMTmTp1KjabjSVLlrg9iSElJYWHHnoIPz8/1qxZ0+DxPaLpLqtlUAghhBCircTExChjLNu3b68MAQsLC6N79+74+flRXV2t3EMgWkaCQSGEEEJck86ePYvFYiE4OJj/+q//Up7jOW/ePO644w40Gg0HDx7kp59+8lxUNIMEg0IIIYS4Jh06dIh3332XnJwcZews9cZwf/jhhyxfvtxtGdF8MmZQCCGEEMKLScugEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwotJMCiEEEII4cUkGBRCCCGE8GISDAohhBBCeDEJBoUQQgghvJgEg0IIIYQQXkyCQSGEEEIILybBoBBCCCGEF5NgUAghhBDCi0kwKIQQQgjhxSQYFEIIIYTwYhIMCiGEEEJ4MQkGhRBCCCG8mASDQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwoupBgwY4Ko/4+X5r/DMrKfrzxLXgfv7dWVI12jl/xq7g7d2ZpBRUOqWry08OSyV7h1ClP8rrTZe2bKfMxVmt3wdAv14ekRvAvQ+yrzjZ87x+taDbvnEeZd7TOv2d35Z5RXdx08OS6VTWOBFy+pZZ5q7bUIIIVrPZbUMTpgwgWXLll1wmjBhguciV920adP4xz/+QVJSkmfSZenXrx/PP/88S5cuZfbs2Z7JV11SZCiv3TeIJ4eleibRIdCPv99zK/9z14341wvUhBBCCPHrd1nB4PWoXbt26PV6z9ktdsstt/DCCy/w2GOPERcXh1p99XZppdXGnI17mLZyG099sOOKtbK8vvUg01ZuY9rKbRw/c84zWXGmwsyf1n3DtJXbmLNxD5VWm2cWUc/qvT8p+3X7T/meyde1+nXm17ZtQghxvbmsyGXNmjWkp6eTnp7O5s2bAdi8ebMyb82aNZ6L/KqEhYUxatQoIiMjMZlM7Ny5E7vd7plNCCGEEOKapYmJifl/9WfcfucdbP78fGDXHElJSSQkJJCVlUVGRoZnMrNnz+bee++lrKyMyZMnM2XKFMaNG8fgwYOprKzk1KlTynqef/55+vfvT01NDTNnzuS+++5jzJgx9OzZk+zsbCorKwEYMWIEf/rTn+jbty/bt29XPqtuHWlpaRw7doyRI0cya9Ysxo4dS1BQED4+Ptx8882MHTtWmTp27MiePXuUdTSFxWKhY8eOlJaWsnjxYvz8/OjZsyeVlZVu5fE0dOhQ/vjHPzJw4EB+/vlnysvLPbM0W3JUGFFB/uzKLqSqkRa39kYDfTu2p8xi5bsTZ9zSAvQ+3NI5Eqvdwe4ThdgcTgC6tA9m+pAbmNC3K3clx3NjfASV1TYKK6rclq9vQKcIAn11FyxHnbrPNFXXNChPHaPeh9/elMiDAxIZl9qZO5LiSIkJ53R5FWVmq2f2VnNTfAf++85+OJwusooaHhutWsWs4b0ZnRzPgVPFWGz2Jpc1KTKU50b2w2yz43C6eHzwDUy6sRsjk+KJDPLnWOE57M7z+7++5KgwYkMC2HeyiKJKi2cyKhX07diexwYmMb5PF8akdGJgQiQ6rYbSquoL7uO2MKBTBCF++guW1dOltk0IIUTbuqyWwebS6XRMnjyZrl27otFoUKlUhISEcO+99xIXF+eWNzw8nClTphAaGoparUatVtO1a1cefvhhAgIC3PJeTe+++y4LFy6kuLjYM+mCkpKS0Ov1hIeH07dvX8/ka8IN0WFMG3IDkUH+qFUq1CoVHYwGfndzD4Z2i/HM3uoCfXXMHJZKv7j2+Go1UBuExYUa+f3QVG6IDvNcpNUUV1VjtTkI9mt8OIHeR4tBp8Vss1Nts7eorMlRYfzhtt7EhxlRq1Ro1Cr6xbVnZJL7edBUd/aM43c396BDoB8atQoArUaNWnX+7yupyCQBnRBCXE+uaDCo1WrR6XQcPHiQZ599lpdffpnS0lICAgLo3r27W16DwYDNZmPjxo1Mnz6dFStWYDabiYqKIjEx0S3vpdTvzs7Ly8NsNjN//nxlXnp6OgsXLvRcrM1kZGRgtVo5d+4cBw9eubs8Abp3COHNSWlu0+xR/d3u8PXX+zAutTMA739/nJlrvmbGqu0s352JxWZnUJcoAn119dba+saldiY6OICcEhMvbdrLtJXbeG7Dbg7nl6DXqBmRGIu2NuhpbeUWKzaHk4hAPwBC/X3529gB/O7mHgAYfDQE6Hwot9RgrrG3qKzJUWFUVtfw+taDTF+1jWXfZFBjd9AzMgQ/ndYt76XEhgQwtHsMNXYH7+/5kZlrvmb6qm28+uWBi7bOthWHy0WNw0lJVbVnkhBCiGvQFQ0GXS4Xhw4dYtGiRRQXF5OZmcnJkyfRarUEBwe75TWbzfz73/9m/fr1WK1Wtm/fzo8//oheryc+Pt4t7/Xmq6++Yvr06fz5z38mKyvLM/mqiw81Eu7vy46fTrPz5wLsDidOl4vdJwo5kFdEsJ+eqGB/z8VaTZBBR9f2QZwzW3nn2wzyzp0fFnDOXM17e45z1mQhMsifsACD56KtwmJzUFljw1erRa/VEBXkT4Cvjs7hgQQZdGjVatRqFUWVlhaXNb+skpe/2M/xM+dwueD4mTKKq6pRqVSomtma17tje/x1WjYdzWVn1mnsDicuF5iqa3B7btQVdjU/WwghRNNd0WDQ4XCQmZmJs5ExUZ4sFguFhYVu886ePYtKpbqqd+xe746fOafcxVk3ed7ZGxbgi06r4bYesQ1aEQd2icJHoyaoDVsGfX20+PpoOWsyU1zp3rpUUV1DYYUZnUZNuL+vW1prqbLaMFXXoPfR4KNR07nd+WBPq1ETExxAmL8vOo2agvKqFpf156JyKqprlP+rrDb++un3/PXT75vdmhfqp8fmcHK67MJjOYUQQogLkahKXJdctd2RbaWwwkyA3gejr46u7YLYn1eEucZOtw4hBOh9cAKlVU27iaWty1q/i18IIYRorusmGFSr1XTp0gWHw0FV1cVbQHr27IlOd+GWq8a6pcUvKqtt2J1ONhzMbtCKOG3lNmas2s53OW13d6rN4cRmd9De6Ed4gHuLWoifL1HB/pitNs624Y0KZ00WfDRqEtoFEqD34cjpEn46W0a39sEYfXU4nC5lbOHVLmtRpQWtRt0gKEyKDMXg07zxh61h9d6f+NO6bxq8gUYIIcS16ZoNBtVqNWFh5+/CjIyMZMaMGcTHx2MymThy5AgABQUF1NTUEBYWxtChQ9Hr9UyZMoXhw4ej1Tb+JVhZWYlOp2PAgAFERkZ6Jl8RQ4cOZdGiRbz44osN7qK+Fpw8Z8JUbWN4YixDu8egr71D9ko5Z67m5LlKQvz0TO7fXQmyQvx8mTKgO+H+vuSWmjhndu+WDfTV8d939uWN+4cwvk8Xmjn0zk252YoKSIoMw+Z0Ulhh5qczZYQF+JISHUaNzU5Vjb3FZW1NOcUVANzWoyPhAQb8dVom3diN3/RKUO4s9qTVqJk25AYWThzC1EHJaDWtcynQqlVMHZTM/PsG0Se2nWeyEEKIa9BlvZt4woQJ3H777Z6zFZs3b1YePD179mwiIyNZu3YtW7ZsUfLMmDGDXr16KXmTkpKYOnUqfn7n7+Ssz2az8dFHH/H5558DEBAQwB//+Eeio395fytATU0NKpWKiooKFixYQH7+L284+M1vfsPIkSMbjDs8cOBAi+4oriv/heTl5TFnzhy3eXXLOJ1ONm3axLp169zSW+L+fl3p27F9o+8EpraV6NGBSZwoqWjwntq6d9hWVNcwv/YO1GHdYy4YTJwur1LyNfa+4frq5x3cNYqJ/bp5ZlFs/ymf1Xt/AiA6OIAZaSkEGRq28J4zW3lz+2Hyy87frFEnMSKEqYOS0Ws1VFhqeHnLvgbj+JoqoV0Q0wbfgEGnZU/OGd7ddYxQf1+eHt6LID89BbXbZa6xN6usdcdh14lCZVsb46/3YdbwXkQFNX6jTv33P/vrfXhyWCoxwe6PXDpVVonBR0txpaXBMQ8P8OWZEX0INOiw2h0s2XGEzMILvz2mqerXh/rH82Lu79eVmztFyLuJhRDiKmmd5oA25nA4KCws5L333lMCQWpb+VavXk1+fj5OpxOn00l+fj7vvPMORUVFbuuos2HDBrZt20ZlZSWuNhzHdTFX89EyTfXVj6d4d9cxzlSYcTiv/H7KL6vkja8OknG6FHvt51fbHezNPcvcTXsbBILU3hxzIO/8cVerVWgu40ajc2Yr1TY7TpeLY7UBSmnV+VZAtUpFpdWGueb822ZaUtbWVGW18c43R8kpMeF0ubA5nHyfe5a3dmRgtTs8swNQXFnNth9P4XK5UAGay2lGraek0kJWUTlWu4OfzpR5JgshhLgGXVbLYFuoaxm0WCwNWvXEhV2qZdAb+Ot9GJUcT1q3aPLOVTL/ywNU2+T1gI3RazUM7hLFmNROmKptzP9yf4tbUS+XtAwKIcTV1fKmEyGuEf56H/7nrhv5xz23ktYtmmq7g/87fEICwQt4clgq8+8bxG96J6BCxZeZeVctEBRCCHH1STD4KxKg92H2qP68OSmN1+4bRFJkqGeWXzWr3UHG6VJe3ryPw/klnsmiHrvTRW6pide/OsjW4+ffC34lPTksVXl25ZCu7mN+hRBCXFkSDIrrXt0Dm2d9sINF2w9xuvzijx7ydq9vPcjM1duZ9/kP/HxWxvUJIYS3u+bGDAohhBBCiCtHWgaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwotJMCiEEEII4cUkGBRCCCGE8GISDAohhBBCeDEJBoUQQgghvJgEg0IIIYQQXkyCQSGEEEIILybBoBBCCCGEF5NgUAghhBDCi0kwKIQQQgjhxSQYFEIIIYTwYhIMCiGEEEJ4MQkGhRBCCCG8mASDQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwotJMCiEEEII4cUkGBRCCCGE8GISDAohhBBCeDEJBoUQQgghvJgEg0IIIYQQXkyCQSGucRGBfrwwdgCv3DuQxIgQz2QhhBDismhiYmL+X/0Zt995B5s/31x/lrgO3N+vK9OHpDDqhnhG3RDPbT1iyTtXSVGlxTPrNWdw1yj+fEdfTNYacktNnsler0/HdvSP74CPRk213UHG6VLPLEK0uaTIUJ4b2Y/uESF8d+KMZzLUyzM2tbNyLerSPuiC+YUQ14bLahmcMGECy5Ytu+A0YcIEz0WuumnTpvGPf/yDpKQkz6QWGzhwIH/7299YunQpb731Fq+99hoPPPAAavVl7d42oVJB347t+e87+7Hg/iG8OSmN+fcNYvqQFKKC/D2zi2vAj2fKKKmqptJqY39esWeyEEIIcVmuvWiljbVr1w69Xu85u8UeeOABpkyZQocOHVCr1ahUKgICAhg6dCjp6eme2dtUpdXGnI17mLZyG099sIOMAvcWJK1GzSO3JPHILT2IDQlAq1YBoNdqSIoK5b6+Xdzyi2tDYYWZ5z/ezZ/WfcOPZ855JgtxzcgoKOWpD3YwbeU25mzcQ6XV5plFCHENuqxgcM2aNaSnp5Oens7mzee7ljdv3qzMW7NmjecivzolJSUUFRWxbt06pk+fzqxZszh06BAA3bp1Iy4uznORq2ZUcjx9YsOxOZx8diSXZ9d/y7SV23h2/bf83+ETlJmtnosIIYQQ4ldONWDAAFf9GS/Pf4VnZj1df1aTTJgwgdtvv53Nmzc3GgTOnj2bsLAwVq5cyeDBg+ncuTNqtZqysjLWrVvHrl27AEhKSmLq1KmUlJSwefNmfvOb3xAcHAxAVlYWy5cvp6CgAIARI0Ywfvx4CgoKmDNnjvJZdeuwWCwsWLCAW2+9ldtvv11Jb8yBAwdYuHCh5+wW6dSpE9OnT0en07FkyRIyMjLc0ocOHcr48eMpLy9nyZIl5ObmuqW3xP39utK3Y3te2bKfMxVmz2TCA3yZNbw3Rr0P/9lznD05Fx/Do1JBn9j2jE7pRLsAX1RApdXO7hOFfHL4BHaHU8mrVqkY1j2GYYkxBPnqcAHllhq+zMzjqx9P4apXw3x9tIxM6siATpEE6LUA2J0ufDRqVu39ka9/Ov1L5lb25LBUjL46Pj96kgl9uqDTavhg30/gUnFvnwScThf/2nWMI6dLoBnb1SHQj6dH9OaHk2f5POMkD/TvRo+IUFQqyCoq51/fHqPMcj7QfnJYKpGB/izZeYTbEmNJjg5Ho4KzldUs332ME8UVSnnv79eVIV2jlf9r7A7e2pnRoMU3KTKURwcm8U1WAbmlJsaldibYoMPudLHz59OsP5jtdryMeh8m3diN5OhwpXW4zoU+o7V1aR/MpH5d6RDoB0BRZTX/d+gE+/LO4nKdryezhvci3N+XN78+TFZRudvyt/WI5e7UzmzJPMX6A1lQW7fGpXbipk4R+Go1VNsdHMkv4YMffsJUr4Wqbn+tO5BFdlEFD/TvTsfQAFwu2J9XxPvf/4jVbuexgcncEB3Git2ZfOdxvqRGh/Nft/Yko6CUpTuP4HI1vb7c368rAzpF8O63x0iMDFHKe85sZf2BLPbmnnX7LM/jVWW18W12AZ9lnKTaZnfL25bq9tuJkgpe33rQM7mBuvMiv6yySfmFEFfPZbUMNpdOp2Py5Ml07doVjUaDSqUiJCSEe++9t0ELWnh4OFOmTCE0NBS1Wo1araZr1648/PDDBAQEuOW9VjkcDmpqajxnk5SUhF6vJzw8nL59+3omt4mE8CCCDDpOlFSw76T7l01jhnaL4Xc396CD0YBapUKlUmH09eG2HrFMHZiMVnO+6qhUcF+fLvymV2eCDXpUKhVqlYoQPz339k5gQp+uqGrjDV8fLVMHJnFbj44YfX1Q1a7Xp3ZdV0KAzocJfbrgr/fBR6Pmth4dGderM3qtBoNOy9Du0ahUzduuOjHBAfz5jr4kR4WhUZ/P37V9MPf3c8/rp9MybUgKvWLboVWf3wcdjAam3JRIoK+u/iqb5cb4Djx4UyIhfufL66NRk9YtmqHdfgko/fU+/H5YqvLZV8MN0WFMG3IDkUH+qGv3awejgd/d3IOh3WIAqLbZySw8h0GnJSkqzG15rVpFclQYVoeTjNrAXatR81+39GRI12h8tRoAfLUa+sW1Z+aw1Eb3a3JUGH+4rTfxYUbUKhUatYp+ce0ZmRSHywUH8ooA6BXbrsGxTokJB2B3diEuV/Pri49GzQP9u7uVN8RPz4S+XYkN+eX6FuirY6bH8fLX+3Bbj4781y09lfPwSiipqqam3o8KIcSvx5W7kgBarRadTsfBgwd59tlnefnllyktLSUgIIDu3bu75TUYDNhsNjZu3Mj06dNZsWIFZrOZqKgoEhMT3fJeSv3u7Ly8PMxmM/Pnz1fmpaent1qrILXBXkBAAIWFhWRlnW+1qC8jIwOr1cq5c+c4ePDK/GKODw9ErVKRVVyB3Xm+ieLJYam8OSlNmZ4clgpAbEgAdybF4XA6+fhgNjPXfM2MVdtZvjuTSquN7h2CSYoIBSAlKpxbEyKpqrGzfHcmM1ZtZ+aar/n4YDY2h5O+ce2JCT7/5TaoSyTdOgSTX1bJvM9/YPqqbefzHjpRr6RtK7C2xey1rQcprDDTLsBAhcXKP77YxzmzlSCDHj+dT7O2q05CuyA0apWS/++bf6DCUkOnsEDC/H2VfFqNGl+thq9/Os0f1u7k//vkOworzIT6+9Ix1KjkW733J6at3Ma0ldvY/lO+Mv9CAvQ+nDNX8+qXB5ixajsfH8zGBfSMPN9KCZAcGUpUkD85JSb+v0++Y/qqbSzcdogKSw3nzFb++tneNm0V9Nf7MC61MwDvf3/crW5ZbHYGdYlSAre9uWeotNro0SEEfW3ABBAZ5E9kkD95pSayi8+3GA7qEkWPyBCOnznH//fJbqat3MZzG3ZzOL+EqKAAburUQVm+TnJUGJXVNby+9SDTV21j2TcZ1Ngd9IwMwU+n5VjhOYpMFjqGGt2OX5BBR9f2QRRUmDl+tgyaeR5Q2+ocoNdyOL+EP6/7hj9+uJPjhecI0PvQpf35nhCAkclxRAX5892JQmVYx0ub9pJTYnI7D6+Eugb+ItO1/4QCIUTzXNFg0OVycejQIRYtWkRxcTGZmZmcPHkSrVardAXXMZvN/Pvf/2b9+vVYrVa2b9/Ojz/+iF6vJz4+3i3vtSQuLo7BgwdTXV3NF198gdPZ8Jf0V199xfTp0/nzn//caLDYlsrM1Z6zGugZGUqA3oftP51m09GT2B1OnC4Xu08U8sWxk2jUKhIjzz/vrnfHdqjVKtYfyGL3iUKcLhd2h5NNR0/yw8kiAvQ+dAoPRK/V0Ce2vfJlmVtqwuUCu8OJuebKDTJ3uVxs//EU+WWVOF0uHE4XXxzL46zJgqVel1tTt6u+csv5wKIuf06JiYKKKtRqFZp6d5a7XOe7b1f/8CPVNjvFlRaOnzmHj0ZNqH/Lb24qt9SwZEcGP58tw+lysf9UMeYaO0ZfHX46HwBiQ8+3gu0+UUBxpQWXC44WlLL/VBH+Oi0djAbP1baq+FAj4f6+7PjpNDt/LnCrWwfyigj20xMVfP6u9lNllWQVldPeaCCmXmtZYkQIBh8tu08UYne6UKkgOSqUcksN7+05TnHl+Tp+zlzN+gNZVNXY6N6h4fMZ88sqefmL/Rw/cw6XC46fKaO4qlppsa6oruFQfgnBBh1Jkb+0TiaEBxFs0LM394zSTduS+nK04BxvfZOByWqjqsbO/lPnWyLb1x4Df70PXdsHc6qsitU//ExF9flehrxzlXx6JAcX0LWD+3XzSnDUH/chhPhVuKLBoMPhIDMzs9EAyZPFYqGwsNBt3tmzZ893v1yDj2yhNhCcOnUqRqORzz77jH379nlmueoMPufH6QG8vvVgo3f9BfvpqbE7Gr1zNbu4gmqbg4jasV5BBh3mGjvZ9ca61fnxzDmcLheRQf74+mjw12upqK6h9CreqHL+S/eXx7NYbHZOlDQse1O3q77Ciiryyyrd5r2+9SB/WveN2zhOm8PJkdMlbmMpW0Njn++psNyMy+XixrgOhPj5olKdD/57x7TD5nQpAUdbCQvwRafVcFuPWLdW6TcnpTGwSxQ+GjVBtS2DLhccyi9G76NRuorruogrqmv4qbZVzk/nQ5BBT4ifnv8dM8Btnf/fqP4E6H3w1/m4tS4C/FxU7ra9VVYbf/30e/766fdU1Z4Pe3PPUFVjJyUmTGldvTG+Ayarjf213ci0sL4cPl3sNpbTU4Deh0BfHbEhAbw6fqDbdk0bcgM+GrVbi6UQQrTUtRlVXYd69uzJ9OnTCQkJ4eOPP+bzzz/3zHJVna3t2okPC2ww/qklHLVdzU3hqL1BRKdx/zK+3jVnH1wrDpwq4ozJQkK7IP42bgCLJqYxIy2FQIOOn86c49QlgskrLeN0KUUmi9JVXNdFnFl4TmkBbEsF5VUUlFcRG2IkJjiA8IDzXfkt+fzrsb4IIbzDdRMMqtVqunTpgsPhoKqqyjPZTc+ePdHpGg4Yr9NYt/Tl6N+/P48++ihGo5GPPvqIzz77zDPLVXey1ISlxk6X9sF0DPllXFpjLDYHOq2Gbo10rXVtF4Svj4aT584HDdU2B346LZ09usBUqvPdeQB5pSYsNgeVNTYMWo1bC41Wo+aGqPOD8a8lTd2u601yVBjhAb4UV1qUsaNVVhtfHMvjnV3HWr210lNltQ2708mGg9nKeMj604xV293u3K2oriHzzDmlqzgxIgSNSsV3J37pNbDa7Fhqzne3//dH58fVeU7zNv+A1e5Qlmkqu/N8F7bBR0tiRAhd2wej12rcPp82qi/VNjvVNjtZReXMXL29wTZNW7mNJTuOeC7WZs5UmPnTum9YvfcnzyQhxHXumg0G1Wo1YWHnu4YiIyOZMWMG8fHxmEwmjhw5fwEsKCigpqaGsLAwhg4dil6vZ8qUKQwfPhyt9pfu0PoqKyvR6XQMGDCAyMhIz+Rmu/XWW/ntb3+LTqfjww8/bFKL4NChQ1m0aBEvvvhig7uo20puSQUnSirw12l5fHAyN3eOUIIyvVZD/cbCzIJSqu0OBnWNYmCXSLQaNWqVils6R3Jbj45U2xwcrb2L81D++S7XcSmdSY4KQ61SodWoGZXcib5xHSi31JBVXEGV1UaRyUKwn547e3ZEq1ETGxLAH4b3JinqwoPgtRo104bcwMKJQ5g66Je7mNtaU7fretMvrj3VNger9/7E7E92M33VNv647hvWH8i6YJdloK+O/76zL2/cP4TxfbpcVsvyyXMmTNU2hifGMrR7TIOu28bsP1mEqrZ7ODEihOKqauXHCLUBW3ZxBWH+vvy2fyIxwQGXVUZPGadLKa60cGN8B27pHEl+WaVy40qdtqgvFdU15JdVER8WyKQbuxEe0LQu4dY8XvUlRoTwyr0DeXJoKr71hpsIIa5/l/Vu4gkTJjBr1izGjh1LQkICAAkJCYwdO5axY8fi6+urPGMvLS0No9HI0aNHyc7OVtbRv39/IiIiyMrKIiMjg/bt29OvXz8CAwO56aabGDt2LMOGDaNDhw7Y7XY+/fRTfvjhB6gdV9inTx/CwsJISUlh1KhRxMXFYbfbcTqdWK1WvvvuO0ymX36RR0RE0KVLF9q3b8+wYcOUsnbs2JE9e/Yo+ZrqkUceITw8HK1WS3JysrK+uql3795s377dbZnRo0cTExODwWDAbDZz7Ngxt/SWSI4KIyrIn13Zhcp4p/qcrvOtg0lRoYT6+5IaE86dSXGMuiGegV2i0Gk1lFRV892JM5Saqwn21ZMQHkhKdDgjk+K4KzmelJhwNGoV32QV8E32+Wc9FlaYiQsNJDY0gBvjO3BXcjwjk+Lo2j4Ypws+PZLD0dq7U801NlJj29E5PIiRSXEM6hKF0VfHT2fLCPTVcazwXIN3E4f66xmZFI+vj5YQPz0niiua3T1XZ0CnCAJ9dezKLqTG4VQC4vr/A+w+UcjJc5VN3q4AvQ+3dI7EVF1zyXewDugUQYifnn0ni9zeG50cFUZ8WCBHTpeQW2rCX+/Dn+/oy8R+3Rh1QzzxYYFo1Gr6x3dg1A3xDOkWzaH8EqqsNtobDfTt2J4yi9Xt8+vKZbU72H2iEJvDSfcOISS0C6J/fAeGJ8YyKvn8+2PvTIqjb1x78s5VNnj4eKfwQIZ0jVbGqB04VYS5pmXPt6tb7obocJKjwpQ6WDf17tiOH04WYasXmJZbrCRGhNApPJAwfwM7fz7Nj2fOjxesc7qsip6RoSS0C2JQ1yhlu+qm+u+9rttfp8oqm/SeZ6vdQUSQHzdEhRHsp+fLzFMNxpk25zzwPNZ14sKMJEeFkVtqUspVWGGmV2w7urQPZmj3GLdtutD7x1vzeNU3PDGWLu2D8fXRsi+vqNHrTH3NOS+EEFfXlWlmuUwOh4PCwkLee+89t5a3yspKVq9eTX5+Pk6nE6fTSX5+Pu+88w5FRb8M7q5vw4YNbNu2jcrKSlxt3Sd2AVfj0TLUfrG8uOkHvjh2ElO1Tdl+c42dw/klfFL7iBeXCz7Y9zPrD2RTZrHicrlwulycMVn417fHWLv/Z6U70e5w8va3R/ni2Enly8HhdJFbYmLBVwf56vgp5fOPFZ7jgx9+orz2AczllvMP2f0y85c8noorq9n24ylcLhcqQNNazRyX0Jztup5sycyjwlKDw+miqsaOs/ZAatQqooL8eWxgUoMbHY6fOac8c8/zzuiW+OrHU7y76xhnKsxNGkdnd7o4crqEcH/f2ptKzrdK11dRXcOCrQfZm3u2VQIfT/tPFmF1OCm31JBR0PDz26q+5JdV8sZXB8k4Xdrkbu7WPl51Dp4qxlJjJ/9cJeWWtr3RSAhxZbXaG0hai+fbQ/LzL/18NXHpN5Bcr/RaDYO7RDEmtROmahvzv9zf4pZBb+fro2Xm0FRC/fQs3nGYnJJfWqX8dVqmp6USFxrAyu9/YmfWL2+C8df7MCo5nrRu0eSdq2T+lweu6JsvrgV9OrbjdwN68H3uWVZ8l+mZfE25Vo6XvIFEiOtH6/xkFKINPDkslfn3DeI3vRNQoeLLzDwJBC9DkEFHeIAvarWKQF+9Mv5Sq1HTKTyIUD891TYHBRXnb9Dy1/vwP3fdyD/uuZW0btFU2x383+ETVyWwuFrqxspOvrE75hp7i1v4rgQ5XkKIlpJg8FckQO/D7FH9eXNSGq/dN4ikyAvfmHG9sDtd5JaaeP2rg2y9hr+IrweVVhtlFisBeh8eH5zMggmDeXNSGgsmDGbakBsw+vpwML+4wc0RVruDjNOlvLx5H4cb6aL9NaoLrBZOHMJvb+qOTqth87GT19yjdxpzNY9XUmQor903iDcnpTG79hmPQohrnwSD4pr1+taDzFy9nXmf/8DPtQ8YFi1XZbXxRu24uirrL2NGHU4XZyrMrNz7I//57rgyHrTuIcyzPtjBou2HOF1+8Uc6/Ro5XS6KKy2s+C6Tr368tn+MyPESQrTUNTdmUAghhBBCXDnSMiiEEEII4cUkGBRCCCGE8GISDAohhBBCeDEJBoUQQgghvJgEg0IIIYQQXkyCQSGEEEIILybBoBBCCCGEF5NgUAghhBDCi0kwKIQQQgjhxSQYFEIIIYTwYhIMCiGEEEJ4MQkGhRBCCCG8mASDQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwotJMCiEEEII4cUkGBRCCCGE8GISDAohhBBCeDEJBoUQQgghvJgEg0IIIYQQXkw1YMAAV/0ZL89/hWdmPV1/VqP+9l/fe866oOfevtFzlhBCCCGEuAZcVjA4f+0NnrMbmDX+sASDQgghhBDXqFYNBjWJk9AYI0ClUualdi5h97H2bvkaUIHOYMBhOk3xN//xTBVCCCGEEG2k1YJB181LMMR2w9AuwC1fUxkMKnx9ofToPn7+10zPZCGEEEII0QZa7QYSG5do/bsEh8OFzQH6qGTPJOEhJSWF+fPns2jRIoYOHeqZLIQQQgjRZK0WDLpc4HC6sNtbNlksLixVTixmp+eq20xSUhILFixg3rx5REdHeyZfsxITEzEajej1epKSkjyTxa/YiBEjWLx4MTNmzPBMuqJmz57N0qVLefnll5kyZQrh4eGeWYQQQlwnWi0YBMAJOF0NpnC9i//qoWFpmg8vDdAyIUGNXtUwn73Ghc3q1mvdbLNnz2bBggUXDJLqvkwnTJjgmdTqpk2bxj/+8Y8LlqWlMjMzMZlMmM1mDh8+7JksriEjRoxg/vz53HfffZ5J1z21Wk1wcDCDBw/m2WefJSUlxTOLEEKI60CrBoOu2hbC+lOEQcXfb9WRFq3GoIWYABVjOmmY2FXbIK/r8uLAJtHr9ajq3eDSltq1a4der/ecfdkOHTrErFmzmDlzJtu3b/dMFteQ0NBQjEbjFatzV8qcOXOYNWsW//nPfzhz5gxBQUFMmDBBWgiFEOI61HrBoKvxVkGj1oWP+vy//zlmZ9/Z893At0bVzqybWikiPHfuHDqdjsjISM8kAPz9/XG5XJSVlXkmCSGawWQysW3bNubOncvp06cJDw+nX79+ntmEEEJc41rtbmLrTevwjwxEH+Lvlg+go1GFzQkFVS5ujtIwo5cPAOlfVGOxnc+jUoNaDTWVNZxafIf7CpphxowZJCcns3btWrZs2eKZzIQJExg2bBhr166loKCAqVOnYjKZ+PTTTxk5ciTt25+/ESYrK4vly5dTUFCgLBsbG8u4cePo3r07vr6+uFwuqqqq+P7771m1ahVOp5MJEyZw++231/vEhg4cOMDChQs9Z1/SjBkz6NWrl/K/3W6/4Haq1WpGjRrF4MGDCQoKQqVSUVlZybZt2/jkk09wOls2NvOWW25h+PDhREVF4ePjg8PhoKioiI0bN7Jr1y4l3+zZswkLC2PlypUMHjyYzp07o1arKSsrY926dW55m0OtVjNmzBjS0tIICAjA5XJRVFTEZ599xs6dO93yxsbGcv/995OQkHDRsl4ob3Z2Nu+//z55eXlueS8lKSmJqVOn4ufn55mkKCkpYcGCBeTn58MFPr+xso4YMYLx48dz5MgRpQ4ZjUaefPJJ4uPjycnJYfHixRQXFyvrfeCBB+jcuTMajQaLxUJGRgZr165V8tSV9+zZs2zbto2xY8cSHBwMFzgPGjNz5kxSUlLYvHkza9as8UwWQghxDWvllsHGWwdPljspMDkJ0cGYzloAzppdWKwN8+K8vJZBq9UKtUEDwEMPPcTChQuVFguDwYDT6cRisSjLGI1GHnjgASIiIlCr1ajVarp27crEiROVPADjxo2jV69eGAwGVCoVarUao9FIWloa48aNc8t7td13332MGTOGkJAQ1Go1KpUKo9HI8OHD6dGjh2f2JomOjmbcuHHExcXh43M+oNdoNERERDBhwgQSExPd8ut0OiZPnkzXrl3RaDSoVCpCQkK49957iYuLc8vbVBMnTmTUqFFK16taraZDhw5MnjyZO+745UdEXFwc06dPJzExsUFZp0yZ4pY3OjqaadOmNcjbtWvXK3Jcm1NWT0ajkfT0dOLi4sjJyeH1119Xgry69dbtf2rrf79+/Xj88ccxGo1u64qMjOS3v/0toaGhFz0PPN1yyy107twZp9NJTU2NZ7IQQohrXOsFg4DL5cLlbHzSqVz8sb+OuMDzY6c++tHWII/LeX4dl6O8vBytVktwcDBqtZqOHTvi6+ur3MRhNBqx2+1u3cR1LTifffYZv//979mwYQN2u53IyEhiYmKUfNXV1Rw4cIBXXnmF9PR0Zs2apbTa1K1/zZo1pKenk56eTl5eHmazmfnz5yvz0tPTW9QqCLBw4UJlHQcOHPBMdpOYmIjNZuP999/nscceY/r06axYsYLCwsIWtwpS2w2/ZcsW/vznP5Oens5LL73EqVOnCAgIIDnZ/bFAWq0WnU7HwYMHefbZZ3n55ZcpLS0lICCA7t27u+Vtit69e3PTTTdRXV3NqlWrmD59OrNmzWLHjh2oVCpuueUWAgLOP+fynnvuITw8nDNnzrB48WKlrFlZWWi1Wre8PXr0ICQkhJ9++knZrhdeeIFdu3ZRXV3tUYpLy8jIYObMmaSnp7N582YANm/e7FYH/vznPyutgs0pa31qtZqHHnqIHj16cOLECV5//XVMJpOSPmbMGMLCwjh69CgvvPAC6enpLF68mDNnzhATE8Ott97qtj69Xk9NTQ2ffPIJ06dPv+B5QG0APW/ePJYtW8YjjzxCQEAAFRUVckOTEEJch1o1GGxwN0i96fd99cQFnv+4Xfl2tuXaGuQ5P3mutOUSEhIIDAzEYrHQsWNHpbXQk81m4+OPP+bDDz/EYrGwb98+ysvL8fX1JSgoSMm3bNkyFi5cyLFjx6B2zNTu3buprq5WWl6uFTU1Nfj4+JCUlERoaChWq5Xt27fz4osvKuVvrvz8fObOncuqVasoKSmB2m7Eo0ePKq109blcLg4dOsSiRYsoLi4mMzOTkydPKsF6c3Xr1g2DwcC2bdvYsmULVqsVk8mkBLkhISHExsbSqVMnoqOjMZlM/Oc//2Hv3r1QW9b333+f8vJyQkJClNZJi8WC0+mkffv2Sqtpbm4ub7/9NsuWLXMrQ2trblnrqFQqnnjiCVJSUsjJyeGNN95wCwSjo6OJjY2luLiY5cuXk5ubC8DevXuVm446d+6s5Kf2PNi4cSMbNmzAarVe8Dyg9kdXXSu8w+EgJyeHd955h6ysLLd8Qgghrn2NR0ct4HK5zsdyHq19Wlz8ob+ePhHng6WdJ+0s2FPdIF9rdROXlZVht9vRaDTExcXhcrk4duwYoaGhJCQkoNfrqa6upry8XFmmoqKCI0eOuK2nMbGxsTzzzDMsWrSIZcuWsWzZMmbNmnXRsWFXy8aNG6moqCA1NZUXX3yROXPmcPfdd1/23c0DBw7kr3/9K0uWLFH2wYXGSDocDjIzMy+rJbK+du3aoVarueuuu5TPXrZsGUuXLiUmJgatVktoaCh+fn7odDrKysoaBL65ubmUlJS43WS0a9cuDh48iNFo5He/+x0LFixgxowZrf5IoMY0t6x14uLiSE1NJT8/v0EgCBAcHIzBYKBdu3bMnTvXbX9NmDABrVbbIMBr6nkAUFlZicPhoKSkhP/93//lr3/9K0ePHvXMJoQQ4jrQasEgNN4y+F+99PSLPD9OsNTi4uuTNfQIVxOibzw/rssLBusCj7CwMDp16qQ8i0+j0dCtWzflpoPmdkfXH9d1uQHVlXDo0CGee+451q1bR0FBAREREYwePZq5c+fSp08fz+xNcscddzB58mQiIiKuuZbQ5nK5XDgcDqitM0uWLGHevHns378fp9NJamoqTz75JNOnT2/Q4nml1S9rncLCQkpLS4mKimL06NFuaUIIIURztN63XO1DBuu39rUzwJC48wPiAUINKv4y0I//b5Afr90RQKxR5d5C2IIgzVNdl59arSY2Npb8/Hyys7OxWCzKDQ52u73ZY8F69+5NaGgopaWlyriu9PR05s+fj9ls9syuaGmXaGuwWq18+umnzJ49m2eeeYb9+/fj7+/f4lfY9enTB61Wy9GjR3n++eeVfVA3Lq6tmUwmHA4H69evdxt/VzdNmzaNb775hpqaGhwOB8HBwQ1ulklISCAsLIzq6mpOnTrllpaVlcWiRYt46qmnePvtt6msrKRHjx4tDp7r82yFq9PSslZXV7NmzRrMZjODBw9ucJOJ2WympqaG06dP89RTTzXYV3XjEi/HnDlz3MY+CiGEuD61XjBI/TuKz08llS5yyhrvItRpIFCncsuvTJehrps4Li6OoKAgTpw4QX5+PmfPniUmJgaDwUBNTY0y5q2pdDodarWaqqoq8vPz0ev1DBkyhPHjx+Pr6+uZHWq70nQ6HQMGDGjQzdeWoqOjefbZZ5k4caLymr2amhry8vJwOp0YDAbPRZpEp9PhcrkoKSmhtLSUsLAwJk6cyE033eSZtU2cOHECp9PJsGHDuOuuuxrcDVsnKyuLoqIijEYj999/vxJkJSQk8Nvf/pagoCAKCwuV8W133303f/rTn+jfv7/S6ltQUEB5eTkajeaCn9MUVVVVOBwOunfv3ugz+Jpb1vr27dvHihUrsFqtjB07lhEjRihpubm5lJaWEhERwaOPPtog0LxccXFxvPjii8yfP1/ePCKEENc5TUxMzP+rP+P2O+9g8+eXbukZ3uc0u492UP63R92P1qBDozvfJUztU2a+zKrhUKGdr3NtbM/5Zfoyq4ZjRb90falU5ydHjYPKQyuV+c3Vvn17+vXrh9FoxGw28/nnn1NWVkaHDh1ISkrC39+fiooKtm/fruS12+189913yrirwMBAbrrpJnx8fPjhhx8oKioiKCiInj17EhoayrBhwxg1ahSpqakEBgZCbauV59tAIiIi6NKlC+3bt2fYsGGMHTuWsWPH0rFjR/bs2eOW91Kio6P5n//5HyZOnMjYsWOVx+AkJycrgcDJkycpKioiMDCQtLQ0kpOTGTp0KGPHjmXUqFHKHbzffvstmZmZnh9xSV26dCE2Npa4uDhGjRrFbbfdRufOndHpdABkZ2eTkZEBQFpaGkajkaNHj5Kdna2so3///kRERJCVlaXkbaq8vDzi4+OJi4ujZ8+e3Hnnnco+HTt2LL1792b79u3K8x979OhBeHg4t9xyC2PHjmXQoEEEBQVRWVnJ+vXrOX36NAApKSn079+fvn37MmrUKMaOHcuQIUMICgqiqKiIjz/++KKtvxej0WhITU0lKCiIfv36KWW99dZbOXbsGBUVFc0qa+fOnenZsydnz55lz549FBQU4HK56NGjBwkJCZw9e1aZV1VVRc+ePYmJiVHWWzfdddddVFdXk52d3azzoM6NN95I3759MRgMlJeXN/tYCiGEuHa0Wsugy+HEZbc3eGagy+nieJGdo4U2t+nHooZ5cV7+3cRlZWXKMwRLSko4ceIE1D7uo6qqSpnfXN9++y2bNm3CZDLhcrlwOp2UlJSwcePGC77NZMOGDWzbto3KysrL7v5ujvz8fD755BMKCwuVsWYOh4PCwkKWL1/O+vXrPRdpkjVr1nDgwAFstvNPCrfZbGRnZ/PFF180GNPWFpxOJ2+99Rbbtm3DZDJd9MaUffv28a9//YucnBylbDabjczMTF599VX27dun5N26dSt79+7FYrEoQxUsFgt79+7ltddeU57b1xKZmZmsWbOGM2fOXLC8zSlrYz7//HN27NiBn58fDz74oNKtXX+9dcestfz000+UlpZiMpla9MNCCCHEtaPV3kBiTl2NLkCPPtAXtc8vrYNNpVaBWqPCZrFRuOpuz2QhhBBCCNEGWq1l0OmwY7fYsJZXYy2zUFPevMlaYaG6zILlbI7nqoUQQgghRBtptZZBV0h/rNFTQaPm/DtGzq/WT2+nqvqXO4ovSAUqpxnLvnnYzWc9U4W4qjzfC30xLX33tBBCCHE1tFoweCGzxh/mubdv9JwtxHVFgkEhhBC/VhIMCiGEEEJ4sVYbMyiEEEIIIa4/l9Uy2FTSMiiEEEIIcW1qcTAohBBCCCGuf9JNLIQQQgjhxSQYFEIIIYTwYhIMCiGEEEJ4MQkGhRBCCCG8mASDQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgkEhhBBCCC8mwaAQQgghhBeTYFAIIYQQwotJMCiEEEII4cUkGBRCCCGE8GISDAohhBBCeDEJBoUQQgghvJgEg0IIIYQQXkyCQSGEEEIILybBoBBCCCGEF5NgUAghhBDCi0kwKIQQQgjhxSQYFEIIIYTwYhIMCiGEEEJ4MQkGhRBCCCG8mASDQgghhBBeTIJBIYQQQggvJsGgEEIIIYQXUw0YMMBVf8bL81/hmVlP158lBADR0dHMnDkTgAULFpCfn++ZpU2Eh4dz//3307NnT/R6PQB2u521a9eyZcsWz+xCXHFX69xoiaSkJKZOnYrFYrnmy9oa6ra3pKSEOXPmeCa3mdTUVO655x4iIiLQaDQAlJSUeMU+F9efywoGJ0yYwO233+45W7F582bWrFmj/B8WFsYjjzxCUFAQ//znP+WEaAV1Fzo/Pz/PJGjkGFyO1vjCi4yMZPLkySQkJODj44PD4SAvL49PPvmEgwcPemZHrVbzxz/+ka5du7rNb2kwqFarGTVqFIMGDSI4OBi1Wo3FYuH48eNs2LCBvLw8z0WEuKTWODdaQ9++fbnvvvvIzc3ln//8p2cySDDomdwmEhMTmTp1Kkaj0W2+BIMNTZs2jU6dOvHuu++SkZHhmSyukCvaTezr60t4eDg+Pj6eScILxMXF8eSTT5KYmKjUAY1GQ3x8PA8//DB9+vTxXIQ+ffoQGxuL2Wzm/fff57HHHiM9PZ3HH3+8RYHgtGnTGDt2LKGhoajV56u/wWCgV69ejBs3znMRIa4rISEhBAcHKy1R4uq46aabMBqNnDp1ipdeeon09HTS09P585//LIGgh3bt2ik9PuLquaxgcM2aNUol37x5M9S2RNXNa60WKXFpeXl5yn6vP10rx0CtVnPvvfcSHh7OmTNnWLx4Menp6bzyyiucOnWKgIAAxo0bR0BAgNtywcHBaLVafvzxR7Zu3YrT6XRLb46bb76Znj17YrFY+PTTT5k1axbp6em88MIL7Nq1i+rqas9FhBCi2YxGI3a7nZ07d5KVleWZLMQ157K6ieur6zJurFtyxowZ9OrVy22ep8aWu5i6bhmz2czx48dJS0vDZrPx0UcfkZCQQN++fTGbzaxYsYJ9+/ZBvbFnycnJ+Pj4YLPZyMrKYvXq1Q26B2+55RaGDx9OVFSU0p1ZVFTExo0b2bVrl5Jv9uzZhIWFsXLlSgYPHkznzp1Rq9WUlZWxbt06t7wAAwcOZOTIkbRr107potyzZw9r1qzBarW65W2K5nSBtKSsd955J+3atUOj0SiB2Llz55rd1ZGSksKjjz6K3W5nyZIlZGZmKmkJCQk88cQT+Pr68u6777J3714lbcSIEYwfP54jR46wcOFCZX5L1K2rsLCQ//3f/71kYKlWqxkzZgxpaWkEBATgcrkoKiris88+Y+fOnW55w8PDGT9+PElJSRgMBpxOpxJc7tixgw8++ABqj0FkZGSDLu66c8TzPBg4cCBjx44lODgYgPLycr7++ms2btyolL9uuw4fPsypU6eU8jocDg4fPszy5csxmUzKOqkdzzRmzBhiY2PRaDTYbDaOHDnSIG9sbCwPPPAAnTt3RqPRYLFYyMjIYO3atRQXF7ut82qp2/6vv/6a+Ph44uPjKSoqYvXq1dx7771ERkaSk5PDG2+8gclkQq1Wc9tttzFo0CClbttsNvLz81m3bh1Hjx5V1j1jxgy6devGsmXL6NatG4MHD8bPzw+r1cq2bdv48MMPcTqdF+wmfuCBB0hLS1Natvfs2QMXqC8FBQWsW7eu0eESF1P32WFhYZ5JCrPZzJIlS8jIyFCuGSaTiU8//ZSRI0fSvn17ALKysli+fDkFBQXKss05D5qjKXWrrqxnz55l27ZtbufChcrqOQzEbrejVqvJz8+/5DXyQho7XxqrL3VmzJhBcnJyg/O8pWJjYxk3bhzdu3fH19cXl8tFVVUV33//PatWrbrktexCYmNjuf/++92G7WRnZ/P+++83+E5szWvRpYaYARw4cMDtmt8W9QVAr9czfvx4brzxRvz9/XG5XJSVlbF+/Xq378S2Og+aWreaei1qKU1MTMz/qz/j9jvvYPPn51v5miMpKYmEhASysrIa9Pv379+fiIgIt3meGlvuYgIDA7npppvw9/enc+fO6HQ6fHx8iIyMpFOnTmi1WvR6PT4+PuzZswej0ciMGTPo0aOH0oWi0WgIDw8nOTmZn376ifLycqi9uD700ENERkYqedVqNQEBAXTt2pXc3Fyl8qWlpREcHExycjIdOnRArVajUqkwGAx07tyZ48ePK+sdPnw4EydOJDAwEJVKBYCPjw8xMTFUVVWRnZ1du3VN1759e/r164fFYmH79u2eyW6aU9ZRo0Zx3333ERgYqHSnqlQqVCoVFouF7777rkGAcTE333wzPXr04Mcff2Tjxo1uaefOnaNbt25ER0dTUlJCcnIys2bNYuzYsSQnJ6NWq4mIiGDs2LHK1Lt370tur6eIiAiSk5MxGo3odDqOHz+Oy+X2W8jNpEmTuP322/H19VW2PSAggKSkJOx2u/KLv+7LuFu3bkr3t0qlwsfHBx8fH7Kzs5W6nZaWhtFo5OjRo27Hu+4cqX8e3HHHHUyYMIGAgADl8w0GA926dSMgIIDDhw8D0LlzZ3r27ElISAg9e/ZUylu33wwGA4cOHVI+a8SIEfz2t78lPDxcObYajYZ27dpRUVGhlCsuLo5p06YRGxur5PPx8SEqKoouXbqwf/9+ampqlPVeLXXbHxwcTFRUFGq1Gl9fX7p27UpERAQqlQqj0ahsW8+ePZk0aRIhISFu2x8cHEynTp04dOgQZrMZao9L+/btiYmJoW/fvuh0OgC0Wi1xcXFUVFSQm5urXI8A5dy4++67GTZsGNXV1Sxfvlz5kRMQEMC0adNISkpyqy+BgYFERUU1u17XffaFxg0D2Gw2fvjhB4qKipRrhl6vJzU1leDgYKV+hYWF0aFDB3bv3q0s29TzoDmaWrfqyhoYGEivXr3w9/e/aFkfeughRowYgZ+fH6raa2zdda6ioqLZ+5baoSq/+93vaN++fYP6csMNN3D27FkKCgqYPXs2Dz74IGPHjiUiIgK1Wk1ycrLbdcvX17dZ33N1pkyZQq9evfDx8VG2X6/XExcXh0ajcftx3VTR0dFMnz5dWQe1+yosLIzg4GDlhwttcC2qixcuprCwUClDW9UXo9HIU089Re/evdHr9W7b1q5dO7f60hbnQVPrFs24FrXUFQkG9+zZw8cff8wPP/xASkoKFouFefPm8d577/Hxxx/z8ccfN1jmUuougMHBwZSVlfHBBx/QvXt3goKCyMvL47PPPiMxMRGn08n27du599576dWrF/n5+bz11lu88847ZGRkEBUVRVRUFCqVSvnCDAwMpFu3buzbt48lS5awevVqMjIy6NSpE+Hh4ZhMJiViT0tLU75UDh06xGuvvcaBAwdITEzEaDRSVFSkVJJRo0YRERHBF198wfz589mwYQOnTp0iKCiI/Px8cnJy3LaxKeoqf3h4uNtFZ+zYsYwYMYKTJ09SVFQEzShrXFwc999/P3q9nu+++46FCxeyevVqcnJy6NatGy6Xq9nBYO/evUlISODgwYONHuv4+Hi6dOlCQUEBdrv9kheKllzY8/PziYmJITo6mq5du3LzzTej1+s5efIkDofDLW/v3r0ZPXo0DoeDDz/8kH/+859s2bIFf39/YmNjCQ0NZe/evdTU1HD33XfTs2dPSkpK+M9//sOSJUvYunUroaGhREdHu50TTQ0Go6OjmThxInq9ni1btrBo0SI++eQTqP2F3L59eyV4r7sA6/V6KioqWLVqFW+99RaBgYHExsbi4+PD119/jcvlIjo6mgceeICAgACys7P517/+xTvvvMNPP/2En58fhYWFygXlwQcfpFOnThw7dozFixezYsUKTp8+TWxsLBEREZjNZn7++WdlG66Wuu339/fn4MGDHD16lISEBHx9fdm+fTtFRUXExsZy+vRpMjIyCA8PJy4ujq1bt7J06VLWrl3L6dOn6dy5M8HBwRQXFyvnYv/+/YmJiSEoKIjs7GyWLFnCqlWr6NixI5GRkTgcDr7//vsGweAtt9zC6NGjqa6uduudAOjatStpaWmUlJQwf/58VqxYwTfffINarcbHx8fty6opTCYTW7Zs4eOPP8ZsNpOYmMihQ4f4n//5H+X6+tlnnynXgbprhp+fHw6Hgy+++IIFCxZgs9no0qULOp2OY8eOUVFR0azzoDmaWrfqymowGKiurmbTpk0sWLAAu93eoKz9+vVj5MiRuFwu5ZxZt24dJSUldO/eHbPZ3OxrRkBAAI888ghhYWHK8f/3v/+tlDU0NBQ/Pz92795NWloaQUFBnqtw09j3Y1OkpKRQXFzMe++9xzvvvMNXX31FUFAQMTEx6PV6vv76a89FLummm26ib9++ZGVl8fe//53Vq1dz8OBBdDodDodDqbNtcS06cuSIUjd79+6Nj48PCxcu5O2331bm1w9G26K+ANx777307t2b6upqvvjiC958800+//xzzGYzer2e77//Hpr5fdBUzalbNONa1FKXNWbwWlA3LqO0tBSXy6U0mRYWFmKz2aD2106XLl2oqqpi9erVHDt2DGpPzI0bN2K1WomLi1PWmZ+fz9y5c1m1ahUlJSVK3qNHjyq/cupzuVwcOnSIRYsWUVxcTGZmJidPnkSr1SpN1ABWq1UpS/v27XE6nfzwww+8/PLLfPXVV27rbCtNKWvv3r0JDAwkMzOTt99+W9kHJSUlLW6GvlgXVn1Go9FtLOqqVauw2+0cOHDAbSxkS7p7nE4nS5YsYfny5RQVFREWFsY999zDq6++yoMPPug2iLlbt24YDAa2bdvGli1bsFqtmEwmVqxYQWFhISEhIcTGxiqtxVarlXXr1rFnzx6cTicmk4mysjK3z2+OHj16EBQUxIEDB/jggw8wmUxYrVbWr1/Pzz//rLSI11dZWcl7773Hzp07sVqt7N27l+rqagwGA5GRkQD06tWL4OBgTp48ycKFC5Vz4dixY7z55pvKl2V0dDSxsbEUFxezfPlyJUDcu3evksfz8y9lxowZLFu2rEnTjBkzPBe/pPLycj777DPsdjsAZ8+eZdOmTZw7d84t37Fjx/jrX//Kp59+qvyg2bt3LydPnkSlUimtJHVcLhcHDx5k3rx5ZGVlYbVaycjIULofPfXt25exY8c2GggC1NTU4HA4lNYLtVpNSUkJK1euZP78+W5525LNZuPjjz/mww8/xGKxsG/fPsrLy/H19VWCmqaeB83Rkrpls9nYuHEjGzZswGq1NlrW3r174+vry+7du1m7di0mkwmn08m5c+cu2gNwMYmJiYSFhVFcXMxbb72l/LDfu3cv69evx2KxEBERQXR0NHPmzFGuTwcOHMBut7Nq1apWGcO9bNkyt/PVZDKxe/duqqurG9TXprJYLDidTtq3b0+PHj0AyM3N5e2332bZsmVKvra6FjVVW9WXgIAAevTogdPpZOPGjaxbtw6TyaQMn1i0aJGyvrY4D5pTt+o091rUHJe39DWgvLyc/fv3K/9XVlY26G6NjIwkICAAo9HI008/7falM3PmTAwGAwaDwS1gGThwIH/9619ZsmSJkvdCYxwcDgeZmZmXDJQ2b97M6dOnSUhI4Pnnn+ell17igQceIDw83DNrszV2A8nMmTMb/AptSlmDgoJQ146xaS1NbUWsCzzb0s6dO/nLX/7CvHnzOHz4MBqNhsGDB/P0008rj4KoG9N51113udWXpUuXEhMTg1arJTQ0lHbt2uHn50dVVVWr7q/Q0FC0Wi033nhjg0ApJSUFjUbToBXi3LlzDQIPT+Hh4Wg0Go4fP37RYxIcHKx0lcydO9ft8ydMmIBWq23w+VdbTk6OW1dNYWFho+Ma1bVjf/7+97+zdOlSZbsuNK65sXNmy5YtPP744w3GsRoMBm6//XYcDgfvvfdeo8fjp59+4ttvv0Wn0zFmzBjeeOMNnnnmGfr37++ZtU1VVFRw5MgRz9lumnoeNEdL6lZTyhoYGIjD4WgwJuxyBNfewJafn9+gLp04cYKKigoMBoPbj/62EBsbyzPPPMOiRYuUfTVr1qyLDg24lF27dnHw4EGMRiO/+93vWLBgATNmzCApKcktX1tdi5qqrepL3bW7vLy8SXnb4jxobt1qzrWoua77YLAt3HHHHUyePNntYaGtITc3lzlz5vDuu++Sm5tLcHAww4YNY86cOYwYMcIz+1Xj7+/vOeuyVVVV4XK5aF87UN1TREQETqcTi8XimdRmsrKyeP3113nnnXeoqqqiY8eO3HrrrZ7ZLsrX11cZu3E9uJplXbhwYYMfLBeaLvfCdjETJ05kzJgxbo8Xai3V1dWcOHECX19fbr/99gbPmavzwQcfMGfOHHbs2IHFYqFbt248+uij/OUvf7ngMuLiDAaD56wr5mI/ri9XXFwc06dPJzExsVUfwVLXUzJv3jz279+P0+kkNTWVJ598kunTp7f6uXGt0Wq1rfr93lbasm7Vd1WOtk6nu6xfNM1VXl6O1WqlrKyMv/3tbw2+eNLT03nuueeUVqk+ffqg1Wo5evQozz//vJKn7vE5l8PpdLJz507+9re/8dRTT7F161alZSrA47EqV0vdrxTPX1vJyckEBga6zWuqgoICbDYb8fHxDcYDJiYm0qlTJ6qqqjh+/Lhb2pWwZ88ecnNz0Wq1yvaZTCYcDgfr169vUFfS09OZNm0a33zzDRUVFVgsFnQ6ndu+MRqNdOvWrd6nXFhcXBwRHjdYVVVV4XA42LFjR4PPTk9P57HHHuOjjz5yW6Yp6rrMYmJiLnqxN5vN1NTUcPr0aZ566qkGn5+ens5LL73kudg1LywsTGn52LVrl/J4ofTarr3L5XK5+Oijj8jJySE+Pp6HHnrogvu5oKCAf//73zzzzDO8+uqrnDlzhri4OAYPHuyZtdlaK6Bs6nnQHG1Vt8rKylDX3uhXX2pqKr6+vm7zmqquKzU6OrpBD05CQgJBQUGYTCbOnj3rltaaevfuTWhoKKWlpcojudLT05k/f75yo9PlyMrKYtGiRTz11FO8/fbbVFZW0qNHD+W5r211LarPc0hVfW1VXyoqKrBarfj7+7t1xTamLc6Da6Fu1df4VaqN1AVl/v7+DB06tNUuWJdSWVlJfn4+QUFBTJ48mb59+17wAk1tsOpyuSgpKaG0tJSwsDAmTpyoDBBvqT/84Q88/PDDyts0bDYbp06doqamBp1O1yYtci2Rn59PTU0NSUlJ3HLLLRiNRh566CF+85vfKHc/NtfBgwcpKioiODiYBx98UBmj0q9fPx588EGMRiMnT55UxsS0hccee4xZs2bRv39/5Rd23bjBTp06YbfbKS0thdpmeqfTybBhw7jrrrsuWFfz8/MpKysjICCA2267DaPRSL9+/Xj22WeJj4/3zE5JSQlarZbU1FTCw8NJTU3lscceaxAMZmdnYzab6du3L5MmTWrymMtLycnJobq6msTERGbMmKGMlU1ISGDatGkMGTIEaluxS0tLiYiI4NFHH1WO1/XO19cXrVaL0+nk7NmzVFVVERcXx8MPP9zk4P1SqqureeONNzh58iQpKSk88cQTbteboUOH8pe//EW5mQigqKiIkpIS1Gr1ZbVwlZWVYbfbiYyMZPjw4Re9zjVFU8+D5mirunXq1CkABgwYQGJiIpGRkcycOZO0tLQW74e6GyPCw8OZMmWKcr7069ePe+65B71eT25ubpsOb9HpdKjVamUoil6vZ8iQIYwfP77FQS7A3XffzZ/+9Ce362FBQQHl5eVoNBrlWLfVtahOZWUlOp2OAQMGNDqesK3qy5kzZzhz5gy+vr7cf//9DBs2DL1ej16v56677mL69OlK3rY4D66FulWf1nNGczT2rKDbb79dmef5zLTKykpOnTpFZGQk/fv3dxsj45m3tW3atIlOnToRHx/PE0884Zns9vmnTp0iJiaGQYMGMWjQICWP0+ls8UBkan+t9+zZs0FXpMvl4siRI5w5c8ZtfnPExsayrN6g3zot2a8HDx5kyJAhdO7cmUceeUSZX1ZWhsPjrtumqqysZPPmzUyaNImYmBieftr9WZbFxcWsW7fObV5r0+l0JCUlNRgTQ+0xOHr0KFu3boXacYUpKSnK+0Xvuecet/x5eXnKTSzff/89cXFxpKSkKDcAuFwuCgsLG/zi+/nnn0lOTqZHjx7MnTtXyVtVVeX2Y+DYsWN8//33pKWlMXz4cIYPH15vLS1/rdXevXvp1q0baWlppKSkkJKSoqTZ7XZ+/PFHqK3rmzZtYsqUKY3uM3sLXwd4teXn51NSUkJYWBjjxo1ze+uM0+lstS6ZusHlTzzxBKmpqUycOJH3338fah8d0bFjRzp37sxvf/vbBstdavzSxeTk5FBWVkZERASTJk1i0qRJ4PGcweZoznnQVG1Vt77//ntuuukm2rVrxzPPPKPMLyoqanH3anFxMdu2bVOeGNCzZ0+39LNnz/L555+7zWttp06dwmq1EhsbywsvvKDMd7lcl/V9pNPp6NatW6M/ggoLC5XHxbTVtahOdnY23bt3p0ePHm7bV/ecwbaqLwDr1q2jQ4cOhIeH88ADD/DAAw8oafWfs9gW58G1ULfqa9nPpcuwcuVK9u7de0XHhlH762LRokVkZmZivcTDndesWcOBAweUu5FtNhvZ2dl88cUXLQ6GAKX7qG69TqeT0tJSPv7440YDuaul7k6wul9DTqeT3Nxc3nnnncvqlvjmm2946623yM/PV/aj1Wrl0KFDvP7665f1jKSm+PDDD/nmm2+UuwypHZBbWlrK5s2befPNN5X5TqeTt956i23btrnlb8yWLVvYsmWLUqctFovy+ASt1v331ubNm5U77Orn3bJlS4O6tWrVKj755BNKS0sv+vnN1dh6LRYL+/fv57vvvlPy7du3j3/9619udfbXYMWKFfz0009udfDw4cPN7ua5lNzcXNasWYPZbGbIkCFKYLZjxw527NjhVq+sViuZmZm8+uqrLXpmXJ3i4mLef/99cnJyGtSnlmjOedAcbVG38vPzWblyJYWFhTidTux2O5mZmaxYseKyPuPzzz/nvffe48yZM27ny969e3n11Vfb/Lr17bffsmnTJkwmEy6XC6fTSUlJCRs3brysJxZs3bpV+S6uCyzrtuu1115zu6mhsWtGa9mwYQPbtm2jsrLygsFtW9QXLhAXOJ1Ozpw54zYsrK3Og6tdt+prtTeQCCHcXeytPEIIIcS14oq3DAohhBBCiGuHBINCCCGEEF5MgkEhhBBCCC8mwaAQbaTutXoyXlAIIcS1TIJBIYQQQggvJsGgEEIIIYQXk2BQCCGEEMKLSTAohBBCCOHFJBgUQgghhPBiEgwKIYQQQngxCQaFEEIIIbyYBINCCCGEEF5MgsHLNHv2bBYvXsyIESM8k66IESNGsHjxYmbMmOGZJIQQQghxSa0SDE6bNo1//OMfJCUleSZdESNGjGD+/Pncd999nklCCCGEEOIiWiUYbNeuHXq93nP2FRMaGorRaESlUnkmCSGEEEKIi2iVYFAIIYQQQlyfVAMGDHDVn/Hy/Fd4ZtbT9Wc1asKECdx+++2es90cOHCAhQsXKv/HxsbywAMP0LlzZzQaDRaLhYyMDNauXUtxcTEAv/nNbxg5ciRlZWUsWrSI3NxcAIxGI0899RSxsbF8/fXX7N+/n6lTp+Ln56es31NJSQkLFiwgPz/fM6lJUlNTGTNmDLGxsWg0Gmw2G0eOHGH58uWYTCaoHTPYoUMHPvzwQ1JSUujevTsajYaysjLWrVvHrl27AEhKSmLq1KlYLBa3MkVHRzNz5kwMBgNLliwhIyODESNGMH78eA4fPsypU6dIS0sjICAAh8PB4cOH3T6/Lu+RI0eUfW00GnnyySeJj48nJyeHxYsXK/tXCCGEEKK+K9YyGBcXx/Tp0+natSsajQYAg8FAv379ePzxxzEajQBs2LCBI0eOEBoayrhx41Crzxdx3LhxxMbGcvLkSTZs2OC27rYwYsQIHnvsMeLj45Xy+vj4cMMNN3DTTTe55VWr1YwePZqkpCS0Wi0qlYqQkBDuvfde4uLi3PI2R/fu3Rk1apTSBa7VaunVqxd33323Z1aF0WgkPT2duLg4cnJyeP311yUQFEIIIcQFtTgYXLNmDenp6aSnp5OXl4fZbGb+/PnKvPT0dLdWwTFjxhAWFsbRo0d54YUXSE9PZ/HixZw5c4aYmBhuvfVWAJxOJxs2bKC4uJiePXty22230bdvX/r3709FRQUrV67EZDKRkZHBzJkzSU9PZ/PmzQBs3rzZ7fP//Oc/t6hVMDo6mttuuw2dTkdWVhavvPIK6enpvPLKKxw6dAibzeaWX6vV4u/vT0ZGBs8//zwvv/wypaWlBAQE0L17d7e8zeHn54fJZOLdd99l+vTp7NixA5fLRUJCghIk16dWq3nooYfo0aMHJ06c4PXXX1daEIUQQgghGtMwomgD0dHRxMbGUlxczPLly5Wu371797J9+3YAOnfurOTPzc1l48aNOBwObrvtNu6++258fHz44osvyMrKUvK1lV69ehEcHMzJkydZuHAhx44dA+DYsWO8+eabSpnruFwuDh06xOuvv05BQQGZmZmcPHkSrVZLcHCwW97mqKys5L333mPnzp1YrVb27t1LdXU1BoOByMhIt7wqlYonnniClJQUcnJyeOONNyQQFEIIIcQlXZFgMDg4GIPBQLt27Zg7dy7Lli1TpgkTJqDVagkKCnJbZseOHezatYvAwEAiIiI4cuQIn3/+uVuethIeHo5Go+H48eNNCqgcDgeZmZk4nU7PpMty7tw59u3b5zm7UXFxcaSmppKfny+BoBBCCCGa7IoEgy2hVqsJDg5GpVKhUqnQ6/WNdo22BZ1O5znrmldYWEhpaSlRUVGMHj3aM1kIIYQQolGtFl1drEvUbDZTU1PD6dOneeqpp9zG9dVNL730ktsyt912G0lJSeTk5JCVlUViYiITJ050y+PJs3Wxpc6dO4fL5SImJqbNA9DExET8/f09ZzdbdXU1a9aswWw2M3jwYO644w7PLEIIIYQQDbRKpFNZWYlOp2PAgAENxrJROwawtLSUiIgIHn30UXr06OGZxU1KSgp33nknVquVzz77jI0bN1JVVUX//v3p06ePZ3aqqqpwOBx0796dfv36eSY3W05ODtXV1SQmJjJjxgzljuCEhASmTZvGkCFDPBe5pLKyMiwWC0FBQQwYMAC9Xs+4ceMYN24cvr6+ntlbZN++faxYsQKr1crYsWMv+Iq8pKQk/v73v7N06VKmTZvW5gGvEEIIIa5drRIFZGdn43Q66dGjBy+88IIyHrDufblOp5NNmzZhNptJSkri6aefdhs3WP/dvkajkbFjx+Lv78/evXvZt28fhw4d4ocffsDPz4/77ruP6Ohot8/PysrCbDYTHBzM448/rqx33rx5DfI2xd69e5XnA6akpPD888+zbNky/vu//5uUlBR8fHw8F7mk/Px8CgsL0Wq1jBw5kkWLFjFmzBh0Oh1Wq9Uze4vt27ePzz77THncTWPBc1JSEqGhoajVauLi4hoN4IUQQgjhHVolGNywYQPbtm2jsrISl8vtGdaKffv28a9//YucnJwGj2apb8qUKcTFxZGXl+f2PMENGzaQl5dHeHg4v/nNb9xaszIzM1mzZg1nzpxptZs4Vq1axSeffEJpaamyTovFwv79+/nuu+88szfJqlWr+On/b+/eo6Mo7z6Af3d2N9lNdje3zQWXJECKBIIISfStgJAQSBQNl2q5iMABNwFJiuLlVY/todhjVfq2aFoPcEAtVoRSX8NpCy9BqLTwNha5KUQhkJCwDZDLbu57v7x/hAzZ3SgEEoV3vp9z9o/M85vZ3Zmd7HefmWfm7Fl4vV54vV5YLBZ88MEHqK6uDiy9KWVlZTh48CDCwsKwcOHCoEBYUVEhvq/a2lpcunTJr52IiIik44bvQEJEREREt79+6RkkIiIiotsTwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhDEMEhEREUkYwyARERGRhMl++MMf+npO+K91v8Zzq57tOUly0tLSsGzZMoSFhYnTTCYT1qxZ41d3q1i9ejUSExPF1xj490Drfr5uVqsVGzduREVFhV/d7bZeB8LUqVPx6KOPQqFQiNNOnDiB3/3ud35138RgMGDlypUAgJKSEtTV1QWWSFbg5z7wbyIi6t1N9QympaWhpKQEmzdv7vUxZ86cwFloADQ3NwMAPB4PAMDpdPpNx5UQsmHDBqxevVqc1q17O77xxhswGAyBzUS3hevZD4iIKNhNhcH/ryoqKrBy5UoYjUasW7cOVqs1sOSW4vV6AQAtLS0AgPb2dr/pA23NmjUwGo0wGo0wmUyBzaLbbb0OhH379mH58uUwGo3Yvn073G53YAndoO97PyAiul31Sxg0mUxiGOj52LFjR2ApDQB+2dHNEELUiJ1ajMh7HgVkssDm71VIdCLiZ7yM0IThgU1BuB8QEd0Y+eDBg3/ec0LuA3nYW7a356RvFBcXh8zMTNhsNvz9738PbBalpqbi5ZdfxuzZs+FyuVBVVSW2Pf744yguLsbYsWNx/PhxOJ1O6PV6zJkzB4sXL8YjjzyCGTNmIDc3F0lJSaisrITT6YTBYMBPf/pTTJgwAYMGDUJxcTHy8vLgdDoxbdo0FBYWIicnB42Njbh06RKKi4uxdOlSuN1u5OXlYenSpZg9ezays7MREhKCyspKv9fc7XreY2JiIp588kksWrQIs2bNwrRp05CcnIyamppee7+ys7Px/PPPY+LEiTh37hxaW1sDS/pk6NChSElJQW1tLY4dO4aRI0ciOTkZX375pXje3rBhwzBq1Ch0dHQEvY/u9+h2u/Gvf/1L7FGZOHEiiouL8eMf/xj5+fmYNGkSVCoVzp07B5/P71RTUVZWFsLCwnD06FE0NjYGNouuZ73efffdKCgowIIFCzBr1iw8+OCDGDduHJqamr512b2ZMGECXnrpJaSmpqK8vDywGQDw7LPPYuHChWhpaYHJZEJoaCjmzZuHJ554Ao888gjy8/MxZcoUxMTEoKKiQlwHaWlp+NnPfoaUlBSYzWasWLECCxcuRH5+PjIzM2EymXo9VNm9TRoaGnD48OHAZuDKNigsLMTcuXMxa9YsTJ48GWq1Gna73W9b3SiFLg6xDzyD0IQ74W69DPu/KwDf1VAlqLSIySqAPqsAERmzoR2VA0GlhbOxGj6Pf6+mMsqAmMlG6Ccv7aq96wGEJtwJZ2M1vI5Ov1pVwp2IyVmB6AkLEXlluZDJ4GioAnD1sxWiHwrNiInQDJ8AT2czXJZv7nm+nv2AiIiC9UvP4LWcPn0ae/bsAQDk5OQgOTkZAJCRkYF7770XVqsVu3btEr/YpkyZgokTJ0Kr1UIQBMhkMqjVamRmZmL+/Pl+y46MjMT9998PhUIBtVqNadOmIT09HXK5HFqtFuPHjxdrZTIZ8vLyMG7cOCiVSgCAVqvF9OnTkZOT02Op1y85ORlFRUUYPnw45HI5AIivdfny5dBqtYGzIC0tDaGhodDr9cjIyAhs7rMdO3bAaDRi06ZNAIAtW7bcdM9sXl4eFixYgOjoaAiCAEEQEBUVhfz8fMybNy+wvN+lp6djyZIlGDJkiLhelUolhgwZgsLCQqSnpwfO8q0aGhrgcDig0WgCm0QajQZutxsWiwWCIKCgoABZWVnQaDSQyWQQBAFarRbZ2dlYsWIFBMF/94mNjcVPfvITDB06VFxnBoMBc+fODaq9Hg899BAef/xxJCQkiOuge7n9ITRhOOIf+k8odfFoKf8QlkNb4PO4xHa5Woe46c8hbEgGIHQNeBFUGujGPICY7OWQybv2IVzpwYt78FmoE++6WqsMhTrxLsQ//CJCoq8OMFIZRiM272mExg6FTLjyvlQaRIybAfXgNLEOAGymL1D/19fhsbcjetISRP3HXEDW+/sfiP2AiEgKev+v2keJiYlBg0dKSkqQlnb1H/snn3yCU6dOISoqCjNnzkRUVBSmT5+OkJAQ7Nu3D8eOHRNrnU4nKisrsX79ehQWFqKoqAh79uyB2+3GsGHDEB8fL9ZqtVq0t7djy5YtsNls0Ov1qKurw7Zt2+ByuRATEyPWyuVyaDQaVFVV4bXXXsOLL76IiooKKBQKjB07Vqzri/z8fMTExOCrr77CL37xCxiNRmzYsAH19fUYPHgwJkyYEDgLKioq4HA40NzcjC+++CKweUD1tq1WrVrlN8LXYDBgypQpAICysjKsWrUKRUVF2LVrF+x2O9LT08VAPxA0Gg1mzpyJ8PBwcVv1XK8ajQaTJ08OnO1btbW1weFwICwsDAaDATExMXj11VfxyiuvQKPRID4+Hmq1GlarFRaLBbm5uRg9ejRsNht27dqFoqIirFq1CgcPHoTH48GIESOCAqnBYIAgCGL95s2bYbVakZCQgBEjRvjVXktycjKys7Mhk8lQXl6OF154AUajESUlJb32MvZV+A/GIy5vFWRyJZr2r0f7V38DAnp7deNmICRqMDrP/hN1W5/Ghc1Lcan053A2nofqjlSoBo/uKhQUiLrvMcjVEbBdOIGL25/zq5WHRUI7epq4XHXyWEBQwHLofVx4x4gL7zyBSx+vhu3Ccfi8XYM/enK1XMLlna/AcbkS2tG50E9ZBiFEHVhGREQ3qF/C4PXwer3Yvn076uvrMWrUKBQUFCAxMRGnTp3C7t27/Wp37tyJtWvX4ujRo/B6vXA4HCgvL0dbWxtkMpnfZTncbjcOHToEi8UCn88Hh8OBAwcO4PLly3C5rvZydDt58iTeeOMNVFVVoampCfv374fdbkdsbKxfyLweBoMBiYmJaGpqwvvvv4/a2loAwJEjR8RDn8OGDQuYC/j0009RVFSEF154we+Q+a1i5MiRiIiIwIkTJ/CnP/0J7e3tcDgcKC0txblz5xAeHt7r++ovqampiImJQVNTEzZt2iSuoyNHjqC0tBQ2mw0JCQl9GvlcX18Pm82GkJAQMRDqdDpER0cjNTUVSqUSgiDAZrOhvr4eaWlpEAQBBw4cQGlpKRwOh/ij48yZM1Cr1UHroKOjA7///e/F+s8++wxmsxm40qPXF+PGjYNOp8Pp06fxzjvviMsxm803d26cTIaoe+cgetISuDssuPznV2EzBf8gEVQaqAaNgNNiQnP5VnhsbQAAl/kCWo/9GfD5oBqUCgAIiTZAGT0YTosJ5gOb4O6wiLXmQ1vgsbcjJHYoBFVXr6zPaYNMkEOdNAZCaDjg88FlMaHpbxthr/uqx6u4yuvoRGPZm+ioPAj1kAzEP/wi5GFRgWVERHQD+vYN9Q16G0CycuXKoPN0mpqaUFpaCqfTiTvvvBNmsxmlpaVBX256vR7Lly/Hb3/7W2zatAmbN2/GmjVr/Hr5urW2tuL48ePi3x0dHaiurvar6eny5ct+z2c2m2G1WoNC5vWIjIyEWq1GbGwsXn/99aDL6igUCkRERATO9r3qbVsFjuyNjo6GQqHAPffcE9SLOGbMGMjl8gF9X5GRkVAoFKirq0NTU5Nf2/nz59HW1ga1Wo3IyEi/tmsxm83i6QTDhg2Dy+WCx+NBSkoKIiIioFKpxNCl0Whgt9tx5syZwMXg/Pnz8Hq9iIuL85ve3Nzs18ONKyOte9sXriUiIgKCIPT7dQR1dz8EzV258Fpb0Lj3LbjbGgJLAADyUA3kah1CYpIweNHbSDK+Kz5i856CTBEChaZrf5SrdZDJlXDUn4PXafNbjru1Hu72JsjVOshDu8Jge8UncDScgzppLAwL1iFh9hpo06Zds7fP53HBcuh9dJ75B5TRiYjNXQlBFXwaBhER9U2/hMG+0Ov14vl6SqUSKpXKr12r1WL58uXIzMyEWq2G7BYb3Ui3jsAfEdfS0NAApVKJuLg4pKSkiGEzJSVFDMDdYfB69PX5+yI8PDxwUr9o+2IXOk7uhRAWibjpz0EZOSiwZED4vF7xELDH1ob6v76Ohr++DrvpJJS6WETdNx93zPsV1IljAme9SiYg6r75CB8xCS6LCY17S+C139wAGiIi+o7DYPd5UHa7HeXl5dDpdJg/f77fIIsxY8bgjjvugNVqxYcffojCwkIYjUasXr26T1/U1yslJQU6nQ4Oh6PPo3qtViucTicuXryIp59+OqjHzWg04rXXXguc7ZbX2dkJj8eDgwcPBr0fo9GIwsJC7Ny5M3C2fmOz2eD1emEwGKDX6/3aunvx2tvb0dDQe6/WN2lpaYHP58OgQYOg1+tRVVWFyspKxMbGIi4uDj6fD21tXYdDnU4nVCpV0Ll+giCIh4cvXrzo19afuntEA3tgR48eDZ1O5zetT3w+NB/eAcs/3oNcpUX8wy9CnXh3YBW8Lju8Ljsc9Wdx4d1CXNi8NOjRuK/rrilelx0+rxuh8T8I6t1TRg6CUhcHd6dFPNQMdL0O++VKNO59C6b3i9H82TbI5Eroxj4kDkDpSQgNR9wDq6AdmQ1bzdGuQSXWmz93koiIvsMwqNFoxNGpn3/+Of74xz+ipqYGSUlJyM/PF+vUajUEQYDdbhe/bDMyMrBgwYI+HxbsTVhYmDhKefLkyZg5cyYUCgXOnj2Ljo6OwPJvVVtbC4vFgoSEBBQUFGDkyJGBJb3Kzs7G22+/jV/+8pcDOhDjRlVXV8NqtSIjIwPz58/v9fD8QDpz5gxaW1uh1+uxaNEicR1lZmbiRz/6EUJDQ1FbW9vnHwcWiwUejwdJSUlQKpWorq5GVVUV5HI5Ro4cCZlMJl6wuPvwcFZWFqZOnYrQ0FBotVosWbIEI0aMQGdnJ06fPh3wDP2nrq4OTqcTaWlpGD9+PLRaLRYvXozZs2eLPes3o/PcP9FQtg4+jwv6nCehHTXF7xqDHlsrnJZ/IyR2GKInLoJCG+s3f08uy7/hbm9CSHQiou5bALm6K6wqY5IQPWkJhJAwOC5+BZ/bAQCImWyEbtwMyMOjryzBB5fZBK/DCplSBUER0mPpXZe/iX/4JYQOSkX7qU/Q9LeNQYejiYjoxt3UvYl7u9dsT3v37hUv6/DYY48hKysLJpMJb775Jtrb25GRkYHFixdDqVTio48+wv79+5GSkoLi4uKgS7L4fD74fD40NzejpKQEAPzu0RoZGYlly5bBZrP5/W02m7FmzRrxWoa9qampwVtvvSVe2mbOnDnIzc0NLBP1vJdseno6Fi1a1OslS9xuNz766CPs27fPb3r3a/F6vdizZw8+/vhjv/aB0H1P3EuXLgXdp7V7O3avu7q6OnF79TbwwWw2i3XX+gz0rO3Les3Ly8OsWbN6DT4NDQ3YuHGjOGDneg0fPhwrVqyAVqvFxYsXsXbtWoSGhuKZZ55BbGws7HY71q9fj6+//hparRZPPfUUhgwZErgYeDweHDhwANu2bQN6rL/uz9q3+bbPIXrsMxqNBitXrgwapNLS0iLebq0/7k2s0MVBP7UIyigDOr7+FM3l28TrDIZEJyL2gVWQhwX/CPO5nWja/zZsppMAAHXSWOizCyFT+p/2AQDOplo0lq0Tewbjpj8P1R29/HDy+dD25W60fP7f4iT14NGIySqATK6E5X8/QOe5f/rNQkRENy/4m34A5OTk4P7774fdbsfu3bvF0HX06FEcPnwYcrlcvFBzVVUVduzYAYvFAq/XC6/Xi9bWVuzfv79fD8v5fD7YbDYcOXIEGzZsuOGL9x47dgzvvfceampqeh293Jvv89Iy12v79u34y1/+Im6H71pZWRm2bt2K+vp68fm7t9dvfvObPgdBXOkZ7B4oYzKZ0NHRAbPZjJqaGnEkcfdh4vb2dmzYsAFHjhyBzdbVC+X1elFfX48//OEPYhAcKB0dHdi6das4WMXr9aK2thbvvvturxcyv1Hutoauc/dqT8Dncvhd8NlpMaHhf34Nm+kkvK6uXr1vYrtwAo1lb8LReF48N9Br70Dbl3vQsHut3yHi5vIP/Zbp83rgarmEpk83ouWI/w8jj70DrrYGNJStYxAkIhogN9UzeDvp7pHp2VtJREREJHXfSc8gEREREd2aGAaJiIiIJIxhkIiIiEjCJHPOIBEREREFY88gERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYQxDBIRERFJGMMgERERkYT9H70T8vKRjDOBAAAAAElFTkSuQmCC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "X8OyJJV3F70M"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    tokens = tokenizer(examples[\"text\"],truncation=True,padding=\"max_length\",max_length=512)\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dA9g2Tl2F73D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6cced4637b4c4e3fa825bdabf9ba6a40",
            "359ef99acaa74204b8fbe5e350d17946",
            "70766e0e389b4f7eb68fa1fbd9baf0cc",
            "5d134fc9e20d4ee8944b9d78cb9c086f",
            "b91726b6458f4936a4f4a0c209d66995",
            "afff6ba3e5044ee2b10e4d79cfd84d51",
            "7ae585195f1441d69d7429cd6053e530",
            "3ee1c9f974e549f599f6894e89a0b560",
            "024fd7547e73429896dad841a558ac76",
            "032bc8237cc145bc83fe35753afad02a",
            "e534bfd632d64baea011a7171965370d"
          ]
        },
        "outputId": "24371c2e-3f75-4f0b-9cc1-bbe09f3088e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cced4637b4c4e3fa825bdabf9ba6a40"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OTNGYmeSF75s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "267efd22-211a-475a-e0c4-2547945b32be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 80\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5aDv7BsSIZNt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "e55fd334663643bc8c9d135d86e651c1",
            "4b4a4f593287472f96c80f638f7352c6",
            "77fabbee6abb4025adba14782ac11968",
            "284dc483e0e344e3b752c2f564478007",
            "a9d8deaef582424a87c95e8c8d8ff7f3",
            "80581fd7ca8f4cfda68d8495b08ba1b1",
            "ec5d3a333158404ab4e6b463e35d497c",
            "f43184b4293f4fb8b3e4455f1a62a347",
            "41d4e0e188f44c72b00fd7889d256c0e",
            "74bf7893701f42d9be1bb62b9ddc78c0",
            "edbc0728c7e246549c5fc6d8cb858bf1",
            "68666630d4f342f599fc867fc2c68983",
            "c4c9d12917db4e76991a00f3ae00702c",
            "77ab4c65eb6144b6b7935215f4a05ce7",
            "92333c5e4c58468ea095fff15476cfe4",
            "7b7647443cc1482c8310192949e14d7e",
            "5dab3eabc7644c56990db67bc774acfb",
            "1370174729f243d2b2bba9546ba11bf9",
            "6dc2aec43c0e4d63a344226a8fe3166d",
            "cb1803a02b9349d38c1abbba662ecb2b",
            "b1896c3de3dc4e48a28d45b4caa54e3b",
            "910010afdf9d493c9a6a31cc888b1c06",
            "4c48fdc0aaf449b69ed30bd7cf6fdbb9",
            "5198216e58794592bc1ed9113180a88c",
            "e2b83c2cc2ce4c3ea174f61a401faff3",
            "c40448b90fb940c198bd696238ee64be",
            "f47aa85a94e74733b9e81ebcfcb27902",
            "3b12c46bab20477c971163f1a0778121",
            "dc30fb7325684433bcd6c0ea16e6aa69",
            "c6bd48f4482a481097979303c1872feb",
            "b4497261697b4db3ab42b2c34aab499c",
            "f0730413c1bf48e98d1982f845519d30",
            "9520cbe03dcc428dae79c8b0f7c45ed9"
          ]
        },
        "outputId": "4a317cab-78eb-4a10-ce7f-8f93cb81e981"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e55fd334663643bc8c9d135d86e651c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68666630d4f342f599fc867fc2c68983"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c48fdc0aaf449b69ed30bd7cf6fdbb9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TutfMlmaIbQH"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-pharma-domain\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8j12MwmGLA_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "77e7afa7-b0d4-4e74-d526-1ef21533c0bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class TrainingArguments in module transformers.training_args:\n",
            "\n",
            "class TrainingArguments(builtins.object)\n",
            " |  TrainingArguments(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[accelerate.parallelism_config.ParallelismConfig] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n",
            " |\n",
            " |  TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
            " |  itself**.\n",
            " |\n",
            " |  Using [`HfArgumentParser`] we can turn this class into\n",
            " |  [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
            " |  command line.\n",
            " |\n",
            " |  Parameters:\n",
            " |      output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n",
            " |          The output directory where the model predictions and checkpoints will be written.\n",
            " |      overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
            " |          If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
            " |          points to a checkpoint directory.\n",
            " |      do_train (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
            " |          by your training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      do_eval (`bool`, *optional*):\n",
            " |          Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is\n",
            " |          different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n",
            " |          training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      do_predict (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n",
            " |          intended to be used by your training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
            " |          The evaluation strategy to adopt during training. Possible values are:\n",
            " |\n",
            " |              - `\"no\"`: No evaluation is done during training.\n",
            " |              - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
            " |              - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
            " |\n",
            " |      prediction_loss_only (`bool`, *optional*, defaults to `False`):\n",
            " |          When performing evaluation and generating predictions, only returns the loss.\n",
            " |      per_device_train_batch_size (`int`, *optional*, defaults to 8):\n",
            " |          The batch size *per device*. The **global batch size** is computed as:\n",
            " |          `per_device_train_batch_size * number_of_devices` in multi-GPU or distributed setups.\n",
            " |      per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n",
            " |          The batch size per device accelerator core/CPU for evaluation.\n",
            " |      gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
            " |          Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
            " |\n",
            " |          <Tip warning={true}>\n",
            " |\n",
            " |          When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n",
            " |          evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n",
            " |\n",
            " |          </Tip>\n",
            " |\n",
            " |      eval_accumulation_steps (`int`, *optional*):\n",
            " |          Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
            " |          left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\n",
            " |          requires more memory).\n",
            " |      eval_delay (`float`, *optional*):\n",
            " |          Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
            " |          eval_strategy.\n",
            " |      torch_empty_cache_steps (`int`, *optional*):\n",
            " |          Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.\n",
            " |\n",
            " |          <Tip>\n",
            " |\n",
            " |          This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).\n",
            " |\n",
            " |          </Tip>\n",
            " |\n",
            " |      learning_rate (`float`, *optional*, defaults to 5e-5):\n",
            " |          The initial learning rate for [`AdamW`] optimizer.\n",
            " |      weight_decay (`float`, *optional*, defaults to 0):\n",
            " |          The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
            " |          optimizer.\n",
            " |      adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
            " |          The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
            " |      adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
            " |          The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
            " |      adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
            " |          The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
            " |      max_grad_norm (`float`, *optional*, defaults to 1.0):\n",
            " |          Maximum gradient norm (for gradient clipping).\n",
            " |      num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
            " |          Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
            " |          the last epoch before stopping training).\n",
            " |      max_steps (`int`, *optional*, defaults to -1):\n",
            " |          If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
            " |          For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
            " |          `max_steps` is reached.\n",
            " |      lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
            " |          The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
            " |      lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n",
            " |          The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n",
            " |      warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
            " |          Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
            " |      warmup_steps (`int`, *optional*, defaults to 0):\n",
            " |          Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
            " |      log_level (`str`, *optional*, defaults to `passive`):\n",
            " |          Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
            " |          'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n",
            " |          current log level for the Transformers library (which will be `\"warning\"` by default).\n",
            " |      log_level_replica (`str`, *optional*, defaults to `\"warning\"`):\n",
            " |          Logger log level to use on replicas. Same choices as `log_level`\"\n",
            " |      log_on_each_node (`bool`, *optional*, defaults to `True`):\n",
            " |          In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
            " |          node.\n",
            " |      logging_dir (`str`, *optional*):\n",
            " |          [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
            " |          *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
            " |      logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |          The logging strategy to adopt during training. Possible values are:\n",
            " |\n",
            " |              - `\"no\"`: No logging is done during training.\n",
            " |              - `\"epoch\"`: Logging is done at the end of each epoch.\n",
            " |              - `\"steps\"`: Logging is done every `logging_steps`.\n",
            " |\n",
            " |      logging_first_step (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to log the first `global_step` or not.\n",
            " |      logging_steps (`int` or `float`, *optional*, defaults to 500):\n",
            " |          Number of update steps between two logs if `logging_strategy=\"steps\"`. Should be an integer or a float in\n",
            " |          range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
            " |      logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n",
            " |          or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
            " |\n",
            " |          <Tip>\n",
            " |\n",
            " |          `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
            " |          gradient is computed or applied to the model.\n",
            " |\n",
            " |          </Tip>\n",
            " |\n",
            " |      save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |          The checkpoint save strategy to adopt during training. Possible values are:\n",
            " |\n",
            " |              - `\"no\"`: No save is done during training.\n",
            " |              - `\"epoch\"`: Save is done at the end of each epoch.\n",
            " |              - `\"steps\"`: Save is done every `save_steps`.\n",
            " |              - `\"best\"`: Save is done whenever a new `best_metric` is achieved.\n",
            " |\n",
            " |              If `\"epoch\"` or `\"steps\"` is chosen, saving will also be performed at the\n",
            " |              very end of training, always.\n",
            " |      save_steps (`int` or `float`, *optional*, defaults to 500):\n",
            " |          Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`. Should be an integer or a\n",
            " |          float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
            " |      save_total_limit (`int`, *optional*):\n",
            " |          If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
            " |          `output_dir`. When `load_best_model_at_end` is enabled, the \"best\" checkpoint according to\n",
            " |          `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for\n",
            " |          `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained\n",
            " |          alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two\n",
            " |          checkpoints are saved: the last one and the best one (if they are different).\n",
            " |      save_safetensors (`bool`, *optional*, defaults to `True`):\n",
            " |          Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n",
            " |          default `torch.load` and `torch.save`.\n",
            " |      save_on_each_node (`bool`, *optional*, defaults to `False`):\n",
            " |          When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
            " |          the main one.\n",
            " |\n",
            " |          This should not be activated when the different nodes use the same storage as the files will be saved with\n",
            " |          the same names for each node.\n",
            " |      save_only_model (`bool`, *optional*, defaults to `False`):\n",
            " |          When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n",
            " |          Note that when this is true, you won't be able to resume training from checkpoint.\n",
            " |          This enables you to save storage by not storing the optimizer, scheduler & rng state.\n",
            " |          You can only load the model using `from_pretrained` with this option set to `True`.\n",
            " |      restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to restore the callback states from the checkpoint. If `True`, will override\n",
            " |          callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n",
            " |      use_cpu (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n",
            " |      seed (`int`, *optional*, defaults to 42):\n",
            " |          Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
            " |          [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n",
            " |      data_seed (`int`, *optional*):\n",
            " |          Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
            " |          same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n",
            " |          seed.\n",
            " |      jit_mode_eval (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to use PyTorch jit trace for inference.\n",
            " |      bf16 (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
            " |          NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU.\n",
            " |      fp16 (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
            " |      fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n",
            " |          For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n",
            " |          the [Apex documentation](https://nvidia.github.io/apex/amp).\n",
            " |      fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
            " |          This argument is deprecated. Use `half_precision_backend` instead.\n",
            " |      half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
            " |          The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n",
            " |          use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n",
            " |          requested backend.\n",
            " |      bf16_full_eval (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
            " |          metric values.\n",
            " |      fp16_full_eval (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
            " |          metric values.\n",
            " |      tf32 (`bool`, *optional*):\n",
            " |          Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n",
            " |          on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n",
            " |          the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n",
            " |          experimental API and it may change.\n",
            " |      local_rank (`int`, *optional*, defaults to -1):\n",
            " |          Rank of the process during distributed training.\n",
            " |      ddp_backend (`str`, *optional*):\n",
            " |          The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n",
            " |      tpu_num_cores (`int`, *optional*):\n",
            " |          When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
            " |      dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
            " |          or not.\n",
            " |      eval_steps (`int` or `float`, *optional*):\n",
            " |          Number of update steps between two evaluations if `eval_strategy=\"steps\"`. Will default to the same\n",
            " |          value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,\n",
            " |          will be interpreted as ratio of total training steps.\n",
            " |      dataloader_num_workers (`int`, *optional*, defaults to 0):\n",
            " |          Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
            " |          main process.\n",
            " |      past_index (`int`, *optional*, defaults to -1):\n",
            " |          Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n",
            " |          the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n",
            " |          use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n",
            " |          training step under the keyword argument `mems`.\n",
            " |      run_name (`str`, *optional*, defaults to `output_dir`):\n",
            " |          A descriptor for the run. Typically used for [trackio](https://github.com/gradio-app/trackio),\n",
            " |          [wandb](https://www.wandb.com/), [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and\n",
            " |          [swanlab](https://swanlab.cn) logging. If not specified, will be the same as `output_dir`.\n",
            " |      disable_tqdm (`bool`, *optional*):\n",
            " |          Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
            " |          [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n",
            " |          set to warn or lower (default), `False` otherwise.\n",
            " |      remove_unused_columns (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether or not to automatically remove the columns unused by the model forward method.\n",
            " |      label_names (`list[str]`, *optional*):\n",
            " |          The list of keys in your dictionary of inputs that correspond to the labels.\n",
            " |\n",
            " |          Will eventually default to the list of argument names accepted by the model that contain the word \"label\",\n",
            " |          except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the\n",
            " |          `[\"start_positions\", \"end_positions\"]` keys.\n",
            " |\n",
            " |          You should only specify `label_names` if you're using custom label names or if your model's `forward` consumes multiple label tensors (e.g., extractive QA).\n",
            " |      load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to load the best model found during training at the end of training. When this option is\n",
            " |          enabled, the best checkpoint will always be saved. See\n",
            " |          [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)\n",
            " |          for more.\n",
            " |\n",
            " |          <Tip>\n",
            " |\n",
            " |          When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in\n",
            " |          the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
            " |\n",
            " |          </Tip>\n",
            " |\n",
            " |      metric_for_best_model (`str`, *optional*):\n",
            " |          Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n",
            " |          models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`.\n",
            " |\n",
            " |          If not specified, this will default to `\"loss\"` when either `load_best_model_at_end == True`\n",
            " |          or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).\n",
            " |\n",
            " |          If you set this value, `greater_is_better` will default to `True` unless the name ends with \"loss\".\n",
            " |          Don't forget to set it to `False` if your metric is better when lower.\n",
            " |      greater_is_better (`bool`, *optional*):\n",
            " |          Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n",
            " |          should have a greater metric or not. Will default to:\n",
            " |\n",
            " |          - `True` if `metric_for_best_model` is set to a value that doesn't end in `\"loss\"`.\n",
            " |          - `False` if `metric_for_best_model` is not set, or set to a value that ends in `\"loss\"`.\n",
            " |      ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
            " |          When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
            " |          stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n",
            " |          can take a long time) but will not yield the same results as the interrupted training would have.\n",
            " |      fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `None`):\n",
            " |          Use PyTorch Distributed Parallel Training (in distributed training only).\n",
            " |\n",
            " |          A list of options along the following:\n",
            " |\n",
            " |          - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n",
            " |          - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n",
            " |          - `\"hybrid_shard\"`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.\n",
            " |          - `\"hybrid_shard_zero2\"`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.\n",
            " |          - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n",
            " |            `\"shard_grad_op\"`).\n",
            " |          - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n",
            " |      fsdp_config (`str` or `dict`, *optional*):\n",
            " |          Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\n",
            " |          fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n",
            " |\n",
            " |          A List of config and its options:\n",
            " |              - min_num_params (`int`, *optional*, defaults to `0`):\n",
            " |                  FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n",
            " |                  passed).\n",
            " |              - transformer_layer_cls_to_wrap (`list[str]`, *optional*):\n",
            " |                  List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,\n",
            " |                  `T5Block` .... (useful only when `fsdp` flag is passed).\n",
            " |              - backward_prefetch (`str`, *optional*)\n",
            " |                  FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n",
            " |                  `fsdp` field is passed).\n",
            " |\n",
            " |                  A list of options along the following:\n",
            " |\n",
            " |                  - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n",
            " |                    gradient computation.\n",
            " |                  - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n",
            " |                    parameter's gradient computation.\n",
            " |              - forward_prefetch (`bool`, *optional*, defaults to `False`)\n",
            " |                  FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n",
            " |                   If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n",
            " |                   forward pass.\n",
            " |              - limit_all_gathers (`bool`, *optional*, defaults to `False`)\n",
            " |                  FSDP's limit_all_gathers (useful only when `fsdp` field is passed).\n",
            " |                   If `\"True\"`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\n",
            " |                   all-gathers.\n",
            " |              - use_orig_params (`bool`, *optional*, defaults to `True`)\n",
            " |                  If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n",
            " |                  frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\n",
            " |                  refer this\n",
            " |                  [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n",
            " |              - sync_module_states (`bool`, *optional*, defaults to `True`)\n",
            " |                  If `\"True\"`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\n",
            " |                  ensure they are the same across all ranks after initialization\n",
            " |              - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)\n",
            " |                  If `\"True\"`, only the first process loads the pretrained model checkpoint while all other processes\n",
            " |                  have empty weights.  When this setting as `\"True\"`, `sync_module_states` also must to be `\"True\"`,\n",
            " |                  otherwise all the processes except the main process would have random weights leading to unexpected\n",
            " |                  behaviour during training.\n",
            " |              - activation_checkpointing (`bool`, *optional*, defaults to `False`):\n",
            " |                  If `\"True\"`, activation checkpointing is a technique to reduce memory usage by clearing activations of\n",
            " |                  certain layers and recomputing them during a backward pass. Effectively, this trades extra\n",
            " |                  computation time for reduced memory usage.\n",
            " |              - xla (`bool`, *optional*, defaults to `False`):\n",
            " |                  Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\n",
            " |                  and its API may evolve in the future.\n",
            " |              - xla_fsdp_settings (`dict`, *optional*)\n",
            " |                  The value is a dictionary which stores the XLA FSDP wrapping parameters.\n",
            " |\n",
            " |                  For a complete list of options, please see [here](\n",
            " |                  https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).\n",
            " |              - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):\n",
            " |                  Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n",
            " |                  used when the xla flag is set to true, and an auto wrapping policy is specified through\n",
            " |                  fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n",
            " |      deepspeed (`str` or `dict`, *optional*):\n",
            " |          Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n",
            " |          evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
            " |          `ds_config.json`) or an already loaded json file as a `dict`\"\n",
            " |\n",
            " |          <Tip warning={true}>\n",
            " |              If enabling any Zero-init, make sure that your model is not initialized until\n",
            " |              *after* initializing the `TrainingArguments`, else it will not be applied.\n",
            " |          </Tip>\n",
            " |\n",
            " |      accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):\n",
            " |          Config to be used with the internal `Accelerator` implementation. The value is either a location of\n",
            " |          accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,\n",
            " |          or an instance of [`~trainer_pt_utils.AcceleratorConfig`].\n",
            " |\n",
            " |          A list of config and its options:\n",
            " |              - split_batches (`bool`, *optional*, defaults to `False`):\n",
            " |                  Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n",
            " |                  `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a\n",
            " |                  round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set\n",
            " |                  in your script multiplied by the number of processes.\n",
            " |              - dispatch_batches (`bool`, *optional*):\n",
            " |                  If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\n",
            " |                  and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose\n",
            " |                  underlying dataset is an `IterableDataset`, `False` otherwise.\n",
            " |              - even_batches (`bool`, *optional*, defaults to `True`):\n",
            " |                  If set to `True`, in cases where the total batch size across all processes does not exactly divide the\n",
            " |                  dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\n",
            " |                  all workers.\n",
            " |              - use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n",
            " |                  Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n",
            " |                  training results are fully reproducible using a different sampling technique. While seed-to-seed results\n",
            " |                  may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n",
            " |                  also be ran with [`~utils.set_seed`] for the best results.\n",
            " |              - use_configured_state (`bool`, *optional*, defaults to `False`):\n",
            " |                  Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n",
            " |                  If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues\n",
            " |                  with hyperparameter tuning.\n",
            " |      parallelism_config (`ParallelismConfig`, *optional*):\n",
            " |          Parallelism configuration for the training run. Requires Accelerate `1.10.1`\n",
            " |      label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n",
            " |          The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
            " |          labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n",
            " |          label_smoothing_factor/num_labels` respectively.\n",
            " |      debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n",
            " |          Enable one or more debug features. This is an experimental feature.\n",
            " |\n",
            " |          Possible options are:\n",
            " |\n",
            " |          - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n",
            " |            the event\n",
            " |          - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
            " |\n",
            " |          The options should be separated by whitespaces.\n",
            " |      optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"` (for torch>=2.8 `\"adamw_torch_fused\"`)):\n",
            " |          The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n",
            " |          \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n",
            " |          for a full list of optimizers.\n",
            " |      optim_args (`str`, *optional*):\n",
            " |          Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n",
            " |      group_by_length (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
            " |          padding applied and be more efficient). Only useful if applying dynamic padding.\n",
            " |      length_column_name (`str`, *optional*, defaults to `\"length\"`):\n",
            " |          Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
            " |          than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n",
            " |          instance of `Dataset`.\n",
            " |      report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n",
            " |          The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
            " |          `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n",
            " |          `\"swanlab\"`, `\"tensorboard\"`, `\"trackio\"` and `\"wandb\"`. Use `\"all\"` to report to all integrations\n",
            " |          installed, `\"none\"` for no integrations.\n",
            " |      project (`str`, *optional*, defaults to `\"huggingface\"`):\n",
            " |          The name of the project to use for logging. Currently, only used by Trackio.\n",
            " |      trackio_space_id (`str` or `None`, *optional*, defaults to `\"trackio\"`):\n",
            " |          The Hugging Face Space ID to deploy to when using Trackio. Should be a complete Space name like\n",
            " |          `'username/reponame'` or `'orgname/reponame' `, or just `'reponame'` in which case the Space will be\n",
            " |          created in the currently-logged-in Hugging Face user's namespace. If `None`, will log to a local directory.\n",
            " |          Note that this Space will be public unless you set `hub_private_repo=True` or your organization's default\n",
            " |          is to create private Spaces.\"\n",
            " |      ddp_find_unused_parameters (`bool`, *optional*):\n",
            " |          When using distributed training, the value of the flag `find_unused_parameters` passed to\n",
            " |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
            " |      ddp_bucket_cap_mb (`int`, *optional*):\n",
            " |          When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
            " |      ddp_broadcast_buffers (`bool`, *optional*):\n",
            " |          When using distributed training, the value of the flag `broadcast_buffers` passed to\n",
            " |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
            " |      dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
            " |      dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):\n",
            " |          If True, the data loader will not shut down the worker processes after a dataset has been consumed once.\n",
            " |          This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\n",
            " |          increase RAM usage. Will default to `False`.\n",
            " |      dataloader_prefetch_factor (`int`, *optional*):\n",
            " |          Number of batches loaded in advance by each worker.\n",
            " |          2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
            " |      skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
            " |          down the training and evaluation speed.\n",
            " |      push_to_hub (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n",
            " |          `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n",
            " |          will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n",
            " |          [`~Trainer.save_model`] will also trigger a push.\n",
            " |\n",
            " |          <Tip warning={true}>\n",
            " |\n",
            " |          If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n",
            " |          pushed.\n",
            " |\n",
            " |          </Tip>\n",
            " |\n",
            " |      resume_from_checkpoint (`str`, *optional*):\n",
            " |          The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
            " |          [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n",
            " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
            " |      hub_model_id (`str`, *optional*):\n",
            " |          The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
            " |          which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
            " |          for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
            " |          `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n",
            " |          name of `output_dir`.\n",
            " |\n",
            " |          Will default to the name of `output_dir`.\n",
            " |      hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
            " |          Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
            " |\n",
            " |          - `\"end\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
            " |            draft of a model card when the [`~Trainer.save_model`] method is called.\n",
            " |          - `\"every_save\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and\n",
            " |            a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
            " |            training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
            " |            finished. A last push is made with the final model at the end of training.\n",
            " |          - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
            " |            last-checkpoint, allowing you to resume training easily with\n",
            " |            `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
            " |          - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n",
            " |            folder (so you will get one checkpoint folder per folder in your final repository)\n",
            " |\n",
            " |      hub_token (`str`, *optional*):\n",
            " |          The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
            " |          `hf auth login`.\n",
            " |      hub_private_repo (`bool`, *optional*):\n",
            " |          Whether to make the repo private. If `None` (default), the repo will be public unless the organization's\n",
            " |          default is private. This value is ignored if the repo already exists. If reporting to Trackio with\n",
            " |          deployment to Hugging Face Spaces enabled, the same logic determines whether the Space is private.\n",
            " |      hub_always_push (`bool`, *optional*, defaults to `False`):\n",
            " |          Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n",
            " |      hub_revision (`str`, *optional*):\n",
            " |          The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n",
            " |      gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
            " |          If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
            " |      gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n",
            " |          Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n",
            " |      include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n",
            " |          This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n",
            " |      include_for_metrics (`list[str]`, *optional*, defaults to `[]`):\n",
            " |          Include additional data in the `compute_metrics` function if needed for metrics computation.\n",
            " |          Possible options to add to `include_for_metrics` list:\n",
            " |          - `\"inputs\"`: Input data passed to the model, intended for calculating input dependent metrics.\n",
            " |          - `\"loss\"`: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n",
            " |      eval_do_concat_batches (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,\n",
            " |          will instead store them as lists, with each batch kept separate.\n",
            " |      auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
            " |          Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n",
            " |          CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
            " |      full_determinism (`bool`, *optional*, defaults to `False`)\n",
            " |          If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n",
            " |          distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n",
            " |      torchdynamo (`str`, *optional*):\n",
            " |          If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n",
            " |          `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n",
            " |      ray_scope (`str`, *optional*, defaults to `\"last\"`):\n",
            " |          The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n",
            " |          then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n",
            " |          are also available. See the [Ray documentation](\n",
            " |          https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n",
            " |          more options.\n",
            " |      ddp_timeout (`int`, *optional*, defaults to 1800):\n",
            " |          The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n",
            " |          performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n",
            " |          (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n",
            " |          information.\n",
            " |      use_mps_device (`bool`, *optional*, defaults to `False`):\n",
            " |          This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n",
            " |      torch_compile (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to compile the model using PyTorch 2.0\n",
            " |          [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n",
            " |\n",
            " |          This will use the best defaults for the [`torch.compile`\n",
            " |          API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).\n",
            " |          You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we\n",
            " |          don't guarantee any of them will work as the support is progressively rolled in in PyTorch.\n",
            " |\n",
            " |          This flag and the whole compile API is experimental and subject to change in future releases.\n",
            " |      torch_compile_backend (`str`, *optional*):\n",
            " |          The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
            " |\n",
            " |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
            " |\n",
            " |          This flag is experimental and subject to change in future releases.\n",
            " |      torch_compile_mode (`str`, *optional*):\n",
            " |          The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
            " |\n",
            " |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
            " |\n",
            " |          This flag is experimental and subject to change in future releases.\n",
            " |      include_tokens_per_second (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether or not to compute the number of tokens per second per device for training speed metrics.\n",
            " |\n",
            " |          This will iterate over the entire training dataloader once beforehand,\n",
            " |          and will slow down the entire process.\n",
            " |\n",
            " |      include_num_input_tokens_seen (`bool`, *optional*):\n",
            " |          Whether or not to track the number of input tokens seen throughout training.\n",
            " |\n",
            " |          May be slower in distributed training as gather operations must be called.\n",
            " |\n",
            " |      neftune_noise_alpha (`Optional[float]`):\n",
            " |          If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance\n",
            " |          for instruction fine-tuning. Check out the [original paper](https://huggingface.co/papers/2310.05914) and the\n",
            " |          [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n",
            " |          `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].\n",
            " |      optim_target_modules (`Union[str, list[str]]`, *optional*):\n",
            " |          The target modules to optimize, i.e. the module names that you would like to train.\n",
            " |          Currently used for the GaLore algorithm (https://huggingface.co/papers/2403.03507) and APOLLO algorithm (https://huggingface.co/papers/2412.05270).\n",
            " |          See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.\n",
            " |          You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \"apollo_adamw\", \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules only.\n",
            " |\n",
            " |      batch_eval_metrics (`bool`, *optional*, defaults to `False`):\n",
            " |          If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics\n",
            " |          rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function\n",
            " |          that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global\n",
            " |          summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.\n",
            " |\n",
            " |      eval_on_start (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n",
            " |\n",
            " |      eval_use_gather_object (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n",
            " |\n",
            " |      use_liger_kernel (`bool`, *optional*, defaults to `False`):\n",
            " |          Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.\n",
            " |          It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\n",
            " |          flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n",
            " |\n",
            " |      liger_kernel_config (`Optional[dict]`, *optional*):\n",
            " |          Configuration to be used for Liger Kernel. When use_liger_kernel=True, this dict is passed as keyword arguments to the\n",
            " |          `_apply_liger_kernel_to_instance` function, which specifies which kernels to apply. Available options vary by model but typically\n",
            " |          include: 'rope', 'swiglu', 'cross_entropy', 'fused_linear_cross_entropy', 'rms_norm', etc. If `None`, use the default kernel configurations.\n",
            " |\n",
            " |      average_tokens_across_devices (`bool`, *optional*, defaults to `True`):\n",
            " |          Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n",
            " |          num_tokens_in_batch for precise loss calculation. Reference:\n",
            " |          https://github.com/huggingface/transformers/issues/34242\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __eq__(self, other)\n",
            " |      Return self==value.\n",
            " |\n",
            " |  __init__(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[accelerate.parallelism_config.ParallelismConfig] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  __post_init__(self)\n",
            " |\n",
            " |  __repr__ = __str__(self)\n",
            " |\n",
            " |  __str__(self)\n",
            " |      Return str(self).\n",
            " |\n",
            " |  get_process_log_level(self)\n",
            " |      Returns the log level to be used depending on whether this process is the main process of node 0, main process\n",
            " |      of node non-0, or a non-main process.\n",
            " |\n",
            " |      For the main process the log level defaults to the logging level set (`logging.WARNING` if you didn't do\n",
            " |      anything) unless overridden by `log_level` argument.\n",
            " |\n",
            " |      For the replica processes the log level defaults to `logging.WARNING` unless overridden by `log_level_replica`\n",
            " |      argument.\n",
            " |\n",
            " |      The choice between the main and replica process settings is made according to the return value of `should_log`.\n",
            " |\n",
            " |  get_warmup_steps(self, num_training_steps: int)\n",
            " |      Get number of steps used for a linear warmup.\n",
            " |\n",
            " |  main_process_first(self, local=True, desc='work')\n",
            " |      A context manager for torch distributed environment where on needs to do something on the main process, while\n",
            " |      blocking replicas, and when it's finished releasing the replicas.\n",
            " |\n",
            " |      One such use is for `datasets`'s `map` feature which to be efficient should be run once on the main process,\n",
            " |      which upon completion saves a cached version of results and which then automatically gets loaded by the\n",
            " |      replicas.\n",
            " |\n",
            " |      Args:\n",
            " |          local (`bool`, *optional*, defaults to `True`):\n",
            " |              if `True` first means process of rank 0 of each node if `False` first means process of rank 0 of node\n",
            " |              rank 0 In multi-node environment with a shared filesystem you most likely will want to use\n",
            " |              `local=False` so that only the main process of the first node will do the processing. If however, the\n",
            " |              filesystem is not shared, then the main process of each node will need to do the processing, which is\n",
            " |              the default behavior.\n",
            " |          desc (`str`, *optional*, defaults to `\"work\"`):\n",
            " |              a work description to be used in debug logs\n",
            " |\n",
            " |  set_dataloader(self, train_batch_size: int = 8, eval_batch_size: int = 8, drop_last: bool = False, num_workers: int = 0, pin_memory: bool = True, persistent_workers: bool = False, prefetch_factor: Optional[int] = None, auto_find_batch_size: bool = False, ignore_data_skip: bool = False, sampler_seed: Optional[int] = None)\n",
            " |      A method that regroups all arguments linked to the dataloaders creation.\n",
            " |\n",
            " |      Args:\n",
            " |          drop_last (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch\n",
            " |              size) or not.\n",
            " |          num_workers (`int`, *optional*, defaults to 0):\n",
            " |              Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in\n",
            " |              the main process.\n",
            " |          pin_memory (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
            " |          persistent_workers (`bool`, *optional*, defaults to `False`):\n",
            " |              If True, the data loader will not shut down the worker processes after a dataset has been consumed\n",
            " |              once. This allows to maintain the workers Dataset instances alive. Can potentially speed up training,\n",
            " |              but will increase RAM usage. Will default to `False`.\n",
            " |          prefetch_factor (`int`, *optional*):\n",
            " |              Number of batches loaded in advance by each worker.\n",
            " |              2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
            " |          auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
            " |              Whether to find a batch size that will fit into memory automatically through exponential decay,\n",
            " |              avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
            " |          ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
            " |              When resuming training, whether or not to skip the epochs and batches to get the data loading at the\n",
            " |              same stage as in the previous training. If set to `True`, the training will begin faster (as that\n",
            " |              skipping step can take a long time) but will not yield the same results as the interrupted training\n",
            " |              would have.\n",
            " |          sampler_seed (`int`, *optional*):\n",
            " |              Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
            " |              same seed as `self.seed`. This can be used to ensure reproducibility of data sampling, independent of\n",
            " |              the model seed.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)\n",
            " |      >>> args.per_device_train_batch_size\n",
            " |      16\n",
            " |      ```\n",
            " |\n",
            " |  set_evaluate(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'no', steps: int = 500, batch_size: int = 8, accumulation_steps: Optional[int] = None, delay: Optional[float] = None, loss_only: bool = False, jit_mode: bool = False)\n",
            " |      A method that regroups all arguments linked to evaluation.\n",
            " |\n",
            " |      Args:\n",
            " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
            " |              The evaluation strategy to adopt during training. Possible values are:\n",
            " |\n",
            " |                  - `\"no\"`: No evaluation is done during training.\n",
            " |                  - `\"steps\"`: Evaluation is done (and logged) every `steps`.\n",
            " |                  - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
            " |\n",
            " |              Setting a `strategy` different from `\"no\"` will set `self.do_eval` to `True`.\n",
            " |          steps (`int`, *optional*, defaults to 500):\n",
            " |              Number of update steps between two evaluations if `strategy=\"steps\"`.\n",
            " |          batch_size (`int` *optional*, defaults to 8):\n",
            " |              The batch size per device (GPU/TPU core/CPU...) used for evaluation.\n",
            " |          accumulation_steps (`int`, *optional*):\n",
            " |              Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU.\n",
            " |              If left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster\n",
            " |              but requires more memory).\n",
            " |          delay (`float`, *optional*):\n",
            " |              Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
            " |              eval_strategy.\n",
            " |          loss_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Ignores all outputs except the loss.\n",
            " |          jit_mode (`bool`, *optional*):\n",
            " |              Whether or not to use PyTorch jit trace for inference.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_evaluate(strategy=\"steps\", steps=100)\n",
            " |      >>> args.eval_steps\n",
            " |      100\n",
            " |      ```\n",
            " |\n",
            " |  set_logging(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps', steps: int = 500, report_to: Union[str, list[str]] = 'none', level: str = 'passive', first_step: bool = False, nan_inf_filter: bool = False, on_each_node: bool = False, replica_level: str = 'passive')\n",
            " |      A method that regroups all arguments linked to logging.\n",
            " |\n",
            " |      Args:\n",
            " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |              The logging strategy to adopt during training. Possible values are:\n",
            " |\n",
            " |                  - `\"no\"`: No logging is done during training.\n",
            " |                  - `\"epoch\"`: Logging is done at the end of each epoch.\n",
            " |                  - `\"steps\"`: Logging is done every `logging_steps`.\n",
            " |\n",
            " |          steps (`int`, *optional*, defaults to 500):\n",
            " |              Number of update steps between two logs if `strategy=\"steps\"`.\n",
            " |          level (`str`, *optional*, defaults to `\"passive\"`):\n",
            " |              Logger log level to use on the main process. Possible choices are the log levels as strings: `\"debug\"`,\n",
            " |              `\"info\"`, `\"warning\"`, `\"error\"` and `\"critical\"`, plus a `\"passive\"` level which doesn't set anything\n",
            " |              and lets the application set the level.\n",
            " |          report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n",
            " |              The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
            " |              `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`,\n",
            " |              `\"neptune\"`, `\"swanlab\"`, `\"tensorboard\"`, `\"trackio\"` and `\"wandb\"`. Use `\"all\"` to report to all\n",
            " |              integrations installed, `\"none\"` for no integrations.\n",
            " |          first_step (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to log and evaluate the first `global_step` or not.\n",
            " |          nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is\n",
            " |              `nan` or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
            " |\n",
            " |              <Tip>\n",
            " |\n",
            " |              `nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
            " |              gradient is computed or applied to the model.\n",
            " |\n",
            " |              </Tip>\n",
            " |\n",
            " |          on_each_node (`bool`, *optional*, defaults to `True`):\n",
            " |              In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
            " |              node.\n",
            " |          replica_level (`str`, *optional*, defaults to `\"passive\"`):\n",
            " |              Logger log level to use on replicas. Same choices as `log_level`\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_logging(strategy=\"steps\", steps=100)\n",
            " |      >>> args.logging_steps\n",
            " |      100\n",
            " |      ```\n",
            " |\n",
            " |  set_lr_scheduler(self, name: Union[str, transformers.trainer_utils.SchedulerType] = 'linear', num_epochs: float = 3.0, max_steps: int = -1, warmup_ratio: float = 0, warmup_steps: int = 0)\n",
            " |      A method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\n",
            " |\n",
            " |      Args:\n",
            " |          name (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
            " |              The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
            " |          num_epochs(`float`, *optional*, defaults to 3.0):\n",
            " |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n",
            " |              of the last epoch before stopping training).\n",
            " |          max_steps (`int`, *optional*, defaults to -1):\n",
            " |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
            " |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
            " |              `max_steps` is reached.\n",
            " |          warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
            " |              Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
            " |          warmup_steps (`int`, *optional*, defaults to 0):\n",
            " |              Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of\n",
            " |              `warmup_ratio`.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n",
            " |      >>> args.warmup_ratio\n",
            " |      0.05\n",
            " |      ```\n",
            " |\n",
            " |  set_optimizer(self, name: Union[str, transformers.training_args.OptimizerNames] = 'adamw_torch', learning_rate: float = 5e-05, weight_decay: float = 0, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-08, args: Optional[str] = None)\n",
            " |      A method that regroups all arguments linked to the optimizer and its hyperparameters.\n",
            " |\n",
            " |      Args:\n",
            " |          name (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n",
            " |              The optimizer to use: `\"adamw_torch\"`, `\"adamw_torch_fused\"`, `\"adamw_apex_fused\"`,\n",
            " |              `\"adamw_anyprecision\"` or `\"adafactor\"`.\n",
            " |          learning_rate (`float`, *optional*, defaults to 5e-5):\n",
            " |              The initial learning rate.\n",
            " |          weight_decay (`float`, *optional*, defaults to 0):\n",
            " |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights.\n",
            " |          beta1 (`float`, *optional*, defaults to 0.9):\n",
            " |              The beta1 hyperparameter for the adam optimizer or its variants.\n",
            " |          beta2 (`float`, *optional*, defaults to 0.999):\n",
            " |              The beta2 hyperparameter for the adam optimizer or its variants.\n",
            " |          epsilon (`float`, *optional*, defaults to 1e-8):\n",
            " |              The epsilon hyperparameter for the adam optimizer or its variants.\n",
            " |          args (`str`, *optional*):\n",
            " |              Optional arguments that are supplied to AnyPrecisionAdamW (only useful when\n",
            " |              `optim=\"adamw_anyprecision\"`).\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n",
            " |      >>> args.optim\n",
            " |      'adamw_torch'\n",
            " |      ```\n",
            " |\n",
            " |  set_push_to_hub(self, model_id: str, strategy: Union[str, transformers.trainer_utils.HubStrategy] = 'every_save', token: Optional[str] = None, private_repo: Optional[bool] = None, always_push: bool = False, revision: Optional[str] = None)\n",
            " |      A method that regroups all arguments linked to synchronizing checkpoints with the Hub.\n",
            " |\n",
            " |      <Tip>\n",
            " |\n",
            " |      Calling this method will set `self.push_to_hub` to `True`, which means the `output_dir` will begin a git\n",
            " |      directory synced with the repo (determined by `model_id`) and the content will be pushed each time a save is\n",
            " |      triggered (depending on your `self.save_strategy`). Calling [`~Trainer.save_model`] will also trigger a push.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Args:\n",
            " |          model_id (`str`):\n",
            " |              The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
            " |              which case the model will be pushed in your namespace. Otherwise it should be the whole repository\n",
            " |              name, for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of\n",
            " |              with `\"organization_name/model\"`.\n",
            " |          strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
            " |              Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
            " |\n",
            " |              - `\"end\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
            " |              draft of a model card when the [`~Trainer.save_model`] method is called.\n",
            " |              - `\"every_save\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`])\n",
            " |                and\n",
            " |              a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
            " |              training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
            " |              finished. A last push is made with the final model at the end of training.\n",
            " |              - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
            " |              last-checkpoint, allowing you to resume training easily with\n",
            " |              `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
            " |              - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the\n",
            " |                output\n",
            " |              folder (so you will get one checkpoint folder per folder in your final repository)\n",
            " |\n",
            " |          token (`str`, *optional*):\n",
            " |              The token to use to push the model to the Hub. Will default to the token in the cache folder obtained\n",
            " |              with `hf auth login`.\n",
            " |          private_repo (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
            " |          always_push (`bool`, *optional*, defaults to `False`):\n",
            " |              Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not\n",
            " |              finished.\n",
            " |          revision (`str`, *optional*):\n",
            " |              The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_push_to_hub(\"me/awesome-model\")\n",
            " |      >>> args.hub_model_id\n",
            " |      'me/awesome-model'\n",
            " |      ```\n",
            " |\n",
            " |  set_save(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps', steps: int = 500, total_limit: Optional[int] = None, on_each_node: bool = False)\n",
            " |      A method that regroups all arguments linked to checkpoint saving.\n",
            " |\n",
            " |      Args:\n",
            " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
            " |              The checkpoint save strategy to adopt during training. Possible values are:\n",
            " |\n",
            " |                  - `\"no\"`: No save is done during training.\n",
            " |                  - `\"epoch\"`: Save is done at the end of each epoch.\n",
            " |                  - `\"steps\"`: Save is done every `save_steps`.\n",
            " |\n",
            " |          steps (`int`, *optional*, defaults to 500):\n",
            " |              Number of updates steps before two checkpoint saves if `strategy=\"steps\"`.\n",
            " |          total_limit (`int`, *optional*):\n",
            " |              If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
            " |              `output_dir`.\n",
            " |          on_each_node (`bool`, *optional*, defaults to `False`):\n",
            " |              When doing multi-node distributed training, whether to save models and checkpoints on each node, or\n",
            " |              only on the main one.\n",
            " |\n",
            " |              This should not be activated when the different nodes use the same storage as the files will be saved\n",
            " |              with the same names for each node.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_save(strategy=\"steps\", steps=100)\n",
            " |      >>> args.save_steps\n",
            " |      100\n",
            " |      ```\n",
            " |\n",
            " |  set_testing(self, batch_size: int = 8, loss_only: bool = False, jit_mode: bool = False)\n",
            " |      A method that regroups all basic arguments linked to testing on a held-out dataset.\n",
            " |\n",
            " |      <Tip>\n",
            " |\n",
            " |      Calling this method will automatically set `self.do_predict` to `True`.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Args:\n",
            " |          batch_size (`int` *optional*, defaults to 8):\n",
            " |              The batch size per device (GPU/TPU core/CPU...) used for testing.\n",
            " |          loss_only (`bool`, *optional*, defaults to `False`):\n",
            " |              Ignores all outputs except the loss.\n",
            " |          jit_mode (`bool`, *optional*):\n",
            " |              Whether or not to use PyTorch jit trace for inference.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_testing(batch_size=32)\n",
            " |      >>> args.per_device_eval_batch_size\n",
            " |      32\n",
            " |      ```\n",
            " |\n",
            " |  set_training(self, learning_rate: float = 5e-05, batch_size: int = 8, weight_decay: float = 0, num_epochs: float = 3, max_steps: int = -1, gradient_accumulation_steps: int = 1, seed: int = 42, gradient_checkpointing: bool = False)\n",
            " |      A method that regroups all basic arguments linked to the training.\n",
            " |\n",
            " |      <Tip>\n",
            " |\n",
            " |      Calling this method will automatically set `self.do_train` to `True`.\n",
            " |\n",
            " |      </Tip>\n",
            " |\n",
            " |      Args:\n",
            " |          learning_rate (`float`, *optional*, defaults to 5e-5):\n",
            " |              The initial learning rate for the optimizer.\n",
            " |          batch_size (`int` *optional*, defaults to 8):\n",
            " |              The batch size per device (GPU/TPU core/CPU...) used for training.\n",
            " |          weight_decay (`float`, *optional*, defaults to 0):\n",
            " |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in the\n",
            " |              optimizer.\n",
            " |          num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
            " |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n",
            " |              of the last epoch before stopping training).\n",
            " |          max_steps (`int`, *optional*, defaults to -1):\n",
            " |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
            " |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
            " |              `max_steps` is reached.\n",
            " |          gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
            " |              Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
            " |\n",
            " |              <Tip warning={true}>\n",
            " |\n",
            " |              When using gradient accumulation, one step is counted as one step with backward pass. Therefore,\n",
            " |              logging, evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training\n",
            " |              examples.\n",
            " |\n",
            " |              </Tip>\n",
            " |\n",
            " |          seed (`int`, *optional*, defaults to 42):\n",
            " |              Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use\n",
            " |              the [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized\n",
            " |              parameters.\n",
            " |          gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
            " |              If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      ```py\n",
            " |      >>> from transformers import TrainingArguments\n",
            " |\n",
            " |      >>> args = TrainingArguments(\"working_dir\")\n",
            " |      >>> args = args.set_training(learning_rate=1e-4, batch_size=32)\n",
            " |      >>> args.learning_rate\n",
            " |      1e-4\n",
            " |      ```\n",
            " |\n",
            " |  to_dict(self)\n",
            " |      Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
            " |      the token values by removing their value.\n",
            " |\n",
            " |  to_json_string(self)\n",
            " |      Serializes this instance to a JSON string.\n",
            " |\n",
            " |  to_sanitized_dict(self) -> dict[str, typing.Any]\n",
            " |      Sanitized serialization to use with TensorBoard's hparams\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |\n",
            " |  ddp_timeout_delta\n",
            " |      The actual timeout for torch.distributed.init_process_group since it expects a timedelta variable.\n",
            " |\n",
            " |  device\n",
            " |      The device used by this process.\n",
            " |\n",
            " |  eval_batch_size\n",
            " |      The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).\n",
            " |\n",
            " |  local_process_index\n",
            " |      The index of the local process used.\n",
            " |\n",
            " |  n_gpu\n",
            " |      The number of GPUs used by this process.\n",
            " |\n",
            " |      Note:\n",
            " |          This will only be greater than one when you have multiple GPUs available but are not using distributed\n",
            " |          training. For distributed training, it will always be 1.\n",
            " |\n",
            " |  parallel_mode\n",
            " |      The current mode used for parallelism if multiple GPUs/TPU cores are available. One of:\n",
            " |\n",
            " |      - `ParallelMode.NOT_PARALLEL`: no parallelism (CPU or one GPU).\n",
            " |      - `ParallelMode.NOT_DISTRIBUTED`: several GPUs in one single process (uses `torch.nn.DataParallel`).\n",
            " |      - `ParallelMode.DISTRIBUTED`: several GPUs, each having its own process (uses\n",
            " |        `torch.nn.DistributedDataParallel`).\n",
            " |      - `ParallelMode.TPU`: several TPU cores.\n",
            " |\n",
            " |  place_model_on_device\n",
            " |      Can be subclassed and overridden for some specific integrations.\n",
            " |\n",
            " |  process_index\n",
            " |      The index of the current process used.\n",
            " |\n",
            " |  should_log\n",
            " |      Whether or not the current process should produce log.\n",
            " |\n",
            " |  should_save\n",
            " |      Whether or not the current process should write to disk, e.g., to save models and checkpoints.\n",
            " |\n",
            " |  train_batch_size\n",
            " |      The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).\n",
            " |\n",
            " |  world_size\n",
            " |      The number of processes used in parallel.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __annotations__ = {'_n_gpu': <class 'int'>, 'accelerator_config': typi...\n",
            " |\n",
            " |  __dataclass_fields__ = {'_n_gpu': Field(name='_n_gpu',type=<class 'int...\n",
            " |\n",
            " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
            " |\n",
            " |  __hash__ = None\n",
            " |\n",
            " |  __match_args__ = ('output_dir', 'overwrite_output_dir', 'do_train', 'd...\n",
            " |\n",
            " |  accelerator_config = None\n",
            " |\n",
            " |  adafactor = False\n",
            " |\n",
            " |  adam_beta1 = 0.9\n",
            " |\n",
            " |  adam_beta2 = 0.999\n",
            " |\n",
            " |  adam_epsilon = 1e-08\n",
            " |\n",
            " |  auto_find_batch_size = False\n",
            " |\n",
            " |  average_tokens_across_devices = True\n",
            " |\n",
            " |  batch_eval_metrics = False\n",
            " |\n",
            " |  bf16 = False\n",
            " |\n",
            " |  bf16_full_eval = False\n",
            " |\n",
            " |  data_seed = None\n",
            " |\n",
            " |  dataloader_drop_last = False\n",
            " |\n",
            " |  dataloader_num_workers = 0\n",
            " |\n",
            " |  dataloader_persistent_workers = False\n",
            " |\n",
            " |  dataloader_pin_memory = True\n",
            " |\n",
            " |  dataloader_prefetch_factor = None\n",
            " |\n",
            " |  ddp_backend = None\n",
            " |\n",
            " |  ddp_broadcast_buffers = None\n",
            " |\n",
            " |  ddp_bucket_cap_mb = None\n",
            " |\n",
            " |  ddp_find_unused_parameters = None\n",
            " |\n",
            " |  ddp_timeout = 1800\n",
            " |\n",
            " |  debug = ''\n",
            " |\n",
            " |  deepspeed = None\n",
            " |\n",
            " |  default_optim = 'adamw_torch_fused'\n",
            " |\n",
            " |  disable_tqdm = None\n",
            " |\n",
            " |  do_eval = False\n",
            " |\n",
            " |  do_predict = False\n",
            " |\n",
            " |  do_train = False\n",
            " |\n",
            " |  eval_accumulation_steps = None\n",
            " |\n",
            " |  eval_delay = 0\n",
            " |\n",
            " |  eval_do_concat_batches = True\n",
            " |\n",
            " |  eval_on_start = False\n",
            " |\n",
            " |  eval_steps = None\n",
            " |\n",
            " |  eval_strategy = 'no'\n",
            " |\n",
            " |  eval_use_gather_object = False\n",
            " |\n",
            " |  fp16 = False\n",
            " |\n",
            " |  fp16_backend = 'auto'\n",
            " |\n",
            " |  fp16_full_eval = False\n",
            " |\n",
            " |  fp16_opt_level = 'O1'\n",
            " |\n",
            " |  framework = 'pt'\n",
            " |\n",
            " |  fsdp = None\n",
            " |\n",
            " |  fsdp_config = None\n",
            " |\n",
            " |  fsdp_min_num_params = 0\n",
            " |\n",
            " |  fsdp_transformer_layer_cls_to_wrap = None\n",
            " |\n",
            " |  full_determinism = False\n",
            " |\n",
            " |  gradient_accumulation_steps = 1\n",
            " |\n",
            " |  gradient_checkpointing = False\n",
            " |\n",
            " |  gradient_checkpointing_kwargs = None\n",
            " |\n",
            " |  greater_is_better = None\n",
            " |\n",
            " |  group_by_length = False\n",
            " |\n",
            " |  half_precision_backend = 'auto'\n",
            " |\n",
            " |  hub_always_push = False\n",
            " |\n",
            " |  hub_model_id = None\n",
            " |\n",
            " |  hub_private_repo = None\n",
            " |\n",
            " |  hub_revision = None\n",
            " |\n",
            " |  hub_strategy = 'every_save'\n",
            " |\n",
            " |  hub_token = None\n",
            " |\n",
            " |  ignore_data_skip = False\n",
            " |\n",
            " |  include_inputs_for_metrics = False\n",
            " |\n",
            " |  include_num_input_tokens_seen = False\n",
            " |\n",
            " |  include_tokens_per_second = False\n",
            " |\n",
            " |  is_torch_greater_or_equal_than_2_8 = True\n",
            " |\n",
            " |  jit_mode_eval = False\n",
            " |\n",
            " |  label_names = None\n",
            " |\n",
            " |  label_smoothing_factor = 0.0\n",
            " |\n",
            " |  learning_rate = 5e-05\n",
            " |\n",
            " |  length_column_name = 'length'\n",
            " |\n",
            " |  liger_kernel_config = None\n",
            " |\n",
            " |  load_best_model_at_end = False\n",
            " |\n",
            " |  local_rank = -1\n",
            " |\n",
            " |  log_level = 'passive'\n",
            " |\n",
            " |  log_level_replica = 'warning'\n",
            " |\n",
            " |  log_on_each_node = True\n",
            " |\n",
            " |  logging_dir = None\n",
            " |\n",
            " |  logging_first_step = False\n",
            " |\n",
            " |  logging_nan_inf_filter = True\n",
            " |\n",
            " |  logging_steps = 500\n",
            " |\n",
            " |  logging_strategy = 'steps'\n",
            " |\n",
            " |  lr_scheduler_type = 'linear'\n",
            " |\n",
            " |  max_grad_norm = 1.0\n",
            " |\n",
            " |  max_steps = -1\n",
            " |\n",
            " |  metric_for_best_model = None\n",
            " |\n",
            " |  mp_parameters = ''\n",
            " |\n",
            " |  neftune_noise_alpha = None\n",
            " |\n",
            " |  no_cuda = False\n",
            " |\n",
            " |  num_train_epochs = 3.0\n",
            " |\n",
            " |  optim = 'adamw_torch_fused'\n",
            " |\n",
            " |  optim_args = None\n",
            " |\n",
            " |  optim_target_modules = None\n",
            " |\n",
            " |  output_dir = None\n",
            " |\n",
            " |  overwrite_output_dir = False\n",
            " |\n",
            " |  parallelism_config = None\n",
            " |\n",
            " |  past_index = -1\n",
            " |\n",
            " |  per_device_eval_batch_size = 8\n",
            " |\n",
            " |  per_device_train_batch_size = 8\n",
            " |\n",
            " |  per_gpu_eval_batch_size = None\n",
            " |\n",
            " |  per_gpu_train_batch_size = None\n",
            " |\n",
            " |  prediction_loss_only = False\n",
            " |\n",
            " |  project = 'huggingface'\n",
            " |\n",
            " |  push_to_hub = False\n",
            " |\n",
            " |  push_to_hub_model_id = None\n",
            " |\n",
            " |  push_to_hub_organization = None\n",
            " |\n",
            " |  push_to_hub_token = None\n",
            " |\n",
            " |  ray_scope = 'last'\n",
            " |\n",
            " |  remove_unused_columns = True\n",
            " |\n",
            " |  report_to = None\n",
            " |\n",
            " |  restore_callback_states_from_checkpoint = False\n",
            " |\n",
            " |  resume_from_checkpoint = None\n",
            " |\n",
            " |  run_name = None\n",
            " |\n",
            " |  save_on_each_node = False\n",
            " |\n",
            " |  save_only_model = False\n",
            " |\n",
            " |  save_safetensors = True\n",
            " |\n",
            " |  save_steps = 500\n",
            " |\n",
            " |  save_strategy = 'steps'\n",
            " |\n",
            " |  save_total_limit = None\n",
            " |\n",
            " |  seed = 42\n",
            " |\n",
            " |  skip_memory_metrics = True\n",
            " |\n",
            " |  tf32 = None\n",
            " |\n",
            " |  torch_compile = False\n",
            " |\n",
            " |  torch_compile_backend = None\n",
            " |\n",
            " |  torch_compile_mode = None\n",
            " |\n",
            " |  torch_empty_cache_steps = None\n",
            " |\n",
            " |  torchdynamo = None\n",
            " |\n",
            " |  tpu_metrics_debug = False\n",
            " |\n",
            " |  tpu_num_cores = None\n",
            " |\n",
            " |  trackio_space_id = 'trackio'\n",
            " |\n",
            " |  use_cpu = False\n",
            " |\n",
            " |  use_legacy_prediction_loop = False\n",
            " |\n",
            " |  use_liger_kernel = False\n",
            " |\n",
            " |  use_mps_device = False\n",
            " |\n",
            " |  warmup_ratio = 0.0\n",
            " |\n",
            " |  warmup_steps = 0\n",
            " |\n",
            " |  weight_decay = 0.0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "help(TrainingArguments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LbWoXTjVLUEe"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XXLlWcadLbRx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "523c03e5-2cc9-40aa-efd8-43cd4994f61c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 2469 has 14.72 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2739\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2740\u001b[0;31m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2742\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_patched_step_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m                 )\n\u001b[1;32m    447\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound_inf\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_grad_scaler_kwarg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mpatched_step\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpatched_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0maccelerated_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerate_step_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpatched_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     )\n\u001b[1;32m    176\u001b[0m                     \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m                     state[\"exp_avg\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    178\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 2469 has 14.72 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 156.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9BSdQbpL1Hc"
      },
      "source": [
        "### Here we are not specfiying anything means this is full fine-tuning\n",
        "### Now we have two methods for Partial Finetuning\n",
        "- Feerze some layer and finetune unfreeze layer(old CNN and Bert sytel method)\n",
        "- LORA(Append some external weight to the already trained pretrain weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hep4hQSIMR4L"
      },
      "source": [
        "### Now lets see the LORA based method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "O06k5LBxLc-2"
      },
      "outputs": [],
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "e5Y1CHfhLdBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93303b4-626a-489b-fd6c-064728ed2a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U peft bitsandbytes transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "RBzEn4y0LdEf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "xpgy6JV8LdHW"
      },
      "outputs": [],
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wZmEaMDoLdJ9"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "h3f9e29ZLdMz"
      },
      "outputs": [],
      "source": [
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hZ03oJoaLdPW"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BXZV1ehzLdSB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "264b4a9daafe4a0d97f17e1215e5750d",
            "791c299e49304c758ef8f7aeebbac31e",
            "5a7b7375e9da4206b2129298d018d899",
            "93529e7ab64d4901bfc7f8f3105b2e7f",
            "c50ad3ffa3cf4addb2854d6a7696d569",
            "b214dadf597b430ea1bdcef7c56df277",
            "ede7cca88adc4014a9e11e067e19feaa",
            "729cefafb85a4f9d920caeed611fb52a",
            "4210977ce46443dc8e6b1f8878e4018f",
            "17e91984ac6a4d3db9501b6b08f7a360",
            "fad718dd873749adb02f93955c7510e8"
          ]
        },
        "outputId": "6e59fda0-80da-4193-baa7-e1f1a246828e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "264b4a9daafe4a0d97f17e1215e5750d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized = dataset.map(tokenize_fn, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ztgvtHP7LdVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6e5224-0fc7-4ae3-a353-d10b7d516a01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 80\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "LiYLP-CxLdYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa2f4d0-72ba-4eb7-b88e-ddcde9f05b63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QFKO3s2kLdap"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "c33yJ9IELddk"
      },
      "outputs": [],
      "source": [
        "# save adapter\n",
        "non_inst_model_lora = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "TcCqpIq9LdgQ"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"./tinyllama-lora\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=20,\n",
        "    save_total_limit=1,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "F1Ih1XszLdi8"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=non_inst_model_lora,\n",
        "    args=args,\n",
        "    train_dataset=tokenized\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uvBVbJL2NEih",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "b09938e3-492c-4e7d-ae07-181da2936703"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 03:24, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>6.176800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.540600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=3.822074546813965, metrics={'train_runtime': 208.7591, 'train_samples_per_second': 1.916, 'train_steps_per_second': 0.24, 'total_flos': 1272592937779200.0, 'train_loss': 3.822074546813965, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "hIRhtt3rNZsL"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/tinyllama-lora/checkpoint-50\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob70TaJvNbNz"
      },
      "outputs": [],
      "source": [
        "# tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "LH7byWE0NkrT"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ojjWeV5lNq-w"
      },
      "outputs": [],
      "source": [
        "prompt = \"When\tyou\twrite\ta\tprompt,\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "I4bfaIDrNurS"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "cIzd_Ux1Nwmk"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.5,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "BtbnhcxmLdlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabe10af-29b6-4687-c508-e99943062ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Output:\n",
            "\n",
            "When\tyou\twrite\ta\tprompt,\tit's\timportant\tto\tknow\tthat\tthe\tprompt\tisn't\tjust\tan\tinteractive\tcommand.\n",
            "The\tprompt\tis\talso\tused\tfor\tauthentication,\tand\tif\tyou\tdon't\thave\tcredentials\tor\tpasswords\tin\tyour\tsystem,\tyou\tcan\tuse\tthe\tprompt\tas\twell.\n",
            "So,\tthere's\ta\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nModel Output:\\n\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ca709c4b01da4f78bbb225630d940531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f012b02d9e7c4ac1897decab613c584d",
              "IPY_MODEL_052ced0cceac4314bd34c3c83676b7b2",
              "IPY_MODEL_29b2a2e089d94ac683fbeec33b3f75a7"
            ],
            "layout": "IPY_MODEL_b1bc7adb51ba4b469c89cfb8c62f788a"
          }
        },
        "f012b02d9e7c4ac1897decab613c584d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c2602c5587e464d99c1c512ea4539cb",
            "placeholder": "",
            "style": "IPY_MODEL_de419d2cf000408aa22fb4ee8da20550",
            "value": "tokenizer_config.json:100%"
          }
        },
        "052ced0cceac4314bd34c3c83676b7b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee295af00978446b9e004c93fdd86dc3",
            "max": 776,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7455a32fbdfb4efe91afd123fd977e56",
            "value": 776
          }
        },
        "29b2a2e089d94ac683fbeec33b3f75a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_102f7c76756d4a9d8c9246cdcf0c9f9c",
            "placeholder": "",
            "style": "IPY_MODEL_e490120f4a684fc0b2be33aa9c271d33",
            "value": "776/776[00:00&lt;00:00,82.6kB/s]"
          }
        },
        "b1bc7adb51ba4b469c89cfb8c62f788a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c2602c5587e464d99c1c512ea4539cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de419d2cf000408aa22fb4ee8da20550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee295af00978446b9e004c93fdd86dc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7455a32fbdfb4efe91afd123fd977e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "102f7c76756d4a9d8c9246cdcf0c9f9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e490120f4a684fc0b2be33aa9c271d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01e37ac189b0477e9313b1c93d7ae8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1713c81bc7804ebbba2d3bf388d82b7f",
              "IPY_MODEL_3aa64bbe67ff41528329268269316b06",
              "IPY_MODEL_b8a6660406674ffb9d6fb33a1228dc88"
            ],
            "layout": "IPY_MODEL_7b17c3e5f0e64cdabb2ef4ae7f134f18"
          }
        },
        "1713c81bc7804ebbba2d3bf388d82b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0294495c8734466cb0fc0db414ec5140",
            "placeholder": "",
            "style": "IPY_MODEL_0c81437b3341438f932e90ba20db1323",
            "value": "tokenizer.model:100%"
          }
        },
        "3aa64bbe67ff41528329268269316b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_145cedc69b5c43f984ec9d4ac8e156e9",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e10a4eedbeb4c68bc643341a78d41d1",
            "value": 499723
          }
        },
        "b8a6660406674ffb9d6fb33a1228dc88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8269ae409604947ae548b330251880c",
            "placeholder": "",
            "style": "IPY_MODEL_23a1e9a8bed141dbbaa98996f2204a6e",
            "value": "500k/500k[00:00&lt;00:00,1.05MB/s]"
          }
        },
        "7b17c3e5f0e64cdabb2ef4ae7f134f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0294495c8734466cb0fc0db414ec5140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c81437b3341438f932e90ba20db1323": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "145cedc69b5c43f984ec9d4ac8e156e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e10a4eedbeb4c68bc643341a78d41d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8269ae409604947ae548b330251880c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23a1e9a8bed141dbbaa98996f2204a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fa34720fe6847e38a61ab91c29fc92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c50b70b060a14a67a351c625ab46ab25",
              "IPY_MODEL_24318c5de1ff43b78b83244e244f29a7",
              "IPY_MODEL_de210994dbb2498db3cd30f5a505205d"
            ],
            "layout": "IPY_MODEL_0ee51ab2809449d6953db7f06902963e"
          }
        },
        "c50b70b060a14a67a351c625ab46ab25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90c253a82f7b4254bf2f64f5411e3747",
            "placeholder": "",
            "style": "IPY_MODEL_84b8a84596074848a0f718e831930817",
            "value": "tokenizer.json:"
          }
        },
        "24318c5de1ff43b78b83244e244f29a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ef6c7c1a46346fe849164872ca2a148",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04f12d3445a743b7a894e8add0c78ee0",
            "value": 1
          }
        },
        "de210994dbb2498db3cd30f5a505205d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2da6d5c2930d4df88ca1105c80ea46ce",
            "placeholder": "",
            "style": "IPY_MODEL_421a20f3ba4248ca896e22451dec3cdd",
            "value": "1.84M/?[00:00&lt;00:00,45.9MB/s]"
          }
        },
        "0ee51ab2809449d6953db7f06902963e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90c253a82f7b4254bf2f64f5411e3747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84b8a84596074848a0f718e831930817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ef6c7c1a46346fe849164872ca2a148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "04f12d3445a743b7a894e8add0c78ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2da6d5c2930d4df88ca1105c80ea46ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "421a20f3ba4248ca896e22451dec3cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78b2d57190cf47c1bcbc249bd14536d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54e2517d071044c08a6e6527aa276a8b",
              "IPY_MODEL_234c6faa4e4c4ca88d3b24dd997b215d",
              "IPY_MODEL_cae25f7a21c44fe997b38f2a569d2be4"
            ],
            "layout": "IPY_MODEL_34c251a2e4f345e09d9e837dfa8359d4"
          }
        },
        "54e2517d071044c08a6e6527aa276a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41fee3ce63b54c2c99b426abc2918be5",
            "placeholder": "",
            "style": "IPY_MODEL_1088ffa1ae3047599e95545a8c33a7be",
            "value": "special_tokens_map.json:100%"
          }
        },
        "234c6faa4e4c4ca88d3b24dd997b215d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd1c266ae8c645be9999fbbf380da595",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5072dcdf7305489f83f513def3c4185d",
            "value": 414
          }
        },
        "cae25f7a21c44fe997b38f2a569d2be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b48bc19bad4a4a96ad66f67faaaa8b09",
            "placeholder": "",
            "style": "IPY_MODEL_fc8d7d455e7943b5bf20b7d6a098ae0c",
            "value": "414/414[00:00&lt;00:00,47.5kB/s]"
          }
        },
        "34c251a2e4f345e09d9e837dfa8359d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41fee3ce63b54c2c99b426abc2918be5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1088ffa1ae3047599e95545a8c33a7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd1c266ae8c645be9999fbbf380da595": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5072dcdf7305489f83f513def3c4185d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b48bc19bad4a4a96ad66f67faaaa8b09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc8d7d455e7943b5bf20b7d6a098ae0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cced4637b4c4e3fa825bdabf9ba6a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_359ef99acaa74204b8fbe5e350d17946",
              "IPY_MODEL_70766e0e389b4f7eb68fa1fbd9baf0cc",
              "IPY_MODEL_5d134fc9e20d4ee8944b9d78cb9c086f"
            ],
            "layout": "IPY_MODEL_b91726b6458f4936a4f4a0c209d66995"
          }
        },
        "359ef99acaa74204b8fbe5e350d17946": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afff6ba3e5044ee2b10e4d79cfd84d51",
            "placeholder": "",
            "style": "IPY_MODEL_7ae585195f1441d69d7429cd6053e530",
            "value": "Map:100%"
          }
        },
        "70766e0e389b4f7eb68fa1fbd9baf0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ee1c9f974e549f599f6894e89a0b560",
            "max": 80,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_024fd7547e73429896dad841a558ac76",
            "value": 80
          }
        },
        "5d134fc9e20d4ee8944b9d78cb9c086f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_032bc8237cc145bc83fe35753afad02a",
            "placeholder": "",
            "style": "IPY_MODEL_e534bfd632d64baea011a7171965370d",
            "value": "80/80[00:00&lt;00:00,300.22examples/s]"
          }
        },
        "b91726b6458f4936a4f4a0c209d66995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afff6ba3e5044ee2b10e4d79cfd84d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ae585195f1441d69d7429cd6053e530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ee1c9f974e549f599f6894e89a0b560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "024fd7547e73429896dad841a558ac76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "032bc8237cc145bc83fe35753afad02a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e534bfd632d64baea011a7171965370d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e55fd334663643bc8c9d135d86e651c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b4a4f593287472f96c80f638f7352c6",
              "IPY_MODEL_77fabbee6abb4025adba14782ac11968",
              "IPY_MODEL_284dc483e0e344e3b752c2f564478007"
            ],
            "layout": "IPY_MODEL_a9d8deaef582424a87c95e8c8d8ff7f3"
          }
        },
        "4b4a4f593287472f96c80f638f7352c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80581fd7ca8f4cfda68d8495b08ba1b1",
            "placeholder": "",
            "style": "IPY_MODEL_ec5d3a333158404ab4e6b463e35d497c",
            "value": "config.json:100%"
          }
        },
        "77fabbee6abb4025adba14782ac11968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f43184b4293f4fb8b3e4455f1a62a347",
            "max": 560,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41d4e0e188f44c72b00fd7889d256c0e",
            "value": 560
          }
        },
        "284dc483e0e344e3b752c2f564478007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74bf7893701f42d9be1bb62b9ddc78c0",
            "placeholder": "",
            "style": "IPY_MODEL_edbc0728c7e246549c5fc6d8cb858bf1",
            "value": "560/560[00:00&lt;00:00,62.6kB/s]"
          }
        },
        "a9d8deaef582424a87c95e8c8d8ff7f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80581fd7ca8f4cfda68d8495b08ba1b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec5d3a333158404ab4e6b463e35d497c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f43184b4293f4fb8b3e4455f1a62a347": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d4e0e188f44c72b00fd7889d256c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74bf7893701f42d9be1bb62b9ddc78c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edbc0728c7e246549c5fc6d8cb858bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68666630d4f342f599fc867fc2c68983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4c9d12917db4e76991a00f3ae00702c",
              "IPY_MODEL_77ab4c65eb6144b6b7935215f4a05ce7",
              "IPY_MODEL_92333c5e4c58468ea095fff15476cfe4"
            ],
            "layout": "IPY_MODEL_7b7647443cc1482c8310192949e14d7e"
          }
        },
        "c4c9d12917db4e76991a00f3ae00702c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dab3eabc7644c56990db67bc774acfb",
            "placeholder": "",
            "style": "IPY_MODEL_1370174729f243d2b2bba9546ba11bf9",
            "value": "model.safetensors:100%"
          }
        },
        "77ab4c65eb6144b6b7935215f4a05ce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dc2aec43c0e4d63a344226a8fe3166d",
            "max": 4400216536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb1803a02b9349d38c1abbba662ecb2b",
            "value": 4400216536
          }
        },
        "92333c5e4c58468ea095fff15476cfe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1896c3de3dc4e48a28d45b4caa54e3b",
            "placeholder": "",
            "style": "IPY_MODEL_910010afdf9d493c9a6a31cc888b1c06",
            "value": "4.40G/4.40G[01:03&lt;00:00,87.0MB/s]"
          }
        },
        "7b7647443cc1482c8310192949e14d7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dab3eabc7644c56990db67bc774acfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1370174729f243d2b2bba9546ba11bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dc2aec43c0e4d63a344226a8fe3166d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1803a02b9349d38c1abbba662ecb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1896c3de3dc4e48a28d45b4caa54e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "910010afdf9d493c9a6a31cc888b1c06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c48fdc0aaf449b69ed30bd7cf6fdbb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5198216e58794592bc1ed9113180a88c",
              "IPY_MODEL_e2b83c2cc2ce4c3ea174f61a401faff3",
              "IPY_MODEL_c40448b90fb940c198bd696238ee64be"
            ],
            "layout": "IPY_MODEL_f47aa85a94e74733b9e81ebcfcb27902"
          }
        },
        "5198216e58794592bc1ed9113180a88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b12c46bab20477c971163f1a0778121",
            "placeholder": "",
            "style": "IPY_MODEL_dc30fb7325684433bcd6c0ea16e6aa69",
            "value": "generation_config.json:100%"
          }
        },
        "e2b83c2cc2ce4c3ea174f61a401faff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6bd48f4482a481097979303c1872feb",
            "max": 129,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4497261697b4db3ab42b2c34aab499c",
            "value": 129
          }
        },
        "c40448b90fb940c198bd696238ee64be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0730413c1bf48e98d1982f845519d30",
            "placeholder": "",
            "style": "IPY_MODEL_9520cbe03dcc428dae79c8b0f7c45ed9",
            "value": "129/129[00:00&lt;00:00,8.68kB/s]"
          }
        },
        "f47aa85a94e74733b9e81ebcfcb27902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b12c46bab20477c971163f1a0778121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc30fb7325684433bcd6c0ea16e6aa69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6bd48f4482a481097979303c1872feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4497261697b4db3ab42b2c34aab499c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0730413c1bf48e98d1982f845519d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9520cbe03dcc428dae79c8b0f7c45ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "264b4a9daafe4a0d97f17e1215e5750d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_791c299e49304c758ef8f7aeebbac31e",
              "IPY_MODEL_5a7b7375e9da4206b2129298d018d899",
              "IPY_MODEL_93529e7ab64d4901bfc7f8f3105b2e7f"
            ],
            "layout": "IPY_MODEL_c50ad3ffa3cf4addb2854d6a7696d569"
          }
        },
        "791c299e49304c758ef8f7aeebbac31e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b214dadf597b430ea1bdcef7c56df277",
            "placeholder": "",
            "style": "IPY_MODEL_ede7cca88adc4014a9e11e067e19feaa",
            "value": "Map:100%"
          }
        },
        "5a7b7375e9da4206b2129298d018d899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_729cefafb85a4f9d920caeed611fb52a",
            "max": 80,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4210977ce46443dc8e6b1f8878e4018f",
            "value": 80
          }
        },
        "93529e7ab64d4901bfc7f8f3105b2e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17e91984ac6a4d3db9501b6b08f7a360",
            "placeholder": "",
            "style": "IPY_MODEL_fad718dd873749adb02f93955c7510e8",
            "value": "80/80[00:00&lt;00:00,464.21examples/s]"
          }
        },
        "c50ad3ffa3cf4addb2854d6a7696d569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b214dadf597b430ea1bdcef7c56df277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ede7cca88adc4014a9e11e067e19feaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "729cefafb85a4f9d920caeed611fb52a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4210977ce46443dc8e6b1f8878e4018f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17e91984ac6a4d3db9501b6b08f7a360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fad718dd873749adb02f93955c7510e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}