ðŸ“˜ Documentation: Non-Instruction Pretraining & Domain-Specific Finetuning of LLMs

This documentation describes the workflow, methodology, and usage of the Jupyter notebook
Non_Instruction_pretrain_llm_finetuning_on_domain_specific_data.ipynb,
which demonstrates how to continue pretraining a Large Language Model (LLM) on raw domain-specific text data.

This process is also known as:

Non-Instruction Finetuning

Domain Adaptation

Continued Pretraining (CTP)

Causal Language Modeling (CLM) Training

1. Overview

The purpose of this notebook is to train an existing LLM (e.g., TinyLLaMA) on custom textual data to make it:

More knowledgeable about a particular domain

Better aligned to domain vocabulary and writing style

More accurate in downstream tasks (summaries, Q/A, completions, etc.)

Ready for later instruction-tuning or RAG pipelines

The training method uses Causal Language Modeling, requiring only plain text, without prompts or annotation.

2. Workflow Summary

The notebook implements the following workflow:

Step 1 â€” Load Raw Custom Corpus

Accepts .txt or combined text files.

Performs cleaning, concatenation, deduplication.

Loads data into a HuggingFace Dataset.

Step 2 â€” Select Base Model

Notebook uses a TinyLLaMA checkpoint (~1.4M steps).

This checkpoint is lightweight and suitable for:

Faster training

Lower compute usage

Research and prototyping

Step 3 â€” Tokenization & Preprocessing

The dataset is processed to fit the CLM training format:

Item	Description
Tokenization	Converts text to token IDs
Grouping	Fixed-length sequences (e.g., 512/1024 tokens)
Labels	Shifted by 1 position for next-token prediction

Example:

Input:  "The cat sat on the"
Label:  "cat sat on the mat"

Step 4 â€” Configure Training Pipeline

The notebook configures:

AutoModelForCausalLM

AutoTokenizer

TrainingArguments

Mixed precision (fp16/bf16)

Gradient accumulation

Checkpoint saving

Evaluation steps

Step 5 â€” Run Training (Continued Pretraining)

During training:

Loss decreases as model learns domain patterns.

Model becomes fluent in vocabulary/style of your text.

No Q/A or instructions required.

Step 6 â€” Save & Export Model

Outputs include:

Finetuned model weights

Tokenizer

Final + intermediate checkpoints

Training logs

Step 7 â€” Testing the Model

The notebook demonstrates:

Domain-aware text generation

Better completion accuracy

Style transformation based on learned patterns

3. Folder Structure

Expected repository structure:

â”œâ”€â”€ data/
â”‚   â””â”€â”€ domain_corpus.txt
â”œâ”€â”€ model/
â”‚   â””â”€â”€ finetuned_model/
â”œâ”€â”€ Non_Instruction_pretrain_llm_finetuning_on_domain_specific_data.ipynb
â””â”€â”€ README.md

4. Usage Guide
4.1. Installation

Install required Python packages:

pip install transformers datasets accelerate bitsandbytes


(Optional for GPU optimization):

pip install peft

4.2. Prepare Your Domain Data

Place your text corpus inside:

data/domain_corpus.txt


This file may contain:

Articles

Research papers

Domain reports

Logs

Documentation

Any raw text

No formatting is required.

4.3. Running the Notebook

Start Jupyter:

jupyter notebook


Then open:

Non_Instruction_pretrain_llm_finetuning_on_domain_specific_data.ipynb


Execute the cells sequentially.

5. Technical Details
5.1. Training Objective: Causal Language Modeling

Causal LM trains the model to predict the next token:

P(token_n | token_1, token_2, ..., token_{n-1})


This teaches:

Domain terminology

Writing structure

Semantic relations

Context continuity

5.2. Why Non-Instruction Finetuning?
Feature	Benefit
No prompts required	Faster dataset preparation
Uses plain text	Minimal overhead
Efficient	Works on small GPUs
Domain specialization	More accurate responses
Prepares model for instruction tuning	Better final performance
5.3. Model Compatibility

This pipeline supports:

TinyLLaMA

LLaMA / LLaMA-2 / LLaMA-3 variants

Mistral

Gemma

Falcon

GPT-NeoX / Pythia

As long as the model uses Causal LM.

6. Inference After Training

Example inference script:

from transformers import AutoModelForCausalLM, AutoTokenizer

tok = AutoTokenizer.from_pretrained("model/finetuned_model")
model = AutoModelForCausalLM.from_pretrained("model/finetuned_model")

prompt = "Explain the key concepts in my domain:"
inputs = tok(prompt, return_tensors="pt")

outputs = model.generate(**inputs, max_length=200)
print(tok.decode(outputs[0], skip_special_tokens=True))

7. Best Practices

Ensure dataset quality â†’ model quality

Remove repeated paragraphs

Use >50k tokens of domain text for meaningful adaptation

Longer training sequences capture better context

Use lower learning_rate for stable continued pretraining (e.g., 5e-5 to 1e-4)

8. Limitations

Not suitable for instruction following (requires instruction-tuning stage).

Bad input text â†’ degraded model behavior

Large models require powerful GPUs

9. Author

Ranjan Yadav

GitHub: https://github.com/Ranjan83711

LinkedIn: https://www.linkedin.com/in/ranjan83711
