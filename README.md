# Non_Instruction_Pretrain_llm_finetuning

Non-Instruction Pretraining & Domain-Specific Finetuning of LLMs

This repository contains a Jupyter notebook demonstrating non-instruction finetuning (also called continued pretraining) of a Large Language Model using your own custom domain-specific text data.

Unlike instruction-tuningâ€”which requires questionâ€“answer pairsâ€”this project shows how to train a model on raw text only, letting it learn domain context, terminology, and writing style.

ğŸ“Œ What This Notebook Covers
âœ”ï¸ 1. Loading and Understanding Custom Non-Instruction Data

The notebook begins by explaining how raw domain text is used for continued pretraining.
This includes:

Preparing raw .txt or combined text corpus

Cleaning, deduplication, formatting

Converting text into a dataset compatible with HuggingFace Datasets

Understanding causal language modeling (CLM) targets

Example:

Input:  "The cat sat on the"
Label:  "cat sat on the mat"

âœ”ï¸ 2. Choosing a Base Model (TinyLLaMA Checkpoint)

The notebook uses a TinyLLaMA checkpoint (~1.4M steps mid-training).
These checkpoints are lightweight and excellent for:

Research

Finetuning on small datasets

Resource-constrained training

It also explains:

Why mid-training checkpoints work better for domain adaptation

Model size, tokenizer behavior, and architecture overview

âœ”ï¸ 3. Preprocessing Dataset for Causal LM

The notebook walks through:

Tokenizing text

Grouping tokens into fixed-size training blocks

Creating trainâ€“test splits

Formatting input/label pairs for CLM

Ensuring the model predicts the next token correctly

âœ”ï¸ 4. Setting Up Training Pipeline

Using HuggingFace Transformers, Accelerate, and Trainer, the notebook configures:

Model loading

Tokenizer alignment

TrainingArguments

Mixed precision (fp16/bf16)

Gradient accumulation for small GPU setups

Evaluation steps

Checkpointing & logging

âœ”ï¸ 5. Running Non-Instruction Finetuning

This is the core of the notebook:

CLM training loop

Loss curve explanation

How the model learns domain patterns

How raw text finetuning differs from instruction finetuning

âœ”ï¸ 6. Saving & Exporting the Finetuned Model

The notebook shows:

Saving weights

Saving tokenizer

Pushing to HuggingFace Hub (optional)

Loading the trained model for inference

âœ”ï¸ 7. Testing the Model

Examples of how the model behaves differently after finetuning:

Better domain terminology usage

More context-aware completions

Improved coherence in the target domain

ğŸ“‚ Project Structure
Non_Instruction_pretrain_llm_finetuning_on_domain_specific_data.ipynb
data/
    domain_corpus.txt (your custom text)
model/
    finetuned_model/
README.md

ğŸ§  Key Concepts Explained in the Notebook
ğŸ”¹ What is Non-Instruction Pretraining?

It is the process of continuing to train a model on raw text without prompts or Q/A pairs.
Useful when your goal is:

Domain vocabulary adaptation

Style alignment

Knowledge infusion

Continuing LLaMA-like pretraining

ğŸ”¹ Why Use It?

No need to create instruction datasets

Much cheaper than pretraining from scratch

Works well even with small domain text

Improves downstream instruction-tuning quality

ğŸš€ How to Use This Notebook
1. Clone the Repo
git clone <your-repo-url>
cd your-repo

2. Install Dependencies
pip install -r requirements.txt


Typical libraries include:

transformers

datasets

accelerate

bitsandbytes (optional)

3. Add Your Custom Text Data

Place your domain text inside:

/data/domain_corpus.txt

4. Run Notebook Cell-by-Cell

Open the notebook:

jupyter notebook


The notebook will:

Load your corpus

Tokenize & preprocess

Train the model

Save outputs

5. Use the Model
from transformers import AutoModelForCausalLM, AutoTokenizer

tok = AutoTokenizer.from_pretrained("path/to/finetuned_model")
model = AutoModelForCausalLM.from_pretrained("path/to/finetuned_model")

ğŸ“‰ Training Tips Included in the Notebook

The notebook explains:

The effect of sequence length

Training on limited GPU memory

Choosing batch sizes

Reducing overfitting

Using gradient accumulation

Efficient use of LoRA (optional)

ğŸ“¦ Outputs You Get

After successful training, you will have:

âœ”ï¸ Finetuned LLaMA/TinyLLaMA model
âœ”ï¸ Tokenizer
âœ”ï¸ Training logs
âœ”ï¸ Evaluation metrics
âœ”ï¸ Model ready for downstream instruction-tuning
ğŸ™Œ Why This Notebook Is Useful

This notebook is ideal for:

Researchers

Students

Startups training small domain models

Custom enterprise LLM use-cases

Anyone working with raw text instead of Q/A datasets

ğŸ“§ Author Info

Ranjan Yadav

GitHub: https://github.com/Ranjan83711

LinkedIn: https://www.linkedin.com/in/ranjan83711
